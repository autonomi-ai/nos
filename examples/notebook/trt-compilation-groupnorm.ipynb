{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6056b2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update && apt-get install -y graphviz\n",
    "!pip install expecttest pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbe592f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.fx\n",
    "import torch.nn as nn\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca67f1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt\n",
    "print(tensorrt.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dee170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_tensorrt\n",
    "print(torch_tensorrt.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf0b4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_tensorrt.fx.tracer.acc_tracer import acc_normalizer, acc_ops, acc_shape_prop, acc_utils  # noqa: F401\n",
    "from torch.fx.experimental.normalize import NormalizeArgs\n",
    "\n",
    "import torch_tensorrt\n",
    "from torch_tensorrt.fx.utils import LowerPrecision\n",
    "import torch_tensorrt.fx.tracer.acc_tracer.acc_tracer as acc_tracer\n",
    "from torch_tensorrt.fx import InputTensorSpec, TRTInterpreter, TRTModule\n",
    "from torch_tensorrt.fx.tools.trt_splitter import TRTSplitter, TRTSplitterSetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53017149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.fx.node import Argument, Target\n",
    "from typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, Union\n",
    "from torch_tensorrt.fx.converters.converter_utils import SourceIR\n",
    "from torch_tensorrt.fx.converter_registry import tensorrt_converter\n",
    "from torch_tensorrt.fx.tracer.acc_tracer.acc_op_properties import AccOpProperty, register_acc_op_properties\n",
    "from torch_tensorrt.fx.tracer.acc_tracer.acc_normalizer import (\n",
    "    register_acc_op,\n",
    "    register_acc_op_mapping,\n",
    "    register_custom_acc_mapper_fn,\n",
    ")\n",
    "from torch_tensorrt.fx.types import (\n",
    "    TRTNetwork,\n",
    "    TRTTensor,\n",
    ")\n",
    "from torch_tensorrt.fx.converters.impl import activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591b0a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.ERROR)\n",
    "trt.init_libnvinfer_plugins(TRT_LOGGER, \"\")\n",
    "print(f\"Register libnvinfer plugins\")\n",
    "registry = trt.get_plugin_registry()\n",
    "print(f\"Registry: {registry}\")\n",
    "for plugin in registry.plugin_creator_list:\n",
    "    print(plugin.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a628c15",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torch_tensorrt.fx.converter_registry import CONVERTERS\n",
    "from torch_tensorrt.fx.tracer.acc_tracer.acc_normalizer import _acc_ops, _normalization_dict\n",
    "from torch_tensorrt.fx.tracer.acc_tracer import acc_ops\n",
    "\n",
    "print(\">>\" * 40)\n",
    "print(\"Converters\")\n",
    "print(\">>\" * 40)\n",
    "for op in CONVERTERS:\n",
    "    print(op)\n",
    "    \n",
    "print(\">>\" * 40)\n",
    "print(\"acc_ops\")\n",
    "print(\">>\" * 40)\n",
    "for op in _acc_ops:\n",
    "    print(op)\n",
    "    \n",
    "print(\">>\" * 40)\n",
    "print(\"_normalization_dict\")\n",
    "print(\">>\" * 40)\n",
    "for op in _normalization_dict:\n",
    "    print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156214f5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for op in list(CONVERTERS.keys()):\n",
    "    if op == acc_ops.gelu:\n",
    "        CONVERTERS.pop(op)\n",
    "        print(f\"removed converter {op}\")\n",
    "\n",
    "for op in list(_acc_ops):\n",
    "    if op.__name__ == \"gelu\":\n",
    "        _acc_ops.remove(op)\n",
    "        print(f\"removed acc_op: {op}\")\n",
    "        \n",
    "for (op, target) in list(_normalization_dict.keys()):\n",
    "    if \"gelu\" in str(target) or \"GELU\" in str(target):\n",
    "        _normalization_dict.pop((op, target))\n",
    "        print(f\"removed normalization_dict op: {op}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d929e651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def load_torchtrt_plugins():\n",
    "    # ctypes.CDLL(osp.join(dir_path, 'libamirstan_plugin.so'))\n",
    "    # suppose plugins lib installed into HOME\n",
    "    lib_path = Path(torch_tensorrt.__file__).parent / \"lib\"\n",
    "    print(f\"Using torch_tensorrt: {torch_tensorrt.__version__}, lib_path={lib_path}\")\n",
    "    # \"libtorchtrt.so\", \"libtorchtrt_runtime.so\", \n",
    "    for lib in [\"libtorchtrt_plugins.so\"]:\n",
    "        path = lib_path / lib\n",
    "        if not path.exists():\n",
    "            print(f\"Failed to load lib: {path}\")\n",
    "        ctypes.CDLL(str(path))\n",
    "        print(f\"Loaded {path}\")\n",
    "\n",
    "load_torchtrt_plugins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f34a7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in registry.plugin_creator_list:\n",
    "    print(item.name, item.plugin_version, item.plugin_namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8852e377",
   "metadata": {},
   "outputs": [],
   "source": [
    "plugin.plugin_namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02979966",
   "metadata": {},
   "outputs": [],
   "source": [
    "plugin.plugin_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470d3b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "\n",
    "trt.ITensor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a581642",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_acc_op_properties(AccOpProperty.pointwise, AccOpProperty.unary)\n",
    "@register_acc_op_mapping(op_and_target=(\"call_function\", torch.nn.functional.gelu))\n",
    "@register_acc_op_mapping(op_and_target=(\"call_method\", \"gelu\"))\n",
    "@register_custom_acc_mapper_fn(\n",
    "    op_and_target=(\"call_module\", torch.nn.GELU),\n",
    "    arg_replacement_tuples=[\n",
    "        (\"input\", \"input\"),\n",
    "    ],\n",
    ")\n",
    "@register_acc_op\n",
    "def gelu(*, input):\n",
    "    return nn.functional.relu(input=input, inplace=False)\n",
    "\n",
    "\n",
    "# @tensorrt_converter(torch.nn.functional.gelu)\n",
    "# @tensorrt_converter(torch.nn.modules.activation.GELU)\n",
    "# def _gelu(network, submod, args, kwargs, layer_name):\n",
    "#     # args/kwargs should have already been normalized to kwargs\n",
    "#     assert len(args) == 0\n",
    "\n",
    "#     return activation.relu(\n",
    "#         network=network,\n",
    "#         target=\"torch.nn.functional.relu\",\n",
    "#         source_ir=SourceIR.NN,\n",
    "#         name=layer_name,\n",
    "#         input_val=kwargs[\"input\"],\n",
    "#     )\n",
    "\n",
    "\n",
    "# @register_acc_op_mapping(\n",
    "#     op_and_target=(\"call_function\", torch.nn.modules.GroupNorm),\n",
    "#     arg_replacement_tuples=[\n",
    "#         (\"input\", \"input\"),\n",
    "#         (\"num_groups\", \"num_groups\"),\n",
    "#         (\"weight\", \"weight\"),\n",
    "#         (\"bias\", \"bias\"),\n",
    "#         (\"eps\", \"eps\"),\n",
    "#     ],\n",
    "# )\n",
    "# @register_acc_op\n",
    "# def group_norm(*, input, num_groups, weight=None, bias=None, eps=1e-05):\n",
    "#     return GroupNormalizationPlugin.apply(input, self.weight, self.bias, self.num_groups, self.eps)\n",
    "#     return torch.nn.functional.group_norm(\n",
    "#         input, num_groups, weight=weight, bias=bias, eps=eps\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32437f57",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# auto creator = getPluginRegistry()->getPluginCreator(\"Interpolate\", \"1\", \"torch_tensorrt\");\n",
    "# auto interpolate_plugin = creator->createPlugin(name, &fc);\n",
    "\n",
    "# auto resize_layer = ctx->net->addPluginV2(reinterpret_cast<nvinfer1::ITensor* const*>(&in), 1, *interpolate_plugin);\n",
    "# TORCHTRT_CHECK(resize_layer, \"Unable to create interpolation plugin from node\" << *n);\n",
    "\n",
    "# resize_layer->setName(util::node_info(n).c_str());\n",
    "\n",
    "# auto layer_output = ctx->AssociateValueAndTensor(n->outputs()[0], resize_layer->getOutput(0));\n",
    "\n",
    "creator = trt.get_plugin_registry().get_plugin_creator(\"GroupNormalizationPlugin\", \"1\", \"torch_tensorrt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3998d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_tensorrt.fx.converter_registry import tensorrt_converter\n",
    "\n",
    "\n",
    "def gelu_fn(x):\n",
    "    \"\"\"\n",
    "    https://github.com/geohot/tinygrad/blob/18892242b006785d4e92abae7c792e7874c17df9/tinygrad/tensor.py#L522\n",
    "    \"\"\"\n",
    "    return 0.5 * x * (1 + (x * 0.7978845608 * (1 + 0.044715 * x * x)).tanh())\n",
    "\n",
    "\n",
    "@tensorrt_converter(torch.nn.functional.gelu)\n",
    "@tensorrt_converter(torch.nn.modules.activation.GELU)\n",
    "def relu(network, submod, args, kwargs, layer_name):\n",
    "    # args/kwargs should have already been normalized to kwargs\n",
    "    assert len(args) == 0\n",
    "\n",
    "    return activation.relu(\n",
    "        network=network,\n",
    "        target=\"torch.nn.functional.relu\",\n",
    "        source_ir=SourceIR.NN,\n",
    "        name=layer_name,\n",
    "        input_val=kwargs[\"input\"],\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983333d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_tensorrt as torchtrt\n",
    "\n",
    "\n",
    "# Create a sample network with a conv and gelu node.\n",
    "# Gelu layer in Torch-TensorRT is converted to CustomGeluPluginDynamic from TensorRT plugin registry.\n",
    "class ConvGelu(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvGelu, self).__init__()\n",
    "        self.conv = nn.Conv2d(3, 32, 3, 1)\n",
    "        # self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        # x = self.gelu(x)\n",
    "        x = torch.nn.functional.gelu(x)\n",
    "        # x = gelu_fn(x)\n",
    "        return x\n",
    "    \n",
    "model = ConvGelu().eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db0fe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torchtrt.Input?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72ee91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.fx import symbolic_trace, replace_pattern\n",
    "\n",
    "# Replace `pattern` with `replacement` in `traced`\n",
    "traced = symbolic_trace(model)\n",
    "print(traced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa19ceb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the pattern. The FX Subgraph Rewriter will match all\n",
    "# non-overlapping instances of the pattern in the larger graph.\n",
    "# Note that Pattern-matching is done based on data dependencies,\n",
    "# not Node names. Even though we're operating on Nodes named `a1` and\n",
    "# `a2` instead of `w1` and `w2`, the pattern is still a valid match\n",
    "# for the two instances of `torch.cat([w1, w2]).sum()` above. Only\n",
    "# operations that contribute to the single output value of the pattern\n",
    "# are considered\n",
    "def pattern(x):\n",
    "    return torch.nn.functional.gelu(x)\n",
    "\n",
    "# Define the replacement (same rules as the pattern)\n",
    "def replacement(x):\n",
    "    return gelu_fn(x)\n",
    "\n",
    "replace_pattern(traced, pattern, replacement)\n",
    "print(traced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6dc62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = [1, 3, 5, 5]\n",
    "compile_settings = {\n",
    "    \"inputs\": [torchtrt.Input(shape, dtype=torch.float32)],\n",
    "    \"enabled_precisions\": {torch.float32},\n",
    "}\n",
    "with torch.inference_mode():\n",
    "    scripted_model = torch.jit.script(traced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62355cb5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scripted_model.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f3e0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    trt_traced = acc_tracer.trace(\n",
    "        traced, [torch.rand(*shape, dtype=torch.float32, device=\"cuda\")], \n",
    "    )\n",
    "trt_traced.graph.print_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ba2b07",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trt_ts_module = torchtrt.compile(scripted_model, **compile_settings)\n",
    "torch.jit.save(trt_ts_module, \"conv_gelu.jit\")\n",
    "print(\"Generated Torchscript-TRT GELU model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f18647",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_ts_module.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307564d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(*shape, dtype=torch.float32, device=\"cuda\")\n",
    "with torch.inference_mode():\n",
    "    trt_traced = acc_tracer.trace(model, [x])\n",
    "\n",
    "    splitter = TRTSplitter(trt_traced, [x])\n",
    "    splitter.node_support_preview(dump_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e529396e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import pydot\n",
    "\n",
    "graphs = pydot.graph_from_dot_file(\"node_support.dot\")\n",
    "Image(graphs[0].create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4c0ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_ts_gelu_model = torch.load(\"conv_gelu.jit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc670357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc209783",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%timeit -n 10\n",
    "trt_ts_gelu_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8552f032",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6dec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorrt as trt\n",
    "\n",
    "from torch_tensorrt.fx.converters.acc_ops_converters import get_trt_plugin, TRTPluginFieldCollection, _LOGGER\n",
    "from torch_tensorrt.fx.converter_registry import tensorrt_converter\n",
    "from torch_tensorrt.fx.converters.converter_utils import get_trt_tensor\n",
    "from torch_tensorrt.fx.utils import torch_dtype_from_trt\n",
    "\n",
    "\n",
    "#     if weight is None:\n",
    "#         weight = torch.ones((input.shape[1],), dtype=input.dtype, device=input.device)\n",
    "#     if bias is None:\n",
    "#         bias = torch.zeros((input.shape[1],), dtype=input.dtype, device=input.device)\n",
    "\n",
    "\n",
    "\n",
    "@register_acc_op_mapping(\n",
    "    op_and_target=(\"call_function\", torch.nn.functional.group_norm),\n",
    "    arg_replacement_tuples=[\n",
    "        (\"input\", \"input\"),\n",
    "        (\"num_groups\", \"num_groups\"),\n",
    "        (\"weight\", \"weight\"),\n",
    "        (\"bias\", \"bias\"),\n",
    "        (\"eps\", \"eps\"),\n",
    "    ],\n",
    ")\n",
    "@register_acc_op\n",
    "def group_norm(*, input, num_groups, weight=None, bias=None, eps=1e-05):\n",
    "    return torch.nn.functional.group_norm(input, num_groups, weight=weight, bias=bias, eps=eps)\n",
    "\n",
    "\n",
    "@tensorrt_converter(group_norm)\n",
    "def acc_ops_group_norm(network, target, args, kwargs, name):\n",
    "    input_val = kwargs[\"input\"]\n",
    "    weight = kwargs[\"weight\"]\n",
    "    bias = kwargs[\"bias\"]\n",
    "    print(\"input: \", input_val.shape, input_val.dtype)\n",
    "    print(\"weight: \", weight)\n",
    "    print(\"bias: \", bias)\n",
    "    shape = (input_val.shape[1],)\n",
    "    if weight is None:\n",
    "        weight = torch.ones(tuple([*input_val.shape])).to(\n",
    "            torch_dtype_from_trt(input_val.dtype)\n",
    "        )\n",
    "    weight = get_trt_tensor(network, weight, f\"{name}_weight\")\n",
    "\n",
    "        # weight = network.add_input(\"weight\", input_val.dtype, shape)\n",
    "        # ones = network.add_constant(shape=shape, weights=np.ones(shape=shape, dtype=np.float32)).get_output(0)\n",
    "        # weight = network.add_elementwise(weight, ones, op=trt.ElementWiseOperation.SUM).get_output(0)\n",
    "\n",
    "    if bias is None:\n",
    "        bias = torch.zeros(tuple([*input_val.shape])).to(\n",
    "            torch_dtype_from_trt(input_val.dtype)\n",
    "        )\n",
    "    bias = get_trt_tensor(network, bias, f\"{name}_bias\")\n",
    "        # bias = network.add_input(\"bias\", trt.float32, shape)\n",
    "\n",
    "    if not isinstance(input_val, trt.tensorrt.ITensor):\n",
    "        raise RuntimeError(\n",
    "            f\"GroupNorm received input {input_val} that is not part \"\n",
    "            \"of the TensorRT region!\"\n",
    "        )\n",
    "\n",
    "    num_groups_field = trt.PluginField(\n",
    "        \"num_groups\", np.array([kwargs[\"num_groups\"]], dtype=np.int32), trt.PluginFieldType.INT32\n",
    "    )\n",
    "    eps_field = trt.PluginField(\n",
    "        \"eps\", np.array([kwargs[\"eps\"]], dtype=np.float32), trt.PluginFieldType.FLOAT32\n",
    "    )\n",
    "\n",
    "    field_collection = trt.PluginFieldCollection(\n",
    "        [eps_field, num_groups_field]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        plugin = get_trt_plugin(\"GroupNormalizationPlugin\", field_collection, \"1\", \"\")\n",
    "    except AssertionError:\n",
    "        _LOGGER.error(\n",
    "            \"Unable to find group norm plugin, fall back to TensorRT implementation.\"\n",
    "        )\n",
    "        raise RuntimeError(\n",
    "            f\"Failed to build group norm plugin.\"\n",
    "        )\n",
    "    print(f\"adding plugin v2\")\n",
    "    layer = network.add_plugin_v2([input_val, weight, bias], plugin)\n",
    "    layer.name = name\n",
    "    return layer.get_output(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f0cf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_tensorrt as torchtrt\n",
    "\n",
    "# create a simple norm layer.\n",
    "# This norm layer uses NormalizePlugin from Torch-TensorRT\n",
    "\n",
    "class Norm(torch.nn.Module):\n",
    "    def __init__(self, C: int):\n",
    "        super(Norm, self).__init__()\n",
    "        self.gn = nn.GroupNorm(C // 2, C)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gn(x)\n",
    "#         num_groups = 2\n",
    "#         return torch.nn.functional.group_norm(\n",
    "#             x, num_groups, eps=1e-05\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8897006",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 6\n",
    "norm_model = Norm(C).eval().cuda()\n",
    "with torch.inference_mode():\n",
    "    norm_ts_module = torch.jit.script(norm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cb7f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_ts_module.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1b8ced",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "shape = [1, C, 64, 64]\n",
    "x = torch.rand(*shape, dtype=torch.float32, device=\"cuda\")\n",
    "with torch.inference_mode():\n",
    "    trt_traced = acc_tracer.trace(norm_model, [x])\n",
    "print(trt_traced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5098d455",
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_settings = {\n",
    "    \"inputs\": [torchtrt.Input(shape, dtype=torch.float32)],\n",
    "    \"enabled_precisions\": {torch.float32},\n",
    "}\n",
    "\n",
    "norm_trt_ts = torchtrt.compile(norm_ts_module, **compile_settings)\n",
    "torch.jit.save(norm_trt_ts, \"norm_trt_ts.pt\")\n",
    "print(\"Generated Torchscript-TRT GroupNorm model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7783e05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(norm_trt_ts.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d60d71",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    splitter = TRTSplitter(trt_traced, [x])\n",
    "    splitter.node_support_preview(dump_graph=False)\n",
    "    \n",
    "split_mod = splitter()\n",
    "inputs = [x]\n",
    "\n",
    "def get_submod_inputs(_mod, _submod, _inputs):\n",
    "    acc_inputs = None\n",
    "\n",
    "    def get_input(self, inputs):\n",
    "        nonlocal acc_inputs\n",
    "        acc_inputs = inputs\n",
    "\n",
    "    handle = _submod.register_forward_pre_hook(get_input)\n",
    "    with torch.inference_mode():\n",
    "        _mod(*_inputs)\n",
    "    handle.remove()\n",
    "    return acc_inputs\n",
    "\n",
    "\n",
    "# Since the model is splitted into three segments. We need to lower each TRT eligible segment.\n",
    "# If we know the model can be fully lowered, we can skip the splitter part.\n",
    "for name, _ in split_mod.named_children():\n",
    "    print(f\"Splitting {name}\")\n",
    "    if \"_run_on_acc\" in name:\n",
    "        submod = getattr(split_mod, name)\n",
    "\n",
    "        # Get submodule inputs for fx2trt\n",
    "        acc_inputs = get_submod_inputs(split_mod, submod, inputs)\n",
    "\n",
    "        # fx2trt replacement\n",
    "        interp = TRTInterpreter(\n",
    "            submod,\n",
    "            InputTensorSpec.from_tensors(acc_inputs),\n",
    "            explicit_batch_dimension=True,\n",
    "        )\n",
    "        r = interp.run(lower_precision=LowerPrecision.FP32)\n",
    "        trt_mod = TRTModule(*r)\n",
    "        setattr(split_mod, name, trt_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f08dc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"norm_trt_engine.pt\"\n",
    "torch.save(split_mod, filename)\n",
    "trt_ts_norm_model = torch.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480591c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, C, 64, 64, dtype=torch.float32, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c779700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit -n 500\n",
    "norm_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d768f147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit -n 500\n",
    "trt_ts_norm_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac3bf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorrt as trt\n",
    "\n",
    "\n",
    "def create_groupnorm_plugin(layer_name, num_groups, eps=1e-5):\n",
    "    creator = trt.get_plugin_registry().get_plugin_creator(\n",
    "        'GroupNormPluginDynamic', '1', '')\n",
    "\n",
    "    pfc = trt.PluginFieldCollection()\n",
    "    pf_num_groups = trt.PluginField('num_groups',\n",
    "                                    np.array([num_groups], dtype=np.int32),\n",
    "                                    trt.PluginFieldType.INT32)\n",
    "    pfc.append(pf_num_groups)\n",
    "\n",
    "    pf_eps = trt.PluginField('eps', np.array([eps], dtype=np.float32),\n",
    "                             trt.PluginFieldType.FLOAT32)\n",
    "    pfc.append(pf_eps)\n",
    "    return creator.create_plugin(layer_name, pfc)\n",
    "\n",
    "\n",
    "@tensorrt_converter(torch.nn.GroupNorm.forward)\n",
    "def convert_GroupNorm(network, submod, args, kwargs, layer_name):\n",
    "    input = kwargs[\"input\"]\n",
    "    weight = kwargs[\"weight\"]\n",
    "    bias = kwargs[\"bias\"]\n",
    "    \n",
    "\n",
    "#     input_trt = trt_(ctx.network, input)\n",
    "#     weight_trt = trt_(ctx.network, module.weight)\n",
    "#     bias_trt = trt_(ctx.network, module.bias)\n",
    "    output = ctx.method_return\n",
    "\n",
    "    num_groups = module.num_groups\n",
    "    eps = module.eps\n",
    "\n",
    "    plugin = create_groupnorm_plugin(\n",
    "        'groupnorm_' + str(id(module)), num_groups=num_groups, eps=eps)\n",
    "\n",
    "    custom_layer = ctx.network.add_plugin_v2(\n",
    "        inputs=[input_trt, weight_trt, bias_trt], plugin=plugin)\n",
    "\n",
    "    output._trt = custom_layer.get_output(0)\n",
    "    \n",
    "\n",
    "\n",
    "# @tensorrt_converter('torch.nn.functional.group_norm')\n",
    "# def convert_group_norm(ctx):\n",
    "\n",
    "#     input = get_arg(ctx, 'input', pos=0, default=None)\n",
    "#     num_groups = get_arg(ctx, 'num_groups', pos=1, default=None)\n",
    "#     weight = get_arg(ctx, 'weight', pos=2, default=None)\n",
    "#     bias = get_arg(ctx, 'bias', pos=3, default=None)\n",
    "#     eps = get_arg(ctx, 'eps', pos=4, default=1e-5)\n",
    "#     output = ctx.method_return\n",
    "\n",
    "\n",
    "#     input_trt, eps_trt = add_missing_trt_tensors(ctx.network, [input, eps])\n",
    "    \n",
    "#     shape = list(input.shape)\n",
    "#     split_shape = [shape[0]] + [num_groups, shape[1] // num_groups] + shape[2:]\n",
    "#     split_shape = tuple(split_shape)\n",
    "#     keepdim = True\n",
    "\n",
    "#     # split into groups\n",
    "#     layer = ctx.network.add_shuffle(input_trt)\n",
    "#     layer.reshape_dims = split_shape\n",
    "#     a = layer.get_output(0)\n",
    "\n",
    "\n",
    "#     # compute mean over groups\n",
    "#     reduce_dims = tuple(range(2, len(split_shape)))\n",
    "#     axes = torch_dim_to_trt_axes(reduce_dims)\n",
    "#     layer = ctx.network.add_reduce(a, trt.ReduceOperation.AVG, axes, keepdim)\n",
    "#     a_mean = layer.get_output(0)\n",
    "\n",
    "#     # compute stdev over groups\n",
    "#     a_diff = ctx.network.add_elementwise(a, a_mean, trt.ElementWiseOperation.SUB).get_output(0)\n",
    "#     a_dist = ctx.network.add_elementwise(a_diff, a_diff, trt.ElementWiseOperation.PROD).get_output(0)\n",
    "#     a_var = ctx.network.add_reduce(a_dist, trt.ReduceOperation.AVG, axes, keepdim).get_output(0)\n",
    "\n",
    "\n",
    "#     a_var, eps_trt = broadcast_trt_tensors(ctx.network, [a_var, eps_trt], len(split_shape))\n",
    "\n",
    "#     a_var_eps = ctx.network.add_elementwise(a_var, eps_trt, trt.ElementWiseOperation.SUM).get_output(0)\n",
    "#     a_std = ctx.network.add_unary(a_var_eps, trt.UnaryOperation.SQRT).get_output(0)\n",
    "\n",
    "#     # divide by stdev\n",
    "#     b = ctx.network.add_elementwise(a_diff, a_std, trt.ElementWiseOperation.DIV).get_output(0)\n",
    "\n",
    "#     # reshape\n",
    "#     layer = ctx.network.add_shuffle(b)\n",
    "#     layer.reshape_dims = shape\n",
    "\n",
    "#     c = layer.get_output(0)\n",
    "\n",
    "#     # handle affine version\n",
    "#     if weight is not None or bias is not None:\n",
    "#         if weight is not None:\n",
    "#             scale = weight.detach().cpu().numpy()\n",
    "#         else:\n",
    "#             scale = np.ones(input.shape[1])\n",
    "\n",
    "#         if bias is not None:\n",
    "#             bias = bias.detach().cpu().numpy()\n",
    "#         else:\n",
    "#             bias = np.zeros(input.shape[1])\n",
    "\n",
    "#         power = np.ones_like(scale)\n",
    "\n",
    "#         layer = ctx.network.add_scale_nd(c, trt.ScaleMode.CHANNEL, bias, scale, power, 1)\n",
    "#         c = layer.get_output(0)\n",
    "\n",
    "#     output._trt = c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafcff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch/TensorRT\n",
    "# def common_batchnorm(network, mod, input_val, layer_name, is_quantized):\n",
    "#     scale = to_numpy(mod.weight) / np.sqrt(to_numpy(mod.running_var) + mod.eps)\n",
    "#     bias = to_numpy(mod.bias) - to_numpy(mod.running_mean) * scale\n",
    "#     power = np.ones_like(scale)\n",
    "\n",
    "#     layer = network.add_scale(input_val, trt.ScaleMode.CHANNEL, bias, scale, power)\n",
    "#     layer.name = layer_name\n",
    "\n",
    "#     if is_quantized:\n",
    "#         mark_as_int8_layer(\n",
    "#             layer, get_dyn_range(mod.scale, mod.zero_point, torch.quint8)\n",
    "#         )\n",
    "\n",
    "#     return layer.get_output(0)\n",
    "\n",
    "\n",
    "# @tensorrt_converter(torch.nn.modules.batchnorm.BatchNorm2d)\n",
    "# def batchnorm2d(network, submod, args, kwargs, layer_name):\n",
    "#     # args/kwargs should have already been normalized to kwargs\n",
    "#     assert len(args) == 0\n",
    "#     input_val = kwargs[\"input\"]\n",
    "\n",
    "#     if not isinstance(input_val, trt.tensorrt.ITensor):\n",
    "#         raise RuntimeError(\n",
    "#             f\"BatchNorm2d received input {input_val} that is not part \"\n",
    "#             \"of the TensorRT region!\"\n",
    "#         )\n",
    "\n",
    "#     return common_batchnorm(network, submod, input_val, layer_name, is_quantized=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3829c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/THUDM/FastLDM/blob/7b5f5ff44551dc44daf938ba007d7827e9ac8c6b/fastldm/modules.py#L307\n",
    "\n",
    "# class BaseApply:\n",
    "#     @classmethod\n",
    "#     def apply(cls, *inputs, **kw_args):\n",
    "#         return cls.forward(None, *inputs, **kw_args)\n",
    "\n",
    "# BasePlugin = torch.autograd.Function # BaseApply if ONNX_ONLY else torch.autograd.Function\n",
    "\n",
    "# class GroupNormalizationPlugin(BasePlugin):\n",
    "#     # https://github.com/NVIDIA/TensorRT/tree/release/8.5/plugin/groupNormalizationPlugin\n",
    "#     @staticmethod\n",
    "#     def forward(ctx, x, scale, bias, num_groups, eps):\n",
    "#         return F.group_norm(x, num_groups, weight=scale, bias=bias, eps=eps)\n",
    "\n",
    "#     @staticmethod\n",
    "#     def symbolic(g, x, scale, bias, num_groups, eps):\n",
    "#         return g.op(\"GroupNormalizationPlugin\", x, scale, bias, plugin_version_s='1', eps_f=eps, num_groups_i=num_groups)\n",
    "\n",
    "\n",
    "# class GroupNorm(nn.Module):\n",
    "#     def __init__(self, num_groups, num_channels, eps, affine=True):\n",
    "#         super().__init__()\n",
    "#         assert num_channels % num_groups == 0\n",
    "#         self.eps = eps\n",
    "#         self.num_groups = num_groups\n",
    "#         self.weight = nn.Parameter(torch.ones(num_channels))\n",
    "#         self.bias = nn.Parameter(torch.zeros(num_channels))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return GroupNormalizationPlugin.apply(x, self.weight, self.bias, self.num_groups, self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9a3260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BreadcrumbsTensorRT/py/torch_tensorrt/fx/tracer/acc_tracer/acc_ops.py\n",
    "\n",
    "# @register_acc_op_properties(AccOpProperty.pointwise, AccOpProperty.unary)\n",
    "# @register_acc_op_mapping(op_and_target=(\"call_function\", torch.nn.functional.gelu))\n",
    "# @register_acc_op_mapping(op_and_target=(\"call_method\", \"gelu\"))\n",
    "# @register_custom_acc_mapper_fn(\n",
    "#     op_and_target=(\"call_module\", torch.nn.GELU),\n",
    "#     arg_replacement_tuples=[\n",
    "#         (\"input\", \"input\"),\n",
    "#         (\"approximate\", \"approximate\"),\n",
    "#     ],\n",
    "# )\n",
    "# @register_acc_op\n",
    "# def gelu(*, input, approximate=\"none\"):\n",
    "#     return torch.nn.functional.gelu(input=input, approximate=approximate)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
