{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6056b2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update && apt-get install -y graphviz\n",
    "!pip install expecttest pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbe592f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.fx\n",
    "import torch.nn as nn\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca67f1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt\n",
    "print(tensorrt.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dee170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_tensorrt\n",
    "print(torch_tensorrt.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf0b4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_tensorrt.fx.tracer.acc_tracer import acc_normalizer, acc_ops, acc_shape_prop, acc_utils  # noqa: F401\n",
    "from torch.fx.experimental.normalize import NormalizeArgs\n",
    "\n",
    "import torch_tensorrt\n",
    "from torch_tensorrt.fx.utils import LowerPrecision\n",
    "import torch_tensorrt.fx.tracer.acc_tracer.acc_tracer as acc_tracer\n",
    "from torch_tensorrt.fx import InputTensorSpec, TRTInterpreter, TRTModule\n",
    "from torch_tensorrt.fx.tools.trt_splitter import TRTSplitter, TRTSplitterSetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53017149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.fx.node import Argument, Target\n",
    "from typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, Union\n",
    "from torch_tensorrt.fx.converters.converter_utils import SourceIR\n",
    "from torch_tensorrt.fx.converter_registry import tensorrt_converter\n",
    "from torch_tensorrt.fx.tracer.acc_tracer.acc_op_properties import AccOpProperty, register_acc_op_properties\n",
    "from torch_tensorrt.fx.tracer.acc_tracer.acc_normalizer import (\n",
    "    register_acc_op,\n",
    "    register_acc_op_mapping,\n",
    "    register_custom_acc_mapper_fn,\n",
    ")\n",
    "from torch_tensorrt.fx.types import (\n",
    "    TRTNetwork,\n",
    "    TRTTensor,\n",
    ")\n",
    "from torch_tensorrt.fx.converters.impl import activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591b0a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.ERROR)\n",
    "trt.init_libnvinfer_plugins(TRT_LOGGER, \"\")\n",
    "print(f\"Register libnvinfer plugins\")\n",
    "registry = trt.get_plugin_registry()\n",
    "print(f\"Registry: {registry}\")\n",
    "for plugin in registry.plugin_creator_list:\n",
    "    print(plugin.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a628c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_tensorrt.fx.converter_registry import CONVERTERS\n",
    "from torch_tensorrt.fx.tracer.acc_tracer.acc_normalizer import _acc_ops, _normalization_dict\n",
    "from torch_tensorrt.fx.tracer.acc_tracer import acc_ops\n",
    "\n",
    "print(\">>\" * 40)\n",
    "print(\"Converters\")\n",
    "print(\">>\" * 40)\n",
    "for op in CONVERTERS:\n",
    "    print(op)\n",
    "    \n",
    "print(\">>\" * 40)\n",
    "print(\"acc_ops\")\n",
    "print(\">>\" * 40)\n",
    "for op in _acc_ops:\n",
    "    print(op)\n",
    "    \n",
    "print(\">>\" * 40)\n",
    "print(\"_normalization_dict\")\n",
    "print(\">>\" * 40)\n",
    "for op in _normalization_dict:\n",
    "    print(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156214f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for op in list(CONVERTERS.keys()):\n",
    "#     if op == acc_ops.gelu:\n",
    "#         CONVERTERS.pop(op)\n",
    "#         print(f\"removed converter {op}\")\n",
    "\n",
    "# for op in list(_acc_ops):\n",
    "#     if op.__name__ == \"gelu\":\n",
    "#         _acc_ops.remove(op)\n",
    "#         print(f\"removed acc_op: {op}\")\n",
    "        \n",
    "# for (op, target) in list(_normalization_dict.keys()):\n",
    "#     if \"gelu\" in str(target) or \"GELU\" in str(target):\n",
    "#         _normalization_dict.pop((op, target))\n",
    "#         print(f\"removed normalization_dict op: {op}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f0cf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_tensorrt as torchtrt\n",
    "\n",
    "# create a simple norm layer.\n",
    "# This norm layer uses NormalizePlugin from Torch-TensorRT\n",
    "\n",
    "class Norm(torch.nn.Module):\n",
    "    def __init__(self, C: int):\n",
    "        super(Norm, self).__init__()\n",
    "        self.gn = nn.GroupNorm(C // 2, C)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gn(x)\n",
    "#         num_groups = 2\n",
    "#         return torch.nn.functional.group_norm(\n",
    "#             x, num_groups, eps=1e-05\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8897006",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 6\n",
    "norm_model = Norm(C).eval().cuda()\n",
    "# with torch.inference_mode():\n",
    "#     norm_ts_module = torch.jit.script(norm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cb7f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_ts_module.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1b8ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = [1, C, 64, 64]\n",
    "x = torch.rand(*shape, dtype=torch.float32, device=\"cuda\")\n",
    "with torch.inference_mode():\n",
    "    trt_traced = acc_tracer.trace(norm_model, [x])\n",
    "print(trt_traced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5098d455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile_settings = {\n",
    "#     \"inputs\": [torchtrt.Input(shape, dtype=torch.float32)],\n",
    "#     \"enabled_precisions\": {torch.float32},\n",
    "# }\n",
    "\n",
    "# norm_trt_ts = torchtrt.compile(norm_ts_module, **compile_settings)\n",
    "# torch.jit.save(norm_trt_ts, \"norm_trt_ts.pt\")\n",
    "# print(\"Generated Torchscript-TRT GroupNorm model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7783e05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(norm_trt_ts.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec85e137-0403-48b9-9eb5-b5bb744ecd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Sequence\n",
    "from torch_tensorrt.dynamo.backend._settings import CompilationSettings\n",
    "from torch_tensorrt.dynamo.backend.lowering._partition import (\n",
    "    partition,\n",
    "    get_submod_inputs,\n",
    ")\n",
    "from torch_tensorrt.dynamo.backend.lowering._partition import logger as plogger\n",
    "from torch_tensorrt.dynamo.backend.conversion import convert_module\n",
    "\n",
    "\n",
    "# def _compile_module(\n",
    "#     gm: torch.fx.GraphModule,\n",
    "#     sample_inputs: Sequence[torch.Tensor],\n",
    "#     settings: CompilationSettings = CompilationSettings(),\n",
    "# ) -> torch.fx.GraphModule:\n",
    "#     \"\"\"Compile a traced FX module\n",
    "\n",
    "#     Includes: Partitioning + Conversion Phases\n",
    "\n",
    "#     Args:\n",
    "#         module: FX GraphModule to convert\n",
    "#         inputs: Inputs to the module\n",
    "#         settings: Compilation settings\n",
    "#     Returns:\n",
    "#         Compiled FX GraphModule\n",
    "#     \"\"\"\n",
    "#     # Partition module into components that can be TRT-accelerated\n",
    "#     level = plogger.level\n",
    "#     plogger.setLevel(logging.DEBUG)\n",
    "#     partitioned_module = partition(\n",
    "#         gm,\n",
    "#         verbose=settings.debug,\n",
    "#         min_block_size=settings.min_block_size,\n",
    "#         torch_executed_ops=settings.torch_executed_ops,\n",
    "#     )\n",
    "#     plogger.setLevel(level)\n",
    "\n",
    "#     # Iterate over all components that can be accelerated\n",
    "#     # Generate the corresponding TRT Module for those\n",
    "#     for name, _ in partitioned_module.named_children():\n",
    "#         if \"_run_on_acc\" not in name:\n",
    "#             continue\n",
    "#         submodule = getattr(partitioned_module, name)\n",
    "\n",
    "#         # Get submodule inputs\n",
    "#         submodule_inputs = get_submod_inputs(\n",
    "#             partitioned_module, submodule, sample_inputs\n",
    "#         )\n",
    "\n",
    "#         # Create TRT Module from submodule\n",
    "#         trt_mod = convert_module(\n",
    "#             submodule,\n",
    "#             submodule_inputs,\n",
    "#             settings=settings,\n",
    "#         )\n",
    "\n",
    "#         # Replace FX Module with TRT Module\n",
    "#         setattr(partitioned_module, name, trt_mod)\n",
    "\n",
    "#     return partitioned_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d60d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_tensorrt.dynamo.backend.lowering._partition import (\n",
    "#     partition,\n",
    "#     get_submod_inputs,\n",
    "# )\n",
    "# from torch_tensorrt.dynamo.backend.lowering._partition import logger as plogger\n",
    "\n",
    "# # with torch.inference_mode():\n",
    "# #     splitter = TRTSplitter(trt_traced, [x])\n",
    "# #     # splitter.node_support_preview(dump_graph=False)\n",
    "# import logging\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# level = plogger.level\n",
    "# plogger.setLevel(logging.DEBUG)\n",
    "# partitioned_module = partition(\n",
    "#     trt_traced,\n",
    "#     verbose=True,\n",
    "#     min_block_size=0,\n",
    "#     torch_executed_ops=[],\n",
    "# )\n",
    "# plogger.setLevel(level)\n",
    "\n",
    "\n",
    "# # Since the model is splitted into three segments. We need to lower each TRT eligible segment.\n",
    "# # If we know the model can be fully lowered, we can skip the splitter part.\n",
    "# inputs = [x]\n",
    "# for name, _ in partitioned_module.named_children():\n",
    "#     print(f\"Splitting {name}\")\n",
    "#     if \"_run_on_acc\" in name:\n",
    "#         submodule = getattr(partitioned_module, name)\n",
    "\n",
    "#         # Get submodule inputs for fx2trt\n",
    "#         submodule_inputs = get_submod_inputs(partitioned_module, submodule, inputs)\n",
    "\n",
    "#         # Create TRT Module from submodule\n",
    "#         trt_mod = convert_module(\n",
    "#             submodule,\n",
    "#             submodule_inputs,\n",
    "#             settings=settings,\n",
    "#         )\n",
    "        \n",
    "#         setattr(partitioned_module, name, trt_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c865963-e9aa-46c6-ba50-876eab40bbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model\n",
    "# del pipe\n",
    "# del traced_unet\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafcff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_tensorrt as torchtrt\n",
    "from torch_tensorrt.fx.tools.trt_splitter import TRTSplitter, TRTSplitterSetting\n",
    "\n",
    "import nos.compilers.trt as nostrt\n",
    "import nos.compilers.trt.ops\n",
    "from nos.compilers.trt.ops import group_norm as gn\n",
    "import torch_tensorrt.fx.tracer.acc_tracer.acc_tracer as acc_tracer\n",
    "from torch_tensorrt.fx.utils import LowerPrecision\n",
    "from torch_tensorrt.dynamo.backend._defaults import (\n",
    "    PRECISION,\n",
    "    DEBUG,\n",
    "    MAX_WORKSPACE_SIZE,\n",
    "    MIN_BLOCK_SIZE,\n",
    "    PASS_THROUGH_BUILD_FAILURES,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe768a26-9579-4a79-88b2-14a1c8eddf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import fx\n",
    "from transformers.utils.fx import HFTracer, get_concrete_args\n",
    "from transformers import CLIPModel\n",
    "\n",
    "import torch_tensorrt.fx.converter_registry as registry\n",
    "registry.CONVERTERS.pop(acc_ops.expand)\n",
    "\n",
    "dtype = torch.float32\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").cuda().eval()\n",
    "model = model.vision_model\n",
    "if dtype == torch.float32: # CLIP is already in half-mode\n",
    "    model = model.float()\n",
    "    \n",
    "shape = [1, 3, 224, 224]\n",
    "x = torch.randn(shape, dtype=dtype, device='cuda')\n",
    "args = {\n",
    "    \"pixel_values\": x\n",
    "}\n",
    "concrete_args = {\n",
    "    \"output_attentions\": None,\n",
    "    \"output_hidden_states\": None,\n",
    "    \"return_dict\": True\n",
    "}\n",
    "inputs = list(args.values())\n",
    "\n",
    "tracer = HFTracer()\n",
    "traced_graph = tracer.trace(model, concrete_args=concrete_args, dummy_inputs=args)\n",
    "traced_model = torch.fx.GraphModule(model, traced_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb9569b-2754-4d8a-8001-67e448885be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "from diffusers import DDIMScheduler\n",
    "from diffusers.pipelines.stable_diffusion import StableDiffusionPipeline\n",
    "from diffusers.models.unet_2d_condition import UNet2DConditionOutput\n",
    "\n",
    "dtype = torch.float32\n",
    "# Use the DDIMScheduler scheduler here instead\n",
    "scheduler = DDIMScheduler.from_pretrained(\"stabilityai/stable-diffusion-2-1\",\n",
    "                                            subfolder=\"scheduler\")\n",
    "# pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\")\n",
    "pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\", torch_dtype=dtype, scheduler=scheduler)\n",
    "pipe = pipe.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb64ec0-645d-4ea2-9824-ad855862eb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import fx\n",
    "from transformers.utils.fx import HFTracer\n",
    "\n",
    "model = pipe.vae\n",
    "\n",
    "# https://github.com/huggingface/diffusers/blob/666743302ff5bd1e02c204b81a80e566648d60de/examples/community/stable_diffusion_tensorrt_txt2img.py#L493\n",
    "H, W = 512, 512Â \n",
    "B = 1\n",
    "x = torch.rand(B, 3, H // 8, W // 8, dtype=dtype, device=\"cuda\")\n",
    "args = {\n",
    "    'sample': x,\n",
    "}\n",
    "concrete_args = {\n",
    "    \"sample_posterior\": False,\n",
    "    \"return_dict\": True,\n",
    "    \"generator\": None,\n",
    "}\n",
    "inputs = list(args.values())\n",
    "\n",
    "tracer = HFTracer()\n",
    "with torch.inference_mode():\n",
    "    traced_graph = tracer.trace(model, concrete_args=concrete_args, dummy_inputs=args)\n",
    "    traced_model = torch.fx.GraphModule(model, traced_graph)\n",
    "# print(traced_graph.print_tabular())\n",
    "\n",
    "traced_model.config = model.config\n",
    "# The model class must be stored as an attribute to allow model deserialization, which uses trace, and thus\n",
    "# _generate_dummy_input, where the model class is needed.\n",
    "traced_model.class_for_deserialization = model.__class__\n",
    "traced_model.device = model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ed0667-d390-43a8-96fc-4d8f236c1dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import fx\n",
    "from transformers.utils.fx import HFTracer\n",
    "\n",
    "model = pipe.unet\n",
    "\n",
    "# https://github.com/huggingface/diffusers/blob/666743302ff5bd1e02c204b81a80e566648d60de/examples/community/stable_diffusion_tensorrt_txt2img.py#L493\n",
    "H, W = 512, 512\n",
    "B = 1\n",
    "D = 1024  # D = (v1-5: 768, v2-1: 1024)\n",
    "unet_dim = 4\n",
    "text_maxlen = 77\n",
    "\n",
    "device = \"cuda\"\n",
    "sample = torch.rand(2 * B, unet_dim, 96, 96, dtype=dtype, device=device)\n",
    "timestep = torch.tensor([1, ], dtype=torch.int32, device=device)\n",
    "encoder_hidden_states = torch.rand((2 * B, text_maxlen, D), dtype=dtype, device=device)  # 2B, 77, 1024\n",
    "\n",
    "args = {\n",
    "    'sample': sample,\n",
    "    'timestep' : timestep,\n",
    "    'encoder_hidden_states': encoder_hidden_states,\n",
    "}\n",
    "concrete_args = {\n",
    "    'class_labels': None,\n",
    "    'timestep_cond' : None,\n",
    "    'attention_mask': None,\n",
    "    'cross_attention_kwargs': None,\n",
    "    'added_cond_kwargs': None,\n",
    "    'down_block_additional_residuals': None,\n",
    "    'mid_block_additional_residual': None,\n",
    "    'encoder_attention_mask':None,\n",
    "    'return_dict': True,\n",
    "    'hidden_states': None,\n",
    "    'cond_proj_dim' : 1,\n",
    "    'condition': None,\n",
    "    'use_ada_layer_norm': True,\n",
    "    'input_ndim': 4,\n",
    "}\n",
    "inputs = list(args.values())\n",
    "\n",
    "tracer = HFTracer()\n",
    "# with torch.inference_mode():\n",
    "traced_graph = tracer.trace(model, concrete_args={**args, **concrete_args})\n",
    "traced_model = torch.fx.GraphModule(model, traced_graph)\n",
    "# print(traced_graph.print_tabular())\n",
    "\n",
    "traced_model.config = model.config\n",
    "# The model class must be stored as an attribute to allow model deserialization, which uses trace, and thus\n",
    "# _generate_dummy_input, where the model class is needed.\n",
    "traced_model.class_for_deserialization = model.__class__\n",
    "traced_model.device = model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6466b2-0cef-48e1-91bf-8347f773166c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(*inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd994148-1d16-4784-9192-005dad943111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "\n",
    "# class TracedModelWithNoInputs(nn.Module):\n",
    "#     def __init__(self, traced_model):\n",
    "#         super().__init__()\n",
    "#         self._model = traced_model\n",
    "#         self._input_idx2name = {}\n",
    "#         for key in dir(self._model):\n",
    "#             if key.startswith(\"_tensor_constant\"):\n",
    "#                 self._input_idx2name[len(self._input_idx2name)] = key\n",
    "#         print(f\"Found inputs: {len(self._input_idx2name)}\")\n",
    "\n",
    "#         self.config = None\n",
    "#         self.class_for_deserialization = None\n",
    "#         self.device = None\n",
    "        \n",
    "#     def forward(self, sample, timestep, encoder_hidden_states, class_labels=None, cross_attention_kwargs=None, return_dict=True):\n",
    "#         args = (sample, timestep, encoder_hidden_states)\n",
    "#         idx = 0\n",
    "#         for arg in args:\n",
    "#             key = self._input_idx2name[idx]\n",
    "#             setattr(self._model, key, arg)\n",
    "#             print(f\"setting {key}: {arg.shape}, old: {getattr(self._model, key).shape}\")\n",
    "#             idx += 1\n",
    "#         # for _key, value in kwargs.items():\n",
    "#         #     print(f\"kwarg: {_key}\")\n",
    "#         #     if idx not in self._input_idx2name:\n",
    "#         #         break\n",
    "#         #     key = self._input_idx2name[idx]\n",
    "#         #     setattr(self._model, key, value)\n",
    "#         #     print(f\"setting {key}: {value.shape}\")\n",
    "#         #     idx += 1\n",
    "#         return UNet2DConditionOutput(self._model())\n",
    "        \n",
    "# traced_unet = TracedModelWithNoInputs(traced_model)\n",
    "# traced_unet.config = pipe.unet.config\n",
    "# # The model class must be stored as an attribute to allow model deserialization, which uses trace, and thus\n",
    "# # _generate_dummy_input, where the model class is needed.\n",
    "# traced_unet.class_for_deserialization = pipe.unet.__class__\n",
    "# traced_unet.device = pipe.unet.device\n",
    "# pipe.unet = traced_unet\n",
    "# # with torch.inference_mode():\n",
    "# #     result = traced_unet(*inputs)\n",
    "# # result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6fb606-3c89-4cb1-bf19-90b5e401bd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = pipe(prompt=[\"fox jumped over dog\"], num_inference_steps=5, num_images_per_prompt=1).images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c01386-a9c0-47b5-bbd8-400a5953fa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f62053c-7300-4936-a5c0-fefc6fa972a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Sequence\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch._dynamo as td\n",
    "from torch_tensorrt.dynamo.backend.lowering._decompositions import (\n",
    "    get_decompositions,\n",
    ")\n",
    "from torch_tensorrt.dynamo.backend.lowering._partition import (\n",
    "    partition,\n",
    "    get_submod_inputs,\n",
    ")\n",
    "from torch_tensorrt.dynamo.backend._settings import CompilationSettings\n",
    "from torch_tensorrt.dynamo.backend.utils import prepare_inputs, prepare_device\n",
    "from torch_tensorrt.dynamo.backend.backends import aot_torch_tensorrt_aten_backend\n",
    "from torch_tensorrt.dynamo.backend._defaults import (\n",
    "    PRECISION,\n",
    "    DEBUG,\n",
    "    MAX_WORKSPACE_SIZE,\n",
    "    MIN_BLOCK_SIZE,\n",
    "    PASS_THROUGH_BUILD_FAILURES,\n",
    ")\n",
    "from torch._dynamo.backends.common import fake_tensor_unsupported\n",
    "from torch._functorch.aot_autograd import aot_module_simplified, make_boxed_compiler\n",
    "from torch_tensorrt.dynamo.backend.lowering._partition import logger as plogger\n",
    "from torch_tensorrt.dynamo.backend.conversion import convert_module\n",
    "\n",
    "from nos.compilers.trt.ops import group_norm as gn\n",
    "# from nos.compilers.trt.backends import nos_tensorrt_backend\n",
    "\n",
    "\n",
    "from torch._functorch.aot_autograd import aot_module_simplified, make_boxed_compiler\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def _compile_module(\n",
    "    gm: torch.fx.GraphModule,\n",
    "    sample_inputs: Sequence[torch.Tensor],\n",
    "    settings: CompilationSettings = CompilationSettings(),\n",
    ") -> torch.fx.GraphModule:\n",
    "    \"\"\"Compile a traced FX module\n",
    "\n",
    "    Includes: Partitioning + Conversion Phases\n",
    "\n",
    "    Args:\n",
    "        module: FX GraphModule to convert\n",
    "        inputs: Inputs to the module\n",
    "        settings: Compilation settings\n",
    "    Returns:\n",
    "        Compiled FX GraphModule\n",
    "    \"\"\"\n",
    "    # Partition module into components that can be TRT-accelerated\n",
    "    print(f\"NOS compile:\")\n",
    "    level = plogger.level\n",
    "    plogger.setLevel(logging.DEBUG)\n",
    "    partitioned_module = partition(\n",
    "        gm,\n",
    "        verbose=settings.debug,\n",
    "        min_block_size=settings.min_block_size,\n",
    "        torch_executed_ops=settings.torch_executed_ops,\n",
    "    )\n",
    "    plogger.setLevel(level)\n",
    "\n",
    "    # Iterate over all components that can be accelerated\n",
    "    # Generate the corresponding TRT Module for those\n",
    "    for name, _ in partitioned_module.named_children():\n",
    "        # TOFIX (spillai): Investigate why this is needed here, \n",
    "        # but not in torch dynamo.\n",
    "        if \"_run_on_acc\" not in name:\n",
    "            print(f\"Skipping partition: {name}\")\n",
    "            continue\n",
    "        submodule = getattr(partitioned_module, name)\n",
    "\n",
    "        # Get submodule inputs\n",
    "        submodule_inputs = get_submod_inputs(\n",
    "            partitioned_module, submodule, sample_inputs\n",
    "        )\n",
    "\n",
    "        # Create TRT Module from submodule\n",
    "        trt_mod = convert_module(\n",
    "            submodule,\n",
    "            submodule_inputs,\n",
    "            settings=settings,\n",
    "        )\n",
    "\n",
    "        # Replace FX Module with TRT Module\n",
    "        setattr(partitioned_module, name, trt_mod)\n",
    "\n",
    "    return partitioned_module\n",
    "\n",
    "@fake_tensor_unsupported\n",
    "def _pretraced_backend(\n",
    "    gm: torch.fx.GraphModule,\n",
    "    sample_inputs: Sequence[torch.Tensor],\n",
    "    settings: CompilationSettings = CompilationSettings(),\n",
    "):\n",
    "    \"\"\"Helper function to manage translation of traced FX module to TRT engines\n",
    "\n",
    "    Args:\n",
    "        module: FX GraphModule to convert\n",
    "        inputs: Inputs to the module\n",
    "        settings: Compilation settings\n",
    "    Returns:\n",
    "        Compiled FX GraphModule\n",
    "    \"\"\"\n",
    "    try:\n",
    "        trt_compiled = _compile_module(\n",
    "            gm,\n",
    "            sample_inputs,\n",
    "            settings=settings,\n",
    "        )\n",
    "        return trt_compiled\n",
    "    except:\n",
    "        logger.error(\n",
    "            \"FX2TRT conversion failed on the subgraph. See trace above. \"\n",
    "            + \"Returning GraphModule forward instead.\",\n",
    "            exc_info=True,\n",
    "        )\n",
    "\n",
    "        if not settings.pass_through_build_failures:\n",
    "            return gm.forward\n",
    "        else:\n",
    "            raise AssertionError(\n",
    "                \"Halting compilation on build failure since \"\n",
    "                + \"pass_through_build_failures was specified as True. \"\n",
    "                + \"To return the default Torch implementation and avoid \"\n",
    "                + \"halting compilation on engine build failures, \"\n",
    "                + \"specify pass_through_build_failures=False.\"\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "@fake_tensor_unsupported\n",
    "def nos_tensorrt_backend(\n",
    "    gm: torch.fx.GraphModule,\n",
    "    sample_inputs: Sequence[torch.Tensor],\n",
    "    settings: CompilationSettings = CompilationSettings(),\n",
    "):\n",
    "    custom_backend = partial(\n",
    "        _pretraced_backend,\n",
    "        settings=settings,\n",
    "    )\n",
    "\n",
    "    print(\"nos_trt_backend() called with FX graph:\")\n",
    "    gm.graph.print_tabular()\n",
    "\n",
    "    # print(\"Acc Tracer\")\n",
    "    # with torch.inference_mode():\n",
    "    #     gm = acc_tracer.trace(\n",
    "    #         gm, sample_inputs, \n",
    "    #     )\n",
    "\n",
    "    # Invoke AOTAutograd to translate operators to aten\n",
    "    return _pretraced_backend(gm, sample_inputs, settings=settings)\n",
    "    \n",
    "    # return aot_module_simplified(\n",
    "    #     gm,\n",
    "    #     sample_inputs,\n",
    "    #     fw_compiler=make_boxed_compiler(custom_backend),\n",
    "    #     decompositions=get_decompositions(),\n",
    "    # )\n",
    "\n",
    "\n",
    "def create_backend(\n",
    "    precision: LowerPrecision = PRECISION,\n",
    "    debug: bool = DEBUG,\n",
    "    workspace_size: int = MAX_WORKSPACE_SIZE,\n",
    "    min_block_size: int = MIN_BLOCK_SIZE,\n",
    "    torch_executed_ops: Sequence[str] = set(),\n",
    "    pass_through_build_failures: bool = PASS_THROUGH_BUILD_FAILURES,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"Create torch.compile backend given specified arguments\n",
    "\n",
    "    Args:\n",
    "        precision: Model Layer precision\n",
    "        debug: Whether to print out verbose debugging information\n",
    "        workspace_size: Workspace TRT is allowed to use for the module (0 is default)\n",
    "        min_block_size: Minimum number of operators per TRT-Engine Block\n",
    "        torch_executed_ops: Sequence of operations to run in Torch, regardless of converter coverage\n",
    "        pass_through_build_failures: Whether to fail on TRT engine build errors (True) or not (False)\n",
    "    Returns:\n",
    "        Backend for torch.compile\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    settings = CompilationSettings(\n",
    "        debug=debug,\n",
    "        precision=precision,\n",
    "        workspace_size=workspace_size,\n",
    "        min_block_size=min_block_size,\n",
    "        torch_executed_ops=torch_executed_ops,\n",
    "        pass_through_build_failures=pass_through_build_failures,\n",
    "    )\n",
    "\n",
    "    return partial(\n",
    "        nos_tensorrt_backend,\n",
    "        settings=settings,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fcd00d-4f62-40b6-89e4-e2139826986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import logging\n",
    "import collections.abc\n",
    "import torch_tensorrt\n",
    "from functools import partial\n",
    "\n",
    "from typing import Any, Sequence\n",
    "from torch_tensorrt import EngineCapability, Device\n",
    "from torch_tensorrt.fx.utils import LowerPrecision\n",
    "\n",
    "\n",
    "torch._dynamo.config.log_level = logging.INFO\n",
    "torch._dynamo.config.verbose = True\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "debug = True\n",
    "custom_backend = create_backend(\n",
    "    precision=LowerPrecision.FP32 if dtype == torch.float32 else LowerPrecision.FP16,\n",
    "    debug=debug,\n",
    "    workspace_size=MAX_WORKSPACE_SIZE,\n",
    "    min_block_size=0, # MIN_BLOCK_SIZE,\n",
    "    torch_executed_ops=[],\n",
    ")\n",
    "\n",
    "torch._dynamo.eval_frame.remove_from_cache(model)\n",
    "with torch.inference_mode():\n",
    "    trt_model = torch_tensorrt.compile(traced_unet, backend=custom_backend)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    trt_model(x)\n",
    "\n",
    "# Clean up model env\n",
    "# torch._dynamo.reset()\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27220470-7ad7-4399-a9ca-73a856079776",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_model = torch_tensorrt.fx.compile(traced_model, \n",
    "                                    inputs,\n",
    "                                    min_acc_module_size=0,\n",
    "                                    max_batch_size=2048,\n",
    "                                    max_workspace_size=33554432,\n",
    "                                    lower_precision=LowerPrecision.FP32 if dtype == torch.float32 else LowerPrecision.FP16,\n",
    "                                    verbose_log=True,\n",
    "                                    timing_cache_prefix='',\n",
    "                                    save_timing_cache=False,\n",
    "                                    cuda_graph_batch_size=-1,\n",
    "                                    is_aten=False,\n",
    "                                    use_experimental_fx_rt=False,\n",
    "                                    explicit_batch_dimension=True, \n",
    "                                    dynamic_batch=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e771b1-cce8-4aa4-ac21-e50c98f52ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.vae = trt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3a70f2-dd3d-4aa8-a62c-faf857baff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = pipe(prompt=[\"fox jumped over dog\"], num_inference_steps=200, num_images_per_prompt=1).images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f08dc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"norm_trt_engine.pt\"\n",
    "torch.save(trt_model, filename)\n",
    "trt_model = torch.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c779700",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    result = model(x)\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d768f147",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    result = trt_model(x)\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b488f8-ae28-4f63-8dcf-59494be04ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 10 -r 3\n",
    "with torch.inference_mode():\n",
    "    model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee82eba1-4cb0-41d5-9c5e-8d62e04bf0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 10 -r 3\n",
    "with torch.inference_mode():\n",
    "    trt_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ebc39d-5a5a-42f5-bac4-2685c525980e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_mod(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb97731-bc58-47a9-aee0-b83cdd11e6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
