{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1afa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01648e1a",
   "metadata": {},
   "source": [
    "# nos üî•: **N**itrous **O**xide **S**ystem for Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1eea8d",
   "metadata": {},
   "source": [
    "**NOS** is a PyTorch library for optimizing and running lightning-fast inference of popular computer vision models. NOS inherits its name from \"Nitrous Oxide System\", the performance-enhancing system typically used in racing cars. NOS is designed to be modular and easy to extend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb9df4f",
   "metadata": {},
   "source": [
    "## Why NOS?\n",
    "- ‚ö°Ô∏è **Fast**: Built for PyTorch and designed to optimize/run models faster\n",
    "- üî• **Performant**: Run models such as SDv2 or object detection 2-3x faster out-of-the-box\n",
    "- üë©‚Äçüíª **No PhD required**: Optimize models for maximum HW performance without a PhD in ML\n",
    "- üì¶ **Extensible**: Easily add optimization and HW-support for custom models\n",
    "- ‚öôÔ∏è **HW-accelerated:** Take full advantage of your HW (GPUs, ASICs) without compromise\n",
    "- ‚òÅÔ∏è **Cloud-agnostic:** Run on any cloud HW (AWS, GCP, Azure, Lambda Labs, On-Prem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b5f7d4",
   "metadata": {},
   "source": [
    "## Batteries Included\n",
    " - üí™ **SOTA Model Support:** NOS provides out-of-the-box support for popular CV models such as [Stable Diffusion](stabilityai/stable-diffusion-2), [OpenAI CLIP](openai/clip-vit-base-patch32), [OpenMMLab](https://github.com/open-mmlab/) object detection, tracking and more\n",
    " - üîå **APIs:** NOS provides out-of-the-box APIs and avoids all the ML model deployment hassles\n",
    " - üê≥ **Docker:** NOS ships with docker images to run accelerated and scalable CV workloads\n",
    " - üìà **Multi-Platform**: NOS allows you to run models on different HW (NVIDIA, custom ASICs) without any model compilation or runtime management."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e449eb19",
   "metadata": {},
   "source": [
    "## 0. Installation\n",
    "\n",
    "```bash\n",
    "pip install autonomi-nos[torch]\n",
    "```\n",
    "\n",
    "Alternatively, if you have `torch` already installed, you can simply run:\n",
    "```bash\n",
    "pip install autonomi-nos\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da9d607",
   "metadata": {},
   "source": [
    "### 1. Quickstart\n",
    "\n",
    "NOS uses a client-server architecture to run inference. The NOS server is a gRPC server that runs on your local machine or in the cloud. The NOS client is a Python SDK that allows you to send inference requests to the NOS server. Let's get started by running the NOS server locally.\n",
    "\n",
    "```bash\n",
    "nos docker start --runtime [cpu/gpu/trt]\n",
    "```\n",
    "\n",
    "This will start the NOS server in a docker container. You can now run inference requests using the NOS client. You have the option to run 3 different server runtimes:\n",
    " - `cpu`: Runs the NOS server on CPU (all models run on CPU with PyTorch)\n",
    " - `gpu`: Runs the NOS server on GPU (all models run on GPU with PyTorch)\n",
    " - `trt`: Runs the NOS server on GPU (all models run on GPU with PyTorch + [TensorRT](https://developer.nvidia.com/tensorrt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf2df73",
   "metadata": {},
   "source": [
    "### 1a. Quickstart (via `docker compose` or `docker run`)\n",
    "\n",
    "You can also get started with NOS using our docker server via:\n",
    "```bash\n",
    "docker run -it \\\n",
    "    -p 50051:50051 \\\n",
    "    -v ~/.nosd:/app/.nos \\\n",
    "    --shm-size 4g \\\n",
    "    autonomi/nos:latest-cpu\n",
    "```\n",
    "\n",
    "Alternatively, you can also start the nos docker server as a service within a docker compose YAML. Navigate to the `examples/quickstart` folder and run the NOS server via:\n",
    "\n",
    "```bash\n",
    "docker compose -f docker-compose.quickstart.yml up\n",
    "```\n",
    "\n",
    "Let's inspect the docker-compose file to understand what's going on:\n",
    "```yaml\n",
    "version: \"3.8\"\n",
    "\n",
    "services:\n",
    "  nos-server:\n",
    "    image: autonomi/nos:latest-cpu\n",
    "    command: nos-grpc-server\n",
    "    ports:\n",
    "      - 50051:50051\n",
    "    environment:\n",
    "      - NOS_HOME=/app/.nos\n",
    "      - NOS_LOGGING_LEVEL=ERROR\n",
    "    volumes:\n",
    "      - ~/.nosd:/app/.nos\n",
    "    deploy:\n",
    "      resources:\n",
    "        limits:\n",
    "          cpus: \"6\"\n",
    "          memory: 6G\n",
    "```\n",
    "\n",
    "We first spin up a `nos-server` service mounting the necessary host directories (`~/.nosd`) and exposing the gRPC port. The command `nos-grpc-server` spins up the gRPC server with the default 50051 port that can be used to send inference requests. The `NOS_HOME` directory is set to `/app/.nos` where all the models and optimization artifacts are stored. This directory is mounted on your host machine at `~/.nosd`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c17875",
   "metadata": {},
   "source": [
    "## Inference Client SDK\n",
    "\n",
    "\n",
    "Once the NOS server is running, you can send inference requests using the NOS client SDK. To double-check if your nos server has started successfully, you can run `docker ps | grep autonomi/nos` to check the status of your server.\n",
    "\n",
    "Let's start by importing the NOS client SDK and creating a client instance. The client instance is used to send inference requests to the NOS server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ae8bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nos.client import InferenceClient, TaskType\n",
    "\n",
    "# Create a client that connects to the inference server via gRPC (50051)\n",
    "client = InferenceClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5537b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We provide helper functions to wait for the server to be ready\n",
    "# if the server is simultaneously spun up in a separate process.\n",
    "client.WaitForServer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abc3b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we can check if the server is healthy.\n",
    "client.IsHealthy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8930d090",
   "metadata": {},
   "source": [
    "### List all available models\n",
    "\n",
    "Now that we have a client instance, let's list all the available models that are supported by NOS. You can use the `ListModels()` method to list all the available models. Here, we render the list of models as a Pandas dataframe for easy viewing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9054cb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "\n",
    "models = client.ListModels()\n",
    "models_df = pd.DataFrame(models)\n",
    "models_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802bb542",
   "metadata": {},
   "source": [
    "### Get model input/output signatures\n",
    "\n",
    "In order to run inference, we need to know the input and output signatures of the model. We can use the `GetModelInfo()` method to get the full model specification (`task`, `model_name`, `signature` with `inputs` and `outputs`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae711641",
   "metadata": {},
   "outputs": [],
   "source": [
    "signatures = [client.GetModelInfo(spec).signature for spec in models]\n",
    "pd.DataFrame(signatures, index=[(m.name, m.task) for m in models])[[\"inputs\", \"outputs\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2271ff2",
   "metadata": {},
   "source": [
    "## 2. Run Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5f164d",
   "metadata": {},
   "source": [
    "Now, we're ready to run inference using our client. First, let's load an image for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e388b67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/open-mmlab/mmdetection/main/demo/demo.jpg\"\n",
    "img = Image.open(requests.get(url, stream=True).raw).resize((640, 480))\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c8d647",
   "metadata": {},
   "source": [
    "### 2a. Inference: Object Detection with YOLOX (in < 5 lines)\n",
    "\n",
    "Here, we will use the `client.Run` API to run inference on the YOLOX object detection model. See the [`client.Run(...)`](https://autonomi-ai.github.io/nos/docs/api/client/#nos.client.grpc.InferenceClient.Run) documentation for more details.\n",
    "\n",
    "*Note:* In the first call of the `client.Run(...)` API, the model is loaded from the model repository and optimized for the first time. This may take a few seconds. Subsequent calls to `client.Run(...)` will be much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26edddc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def visualize_det2d(img: np.ndarray, bboxes: np.ndarray, labels: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Visualize 2D detection results on an image.\"\"\"\n",
    "    vis = np.asarray(img).copy()\n",
    "    for bbox, label in zip(bboxes.astype(np.int32), labels):\n",
    "        cv2.rectangle(vis, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)\n",
    "    return vis\n",
    "\n",
    "# Run YOLOX prediction on the image and get the prediction results as a dictionary.\n",
    "# predictions = {\"bboxes\", \"scores\", \"labels\"}.\n",
    "predictions = client.Run(TaskType.OBJECT_DETECTION_2D, \"yolox/nano\", images=[img])\n",
    "for idx, (img, bboxes, scores, labels) in enumerate(zip([img], predictions[\"bboxes\"], predictions[\"scores\"], predictions[\"scores\"])):\n",
    "    display(Image.fromarray(visualize_det2d(img, bboxes, labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b6f397",
   "metadata": {},
   "source": [
    "### 2b. Benchmark Object Detection\n",
    "\n",
    "Now that we've run inference on a single image, let's run inference on a batch of images and benchmark the performance. We will run `client.Run` repeatedly on an image (`batch_size = 1`) and measure the average latency. You can also modify the code above to increase the batch size and measure the latency for larger batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc9e059",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 10\n",
    "predictions = client.Run(TaskType.OBJECT_DETECTION_2D, \"yolox/nano\", images=[img])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4a2b96",
   "metadata": {},
   "source": [
    "## 3. `client.Module` Interface\n",
    "\n",
    "Instead of calling `client.Run` specifying task and model names repeatedly, we can also use the `client.Module` interface to get model handles for simpler remote-model execution. In this case, the following line re-uses the same `yolox/nano` model instantiated earlier as creates a handle called `det2d` that can be used by the client to get object detection results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1e557f",
   "metadata": {},
   "outputs": [],
   "source": [
    "detect2d = client.Module(TaskType.OBJECT_DETECTION_2D, \"yolox/nano\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775468d0",
   "metadata": {},
   "source": [
    "Here's an example of an object detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bfa06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = detect2d(images=[img])\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a507f84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from itertools import islice\n",
    "from nos.common import tqdm\n",
    "from nos.common.io import VideoReader, VideoWriter\n",
    "\n",
    "# Download a test video for inference\n",
    "filename = \"test_video.mp4\"\n",
    "URL = \"https://zackakil.github.io/video-intelligence-api-visualiser/assets/test_video.mp4\"\n",
    "if not Path(filename).exists():\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(requests.get(URL).content)\n",
    "\n",
    "# Initialize detection module\n",
    "MODEL_NAME = \"yolox/nano\"\n",
    "detect2d = client.Module(TaskType.OBJECT_DETECTION_2D, MODEL_NAME)\n",
    "\n",
    "# Run inference on every 500th frame, and display the results\n",
    "output_filename = \"test_video_out.mp4\"\n",
    "writer = VideoWriter(, fps=30)\n",
    "for img in tqdm(islice(VideoReader(filename), 0, None, 10)):\n",
    "    H, W = img.shape[:2]\n",
    "    img = cv2.resize(img, (640, 480))\n",
    "    predictions = detect2d(images=[img])\n",
    "    for idx, (img, bboxes, scores, labels) in enumerate(zip([img], predictions[\"bboxes\"], predictions[\"scores\"], predictions[\"scores\"])):\n",
    "        vis = visualize_det2d(img, bboxes, labels)\n",
    "        vis = cv2.resize(vis, (W, H))\n",
    "        writer.write(vis)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b42f46c",
   "metadata": {},
   "source": [
    "Let's visualize the results of the inference below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab6392c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "Video(output_filename, width=W, height=H, embed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49c3644",
   "metadata": {},
   "source": [
    "## 4. NOS Model Hub\n",
    "\n",
    "Let's try a few more models that come pre-packaged with NOS. You can find the full list of models in the [NOS Model Hub]()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b8aa58",
   "metadata": {},
   "source": [
    " ### 4a. Text-to-image generation with [StableDiffusionV2](https://huggingface.co/stabilityai/stable-diffusion-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8417340",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"fox jumped over the moon\", \"fox jumped over the sun\"]\n",
    "predictions = client.Run(TaskType.IMAGE_GENERATION, \"stabilityai/stable-diffusion-2\", \n",
    "                         prompts=prompts, width=512, height=512, num_images=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e8219d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt, image in zip(prompts, predictions[\"images\"]):\n",
    "    print(prompt, image.size)\n",
    "    display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcd0dc5",
   "metadata": {},
   "source": [
    "### 4b. Run image-embedding with [OpenAI CLIP](https://huggingface.co/openai/clip-vit-base-patch32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbdc8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = client.Run(TaskType.IMAGE_EMBEDDING, \"openai/clip\", images=[img, img])\n",
    "predictions[\"embedding\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333b5bd0",
   "metadata": {},
   "source": [
    "### 4c. Run text-embedding with [OpenAI CLIP](https://huggingface.co/openai/clip-vit-base-patch32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe462f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = client.Run(TaskType.TEXT_EMBEDDING, \"openai/clip\", texts=[\"fox jumped over the mooon\", \"fox jumped over the sun\"])\n",
    "predictions[\"embedding\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8646f4be",
   "metadata": {},
   "source": [
    "Now that you have familiarized yourself with the NOS client SDK, let's dig a bit deeper into how NOS can accelerate your existing computer-vision workloads. Navigate to the [inference-acceleration-example notebook](./inference-acceleration-example.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf6cef1",
   "metadata": {},
   "source": [
    "### 5. Runnning Optimized Models\n",
    "\n",
    "NOS also comes baked with a number of pre-optimized models that you can use out-of-the-box. Let's try running inference on the pre-optimized CLIP model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3867e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model, and run inference once to warm up the model\n",
    "predictions = client.Run(TaskType.OBJECT_DETECTION_2D, \"yolox/small\", images=[img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e837ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 10\n",
    "predictions = client.Run(TaskType.OBJECT_DETECTION_2D, \"yolox/small\", images=[img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b493ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the optimized model, and run inference once to warm up the model\n",
    "predictions = client.Run(TaskType.OBJECT_DETECTION_2D, \"yolox/small-trt\", images=[img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fe35d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 10\n",
    "predictions = client.Run(TaskType.OBJECT_DETECTION_2D, \"yolox/small-trt\", images=[img])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
