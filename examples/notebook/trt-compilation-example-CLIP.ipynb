{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcd9b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers diffusers optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abda0b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; print(os.getenv(\"CUDA_MODULE_LOADING\", None))\n",
    "print(os.getenv(\"HF_HOME\"))\n",
    "print(os.getenv(\"TRANSFORMERS_CACHE\"))\n",
    "print(os.getenv(\"TORCH_HOME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36786c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.fx\n",
    "import torch.nn as nn\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82acf812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt\n",
    "print(tensorrt.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8323a699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_tensorrt\n",
    "print(torch_tensorrt.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4efb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_tensorrt\n",
    "from torch_tensorrt.fx.utils import LowerPrecision\n",
    "import torch_tensorrt.fx.tracer.acc_tracer.acc_tracer as acc_tracer\n",
    "from torch_tensorrt.fx import InputTensorSpec, TRTInterpreter, TRTModule\n",
    "from torch_tensorrt.fx.tools.trt_splitter import TRTSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97a6358",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_tensorrt.fx.tracer.acc_tracer import acc_ops\n",
    "import torch_tensorrt.fx.converter_registry as registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d66024f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc_ops.expand in registry.CONVERTERS.keys())\n",
    "registry.CONVERTERS.pop(acc_ops.expand)\n",
    "print(acc_ops.expand in registry.CONVERTERS.keys())\n",
    "# for k in registry.CONVERTERS.keys():\n",
    "#     print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbdf68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ecf3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from transformers import AutoProcessor, CLIPVisionModel\n",
    "\n",
    "\n",
    "# processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# CLIPModel: txt + vision\n",
    "# model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").cuda().eval()\n",
    "\n",
    "# CLIPVIsionModel: vision\n",
    "model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\").cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94757098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPModel\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").cuda().eval()\n",
    "model = model.vision_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c50698",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a44cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "# CLIP\n",
    "inputs = [torch.randn((1, 3, 224, 224), dtype=torch.float32, device='cuda')]\n",
    "\n",
    "# >> Inference\n",
    "with torch.inference_mode():\n",
    "    output = model(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a46cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import fx\n",
    "from transformers.utils.fx import HFTracer, get_concrete_args\n",
    "\n",
    "\n",
    "input_names = list([\"pixel_values\"])\n",
    "concrete_args = get_concrete_args(model, input_names)\n",
    "concrete_args\n",
    "\n",
    "args = {\n",
    "    \"pixel_values\": torch.randn((1, 3, 224, 224), dtype=torch.float32, device='cuda')\n",
    "}\n",
    "concrete_args = {\n",
    "    \"output_attentions\": None,\n",
    "    \"output_hidden_states\": None,\n",
    "    \"return_dict\": True\n",
    "}\n",
    "\n",
    "inputs = [\n",
    "    args[\"pixel_values\"], \n",
    "]\n",
    "\n",
    "tracer = HFTracer()\n",
    "traced_graph = tracer.trace(model, concrete_args=concrete_args, dummy_inputs=args)\n",
    "traced_model = torch.fx.GraphModule(model, traced_graph)\n",
    "\n",
    "# traced_model.config = model.config\n",
    "# # The model class must be stored as an attribute to allow model deserialization, which uses trace, and thus\n",
    "# # _generate_dummy_input, where the model class is needed.\n",
    "# traced_model.class_for_deserialization = model.__class__\n",
    "# # traced_model.device = model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4008ab5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMERS FX Tracing\n",
    "from transformers.utils.fx import symbolic_trace\n",
    "\n",
    "# For CLIPModel: input_names=[\"input_ids\", \"pixel_values\"]\n",
    "# For CLIPVIsionModel: input_names=[\"pixel_values\"]\n",
    "with torch.inference_mode():\n",
    "    traced = symbolic_trace(\n",
    "        model, input_names=[\"pixel_values\"],\n",
    "        disable_check=True,\n",
    "    )\n",
    "type(traced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac89a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a2c7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.clip.modeling_clip import CLIPVisionTransformer, CLIPAttention, CLIPVisionEmbeddings, CLIPEncoder\n",
    "\n",
    "# acc_tracer is a custom fx tracer that maps nodes whose targets are PyTorch operators\n",
    "# to acc ops.\n",
    "with torch.inference_mode():\n",
    "    trt_traced = acc_tracer.trace(\n",
    "        traced_model, inputs, \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631af132",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_traced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81629e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitter will split the model into several submodules. The name of submodules will\n",
    "# be either `run_on_acc_{}` or `run_on_gpu_{}`. Submodules named `run_on_acc_{}` can\n",
    "# be fully lowered to TensorRT via fx2trt while submodules named `run_on_gpu_{}` has\n",
    "# unsupported ops and can't be lowered by fx2trt. We can still run `run_on_gpu_{}`\n",
    "# submodules on Gpu if ops there have cuda implementation, the naming is a bit\n",
    "# confusing and we'll improve it.\n",
    "splitter = TRTSplitter(trt_traced, inputs)\n",
    "splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ba4eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview functionality allows us to see what are the supported ops and unsupported\n",
    "# ops. We can optionally the dot graph which will color supported ops and unsupported\n",
    "# ops differently.\n",
    "_ = splitter.node_support_preview(dump_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb84e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Non Acc Nodes\")\n",
    "print(splitter.non_acc_submodule_name)\n",
    "\n",
    "# splitter.sample_input\n",
    "# splitter.split_preview()\n",
    "# dir(splitter)\n",
    "# print(\"Acc Nodes\")\n",
    "# print(splitter.acc_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66266ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split.\n",
    "split_mod = splitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf0c2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(split_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9336f6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After split we have three submodules, _run_on_acc_0 and _run_on_gpu_1.\n",
    "print(split_mod.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d62e499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_submod_inputs(_mod, _submod, _inputs):\n",
    "    acc_inputs = None\n",
    "\n",
    "    def get_input(self, __inputs):\n",
    "        nonlocal acc_inputs\n",
    "        acc_inputs = __inputs\n",
    "\n",
    "    handle = _submod.register_forward_pre_hook(get_input)\n",
    "    # with torch.inference_mode():\n",
    "    _mod(*_inputs)\n",
    "    handle.remove()\n",
    "    return acc_inputs\n",
    "\n",
    "\n",
    "# Since the model is splitted into three segments. We need to lower each TRT eligible segment.\n",
    "# If we know the model can be fully lowered, we can skip the splitter part.\n",
    "for name, _ in split_mod.named_children():\n",
    "    print(f\"Splitting {name}\")\n",
    "    if \"_run_on_acc\" in name:\n",
    "        submod = getattr(split_mod, name)\n",
    "\n",
    "        # Get submodule inputs for fx2trt\n",
    "        acc_inputs = get_submod_inputs(split_mod, submod, inputs)\n",
    "        # print(f\"submod: {submod}\")\n",
    "        # print(f\"name: {name}\")\n",
    "\n",
    "        # fx2trt replacement\n",
    "        interp = TRTInterpreter(\n",
    "            submod,\n",
    "            InputTensorSpec.from_tensors(acc_inputs),\n",
    "            explicit_batch_dimension=True,\n",
    "        )\n",
    "        r = interp.run(lower_precision=LowerPrecision.FP32)\n",
    "        trt_mod = TRTModule(*r)\n",
    "        setattr(split_mod, name, trt_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b544d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# from nos.constants import NOS_MODELS_DIR\n",
    "\n",
    "# model_dir = Path(NOS_MODELS_DIR, f\"cache/{MODEL_NAME}\")\n",
    "# model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# lowered_model_output = split_mod(*inputs)\n",
    "\n",
    "# Save and load model\n",
    "# W, H = 224, 224\n",
    "# model_id = MODEL_NAME.replace(\"/\", \"-\") + \"_\" + f\"{W}x{H}\" + \"_\" + \"fp16\"\n",
    "# filename = f\"{model_dir}/{model_id}.torchtrt.pt\"\n",
    "filename = \"clip_224x224.torchtrt.pt\"\n",
    "torch.save(split_mod, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1c059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading torchtrt filename: {filename}\")\n",
    "reload_trt_mod = torch.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b413f4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    reload_model_output = reload_trt_mod(*inputs)\n",
    "reload_model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e65337a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the results match\n",
    "with torch.inference_mode():\n",
    "    regular_model_output = model(*inputs)\n",
    "dict(regular_model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4886341",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 100\n",
    "with torch.inference_mode():\n",
    "    _ = model(*inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4727827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 100\n",
    "with torch.inference_mode():\n",
    "    _ = reload_trt_mod(*inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01fbdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for o1, o2 in zip(reload_model_output, regular_model_output):\n",
    "    torch.testing.assert_close(\n",
    "        o1.cpu().float(), o2.cpu().float(), rtol=2e-02, atol=2e-02, equal_nan=True\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
