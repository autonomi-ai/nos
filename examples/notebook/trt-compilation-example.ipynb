{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4ee344",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install loguru opencv-python-headless tabulate netron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558ea5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install diffusers transformers onnx\n",
    "!pip install --upgrade colored polygraphy>=0.47.0 onnx-graphsurgeon --extra-index-url https://pypi.ngc.nvidia.com\n",
    "!pip install onnx==1.13.1 onnxruntime==1.14.1\n",
    "!pip install git+https://github.com/microsoft/onnx-script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36786c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.fx\n",
    "import torch.nn as nn\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9281950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxscript\n",
    "from onnxscript.onnx_opset import opset17 as op\n",
    "\n",
    "custom_opset = onnxscript.values.Opset(domain=\"torch.onnx\", version=1)\n",
    "\n",
    "\n",
    "@onnxscript.script(custom_opset)\n",
    "def ScaledDotProductAttention(\n",
    "    query,\n",
    "    key,\n",
    "    value,\n",
    "    dropout_p,\n",
    "):\n",
    "    # Swap the last two axes of key\n",
    "    key_shape = op.Shape(key)\n",
    "    key_last_dim = key_shape[-1:]\n",
    "    key_second_last_dim = key_shape[-2:-1]\n",
    "    key_first_dims = key_shape[:-2]\n",
    "    # Contract the dimensions that are not the last two so we can transpose\n",
    "    # with a static permutation.\n",
    "    key_squeezed_shape = op.Concat(\n",
    "        op.Constant(value_ints=[-1]), key_second_last_dim, key_last_dim, axis=0\n",
    "    )\n",
    "    key_squeezed = op.Reshape(key, key_squeezed_shape)\n",
    "    key_squeezed_transposed = op.Transpose(key_squeezed, perm=[0, 2, 1])\n",
    "    key_transposed_shape = op.Concat(key_first_dims, key_last_dim, key_second_last_dim, axis=0)\n",
    "    key_transposed = op.Reshape(key_squeezed_transposed, key_transposed_shape)\n",
    "\n",
    "    embedding_size = op.CastLike(op.Shape(query)[-1], query)\n",
    "    scale = op.Div(1.0, op.Sqrt(embedding_size))\n",
    "\n",
    "    # https://github.com/pytorch/pytorch/blob/12da0c70378b5be9135c6fda62a9863bce4a4818/aten/src/ATen/native/transformers/attention.cpp#L653\n",
    "    # Scale q, k before matmul for stability see https://tinyurl.com/sudb9s96 for math\n",
    "    query_scaled = op.Mul(query, op.Sqrt(scale))\n",
    "    key_transposed_scaled = op.Mul(key_transposed, op.Sqrt(scale))\n",
    "    attn_weight = op.Softmax(\n",
    "        op.MatMul(query_scaled, key_transposed_scaled),\n",
    "        axis=-1,\n",
    "    )\n",
    "    attn_weight, _ = op.Dropout(attn_weight, dropout_p)\n",
    "    return op.MatMul(attn_weight, value)\n",
    "\n",
    "\n",
    "def custom_scaled_dot_product_attention(g, query, key, value, attn_mask, dropout, is_causal, scale=None):\n",
    "    return g.onnxscript_op(ScaledDotProductAttention, query, key, value, dropout).setType(value.type())\n",
    "\n",
    "\n",
    "torch.onnx.register_custom_op_symbolic(\n",
    "    symbolic_name=\"aten::scaled_dot_product_attention\",\n",
    "    symbolic_fn=custom_scaled_dot_product_attention,\n",
    "    opset_version=17,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4efb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_tensorrt.fx.utils import LowerPrecision\n",
    "import torch_tensorrt.fx.tracer.acc_tracer.acc_tracer as acc_tracer\n",
    "from torch_tensorrt.fx import InputTensorSpec, TRTInterpreter, TRTModule\n",
    "from torch_tensorrt.fx.tools.trt_splitter import TRTSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2953fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection import fasterrcnn_mobilenet_v3_large_320_fpn\n",
    "\n",
    "model = fasterrcnn_mobilenet_v3_large_320_fpn(weights=\"DEFAULT\").cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3eb35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from diffusers import StableDiffusionPipeline\n",
    "from diffusers import DiffusionPipeline\n",
    "from diffusers import DDIMScheduler\n",
    "from diffusers.pipelines.stable_diffusion import StableDiffusionPipeline\n",
    "\n",
    "# Use the DDIMScheduler scheduler here instead\n",
    "scheduler = DDIMScheduler.from_pretrained(\"stabilityai/stable-diffusion-2-1\",\n",
    "                                            subfolder=\"scheduler\")\n",
    "\n",
    "# pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16)\n",
    "# pipe = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
    "# pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\")\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\",\n",
    "                                                custom_pipeline=\"stable_diffusion_tensorrt_txt2img\",\n",
    "                                                revision='fp16',\n",
    "                                                torch_dtype=torch.float16,\n",
    "                                                scheduler=scheduler,)\n",
    "pipe.set_cached_folder(\"stabilityai/stable-diffusion-2-1\", revision='fp16',)\n",
    "pipe = pipe.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d91707",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load(\"Megvii-BaseDetection/YOLOX\", \"yolox_l\").cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db20f2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033e6ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826016dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29291e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1596f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.unet.conv_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53adbddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "H, W = 512, 512\n",
    "\n",
    "vae_input = [torch.randn((batch_size, 3, H // 8, W // 8)).cuda()]\n",
    "unet_input = [torch.randn((batch_size, 4, H // 8, W // 8)).cuda()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7727f959",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipe.unet\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a44cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "img = Image.open(\"test.jpg\")\n",
    "img = img.resize((640, 480))\n",
    "img = F.to_tensor(img).unsqueeze(0).cuda()\n",
    "inputs = [img]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a2c7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_tracer is a custom fx tracer that maps nodes whose targets are PyTorch operators\n",
    "# to acc ops.\n",
    "traced = acc_tracer.trace(model, unet_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8968975e",
   "metadata": {},
   "outputs": [],
   "source": [
    "traced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81629e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitter will split the model into several submodules. The name of submodules will\n",
    "# be either `run_on_acc_{}` or `run_on_gpu_{}`. Submodules named `run_on_acc_{}` can\n",
    "# be fully lowered to TensorRT via fx2trt while submodules named `run_on_gpu_{}` has\n",
    "# unsupported ops and can't be lowered by fx2trt. We can still run `run_on_gpu_{}`\n",
    "# submodules on Gpu if ops there have cuda implementation, the naming is a bit\n",
    "# confusing and we'll improve it.\n",
    "splitter = TRTSplitter(traced, unet_input)\n",
    "splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ba4eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview functionality allows us to see what are the supported ops and unsupported\n",
    "# ops. We can optionally the dot graph which will color supported ops and unsupported\n",
    "# ops differently.\n",
    "_ = splitter.node_support_preview(dump_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb84e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter.non_acc_submodule_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66266ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split.\n",
    "split_mod = splitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9336f6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After split we have three submodules, _run_on_acc_0 and _run_on_gpu_1.\n",
    "print(split_mod.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d62e499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_submod_inputs(mod, submod, inputs):\n",
    "    acc_inputs = None\n",
    "\n",
    "    def get_input(self, inputs):\n",
    "        nonlocal acc_inputs\n",
    "        acc_inputs = inputs\n",
    "\n",
    "    handle = submod.register_forward_pre_hook(get_input)\n",
    "    mod(*inputs)\n",
    "    handle.remove()\n",
    "    return acc_inputs\n",
    "\n",
    "\n",
    "# Since the model is splitted into three segments. We need to lower each TRT eligible segment.\n",
    "# If we know the model can be fully lowered, we can skip the splitter part.\n",
    "for name, _ in split_mod.named_children():\n",
    "    print(f\"Splitting {name}\")\n",
    "    if \"_run_on_acc\" in name:\n",
    "        submod = getattr(split_mod, name)\n",
    "        # Get submodule inputs for fx2trt\n",
    "        acc_inputs = get_submod_inputs(split_mod, submod, inputs)\n",
    "\n",
    "        # fx2trt replacement\n",
    "        interp = TRTInterpreter(\n",
    "            submod,\n",
    "            InputTensorSpec.from_tensors(acc_inputs),\n",
    "            explicit_batch_dimension=True,\n",
    "        )\n",
    "        r = interp.run(lower_precision=LowerPrecision.FP32)\n",
    "        trt_mod = TRTModule(*r)\n",
    "        setattr(split_mod, name, trt_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b544d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lowered_model_output = split_mod(*inputs)\n",
    "\n",
    "# Save and load model\n",
    "torch.save(split_mod, \"trt.pt\")\n",
    "reload_trt_mod = torch.load(\"trt.pt\")\n",
    "reload_model_output = reload_trt_mod(*inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0406449",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e65337a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the results match\n",
    "regular_model_output = model.backbone(*inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74792843",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4886341",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 100\n",
    "_ = model.backbone(*inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4727827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 100\n",
    "_ = reload_trt_mod(*inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01fbdeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
