{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8d725f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dashcam video of driving around San Francisco\n",
    "video_url = \"https://www.youtube.com/watch?v=PGMu_Z89Ao8\"  \n",
    "\n",
    "# Frames to skip\n",
    "N = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeed3494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytube is used to download videos from YouTube\n",
    "!pip install pytube\n",
    "\n",
    "# Intall a newer version of plotly\n",
    "!pip install --upgrade plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f01f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube\n",
    "\n",
    "# Choose a video stream with resolution of 360p\n",
    "streams = YouTube(video_url).streams.filter(adaptive=True, subtype=\"mp4\", resolution=\"360p\", only_video=True)\n",
    "\n",
    "# Check if there is a valid stream\n",
    "if len(streams) == 0:\n",
    "  raise \"No suitable stream found for this YouTube video!\"\n",
    "\n",
    "# Download the video as video.mp4\n",
    "print(\"Downloading...\")\n",
    "streams[0].download(filename=\"video.mp4\")\n",
    "print(\"Download completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371414b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# The frame images will be stored in video_frames\n",
    "video_frames = []\n",
    "\n",
    "# Open the video file\n",
    "capture = cv2.VideoCapture('video.mp4')\n",
    "fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "current_frame = 0\n",
    "while capture.isOpened():\n",
    "  # Read the current frame\n",
    "  ret, frame = capture.read()\n",
    "\n",
    "  # Convert it to a PIL image (required for CLIP) and store it\n",
    "  if ret == True:\n",
    "    video_frames.append(Image.fromarray(frame[:, :, ::-1]))\n",
    "  else:\n",
    "    break\n",
    "\n",
    "  # Skip N frames\n",
    "  current_frame += N\n",
    "  capture.set(cv2.CAP_PROP_POS_FRAMES, current_frame)\n",
    "\n",
    "# Print some statistics\n",
    "print(f\"Frames extracted: {len(video_frames)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9063f597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nos.client import InferenceClient, TaskType\n",
    "\n",
    "# Create a client that connects to the inference server via gRPC (50051)\n",
    "client = InferenceClient()\n",
    "client.WaitForServer()\n",
    "client.IsHealthy()\n",
    "\n",
    "nos_embedding_module = client.Module(TaskType.IMAGE_EMBEDDING, \"openai/clip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8702cdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import datetime\n",
    "from IPython.core.display import HTML\n",
    "import torch\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "# The encoded features will be stored in video_features\n",
    "video_features = np.empty([0, 512], dtype=np.float16)\n",
    "# We don't yet support batched inference, so do this frame by frame (might take a while)\n",
    "for frame in video_frames:\n",
    "    with torch.no_grad():\n",
    "        frame_features = nos_embedding_module(images=[np.asarray(frame)])['embedding']\n",
    "        frame_features_normed = frame_features.copy()\n",
    "        frame_features_normed /= np.linalg.norm(frame_features, axis=-1)\n",
    "        # print(f\"video features: {video_features}\")\n",
    "        # print(f\"frame features: {frame_features}\")\n",
    "        video_features = np.concatenate((video_features, frame_features_normed))\n",
    "\n",
    "print(f\"Final video features: {video_features.shape}\")\n",
    "\n",
    "# Convert to torch tensor\n",
    "torch_video_features = torch.from_numpy(video_features).type(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b19453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text_nos(search_query):\n",
    "    with torch.no_grad():\n",
    "        text_features = client.Run(TaskType.TEXT_EMBEDDING, \"openai/clip\", texts=[search_query])['embedding']\n",
    "        torch_text_features = torch.from_numpy(text_features)\n",
    "        torch_text_features /= torch_text_features.norm(dim=-1, keepdim=True).to('cpu').type(torch.float32)\n",
    "    return torch_text_features\n",
    "\n",
    "def search_video(search_query, video_features, display_heatmap=True, display_results_count=3, encoder='NOS'):\n",
    "\n",
    "    text_features = encode_text_nos(search_query)\n",
    "\n",
    "    # Compute the similarity between the search query and each frame using the Cosine similarity\n",
    "    similarities = (100.0 * video_features @ text_features.T)\n",
    "    values, best_photo_idx = similarities.topk(display_results_count, dim=0)    \n",
    "    \n",
    "    # Display the heatmap\n",
    "    if display_heatmap:\n",
    "        print(\"Search query heatmap over the frames of the video:\")\n",
    "        fig = px.imshow(similarities.T, height=50, aspect='auto', color_continuous_scale='viridis')\n",
    "        fig.update_layout(coloraxis_showscale=False)\n",
    "        fig.update_xaxes(showticklabels=False)\n",
    "        fig.update_yaxes(showticklabels=False)\n",
    "        fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\n",
    "        fig.show()\n",
    "        print()\n",
    "\n",
    "    # Display the top 3 frames\n",
    "    for frame_id in best_photo_idx:\n",
    "        display(video_frames[frame_id])\n",
    "\n",
    "        # Find the timestamp in the video and display it\n",
    "        seconds = round(frame_id.cpu().numpy()[0] * N / fps)\n",
    "        display(HTML(f\"Found at {str(datetime.timedelta(seconds=seconds))} (link)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff2ea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_video(\"firetruck\", torch_video_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc84559d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base-gpu-new",
   "language": "python",
   "name": "base-gpu-new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
