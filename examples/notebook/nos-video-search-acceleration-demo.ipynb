{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1JIIlkTWa2xbft5bTpzhGK1BxYL83bJNU\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# üî• Inference Acceleration Demo\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1cyDL_idJ78Bjp7RdO1baTVZHekb9_OFJ\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this demo, we‚Äôre going to show you how you can take a vanilla Pytorch model, have NOS accelerate it, and then scale it to speed up inference by a **factor of 10x**, in just a few lines of code.\n",
    "\n",
    "By the end of this demo, we‚Äôll use NOS to build an end-to-end semantic video search demo. We‚Äôll process a 10-minute video of San Francisco in real-time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the video clip we'll be using for the demos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nos.common.io import VideoReader\n",
    "\n",
    "FILENAME = \"top-10-things-to-do-in-sf.mp4\"\n",
    "print(VideoReader(FILENAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "Video(FILENAME, width=640)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî• 1. Inference with ü§ó transformers (OpenAI CLIP)\n",
    "---\n",
    "\n",
    "Let's say we're using the popular OpenAI CLIP for extracting image and text embeddings, and we're using the ü§ó transformers library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Union, List\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class CLIP:\n",
    "    \"\"\"Text and image encoding using OpenAI CLIP\"\"\"\n",
    "    def __init__(self, model_name: str = \"openai/clip-vit-base-patch32\"):\n",
    "        from transformers import CLIPModel\n",
    "        from transformers import CLIPProcessor, CLIPTokenizer\n",
    "        \n",
    "        self.processor = CLIPProcessor.from_pretrained(model_name)\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = CLIPModel.from_pretrained(model_name).to(device)\n",
    "        self.model.eval()\n",
    "        self.device = self.model.device\n",
    "        \n",
    "    def encode_image(self, images: Union[Image.Image, np.ndarray, List[Image.Image], List[np.ndarray]]):\n",
    "        \"\"\"Encode image into an embedding.\"\"\"\n",
    "        with torch.inference_mode():\n",
    "            inputs = self.processor(images=images, return_tensors=\"pt\").to(self.device)\n",
    "            return self.model.get_image_features(**inputs).cpu().numpy()\n",
    "\n",
    "    def encode_text(self, texts: Union[str, List[str]]) -> np.ndarray:\n",
    "        \"\"\"Encode text into an embedding.\"\"\"\n",
    "        with torch.inference_mode():\n",
    "            if isinstance(texts, str):\n",
    "                texts = [texts]\n",
    "            inputs = self.tokenizer(\n",
    "                texts,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(self.device)\n",
    "            text_features = self.model.get_text_features(**inputs)\n",
    "            return text_features.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Inference\n",
    "\n",
    "Let's see what a naive inference implementation of inference would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nos.common import tqdm\n",
    "from nos.common.io import VideoLoader\n",
    "\n",
    "# Load the first image\n",
    "images = VideoLoader(FILENAME, shape=(1, 224, 224, 3))\n",
    "image = next(images)\n",
    "\n",
    "# Load the Pytorch model\n",
    "clip = CLIP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in tqdm(duration=5, desc=\"Naive implementation\", unit=\" images\"):\n",
    "    clip.encode_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch in /dev\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1JQcd4hRBIBi77xKgypy-XwL9bwtgHDGB\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch in /prod\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1_ZqGkyGBBy22gtKFoyg-f6f-8qlIME16\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö°Ô∏è 2. Optimizing Inference with NOS\n",
    "---\n",
    "\n",
    "**NOS** provides a convenient way to **compile**, **tune** and **auto-scale** Pytorch models for inference. \n",
    "\n",
    "Let‚Äôs start the NOS backend. NOS can be run locally or in the cloud accessing 100s of GPUs in a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nos\n",
    "\n",
    "nos.init(runtime=\"local\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used `CLIPModel` from the ü§ó transformers library. \n",
    "\n",
    "```python\n",
    "class CLIP:\n",
    "    def __init__(self, model_name: str = \"openai/clip-vit-base-patch32\"):\n",
    "        from transformers import CLIPModel\n",
    "        ...\n",
    "        self.model = CLIPModel.from_pretrained(model_name).to(device)\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip.model = nos.compile(clip.model, [image], precision=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in tqdm(duration=5, desc=\"NOS optimized\", unit=\" images\"):\n",
    "    clip.encode_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key takeaway:** >60% better performance, identical API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üöÄ 3. Optimizing and Scaling Inference with NOS\n",
    "---\n",
    "\n",
    "Now let's compile and scale `CLIP` for production deployment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the model for remote-execution\n",
    "model = nos.load(CLIP, \n",
    "                 init_args=(), init_kwargs={\"model_name\": \"openai/clip-vit-base-patch32\"}, \n",
    "                 method_name=\"encode_image\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's optimize the model and scale the number of replicas so that we maximally use the underlying hardware. NOS automatically decides the optimal number of replicas to give us the best performance for the hardware we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nos.managers.model import ModelOptimizationPolicy\n",
    "\n",
    "# Optimize (ahead-of-time compilation) + Scale the model for max throughput\n",
    "model = model.scale(replicas=\"auto\", policy=ModelOptimizationPolicy.MAX_THROUGHPUT)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let‚Äôs take this optimized and scaled model to build a video search engine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. üîç Video Search Demo (in < 10 lines of code)\n",
    "\n",
    "Let's put together a quick semantic search demo with OpenAI's CLIP model, using the NOS inference engine. \n",
    "\n",
    "The following snippet extracts embeddings from all the frames in the video and uses CLIP to cross-reference text queries with the image embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nos.common import tqdm\n",
    "from nos.common.io import VideoLoader\n",
    "\n",
    "# Load frames from the video lazily\n",
    "B = model.batch_size()\n",
    "images = VideoLoader(FILENAME, shape=(B, 224, 224, 3))\n",
    "images = ({\"images\": img} for img in tqdm(images, unit_scale=B, unit=\"images\"))\n",
    "\n",
    "# Batch inference using auto-scaled model, then normalize embeddings\n",
    "video_features = torch.from_numpy(np.vstack(list(model.imap(images))))\n",
    "video_features /= video_features.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "from nos.common.io import VideoReader\n",
    "\n",
    "encode_text = CLIP().encode_text\n",
    "video = VideoReader(FILENAME)\n",
    "\n",
    "def search_video(query: str, video_features: np.ndarray, topk: int = 3):\n",
    "    \"\"\"Semantic video search demo in 8 lines of code\"\"\"\n",
    "    # Encode text and normalize\n",
    "    with torch.inference_mode():\n",
    "        text_features = encode_text(texts=[query])\n",
    "        text_features = torch.from_numpy(text_features)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Compute the similarity between the search query and each video frame\n",
    "    similarities = (video_features @ text_features.T)\n",
    "    _, best_photo_idx = similarities.topk(topk, dim=0)\n",
    "    \n",
    "    # Display the top k frames\n",
    "    results = np.hstack([video[int(frame_id)] for frame_id in best_photo_idx])\n",
    "    display(Image.fromarray(results).resize((600, 400)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Semantic video search with text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_video(\"golden gate bridge\", video_features, topk=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_video(\"alcatraz prison\", video_features, topk=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_video(\"fishermans wharf\", video_features, topk=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_video(\"golden gate park windmill\", video_features, topk=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_video(\"chinatown\", video_features, topk=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_video(\"lombard street\", video_features, topk=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_video(\"pier 39\", video_features, topk=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_video(\"riding the tram\", video_features, topk=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_video(\"ferry building\", video_features, topk=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How NOS works\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1aHyc3pFX-wPtQbxKzo3SuuXP60KyWRbu\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOS Applications\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1kRj_gEip4aZ7QIpiG_OSqev81nkeeWtQ\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks for watching!\n",
    "\n",
    "Reach out to us: **Sudeep**  (sudeep@autonomi.ai) | **Scott** (scott@autonomi.ai) | https://www.autonomi.ai"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
