{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c67713c7-d33f-4dce-9aa3-205d8f2c0cdf",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1JIIlkTWa2xbft5bTpzhGK1BxYL83bJNU\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1afa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01648e1a",
   "metadata": {},
   "source": [
    "# âš¡ï¸ `torch-nos` âš¡ï¸: Nitrous Oxide System for PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1eea8d",
   "metadata": {},
   "source": [
    "**NOS** is a PyTorch library for optimizing and running lightning-fast inference of popular computer vision models. NOS inherits its name from \"Nitrous Oxide System\", the performance-enhancing system typically used in racing cars. NOS is designed to be modular and easy to extend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb9df4f",
   "metadata": {},
   "source": [
    "## Why NOS?\n",
    "- âš¡ï¸ **Fast**: Built for PyTorch and designed to optimize/run models faster\n",
    "- ðŸ”¥ **Performant**: Run models such as SDv2 or object detection 2-3x faster out-of-the-box\n",
    "- ðŸ‘©â€ðŸ’» **No PhD required**: Optimize models for maximum HW performance without a PhD in ML\n",
    "- ðŸ“¦ **Extensible**: Easily add optimization and HW-support for custom models\n",
    "- âš™ï¸ **HW-accelerated:** Take full advantage of your HW (GPUs, ASICs) without compromise\n",
    "- â˜ï¸ **Cloud-agnostic:** Run on any cloud HW (AWS, GCP, Azure, Lambda Labs, On-Prem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b5f7d4",
   "metadata": {},
   "source": [
    "## Batteries Included\n",
    " - ðŸ’ª **SOTA Model Support:** NOS provides out-of-the-box support for popular CV models such as [Stable Diffusion](stabilityai/stable-diffusion-2), [OpenAI CLIP](openai/clip-vit-base-patch32), [OpenMMLab](https://github.com/open-mmlab/) object detection, tracking and more\n",
    " - ðŸ”Œ **APIs:** NOS provides out-of-the-box APIs and avoids all the ML model deployment hassles\n",
    " - ðŸ³ **Docker:** NOS ships with docker images to run accelerated and scalable CV workloads\n",
    " - ðŸ“ˆ **Multi-Platform**: NOS allows you to run models on different HW (NVIDIA, custom ASICs) without any model compilation or runtime management."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e449eb19",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "NOS is available on PyPI and can be installed via pip. We recommend using a virtual environment or conda to install NOS:\n",
    "\n",
    "```bash\n",
    "$ conda create -n nos-py38 python=3.8\n",
    "$ conda activate nos-py38\n",
    "$ pip install torch-nos\n",
    "```\n",
    "\n",
    "Alternatively, if you want to install the entirety of NOS with its server-side dependencies, you can install NOS with the `server` and `gpu` extra:\n",
    "\n",
    "```bash\n",
    "$ pip install torch-nos[server,gpu]\n",
    "```\n",
    "\n",
    "The tutorial below assumes that you have only installed the vanilla `torch-nos` package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721784f2",
   "metadata": {},
   "source": [
    "## NOS Runtime\n",
    "\n",
    "NOS operates as a client-server system. The NOS server is responsible for optimizing and running the model. The NOS client is responsible for sending inference requests to the NOS server. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37fd2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nos\n",
    "\n",
    "# Start the nos server backend\n",
    "nos.init(runtime=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cf1ebb",
   "metadata": {},
   "source": [
    "To double-check if your nos server has started successfully, you can run `docker ps | grep nos` to check the status of your server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2cbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker ps | grep nos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c17875",
   "metadata": {},
   "source": [
    "## Inference Client SDK\n",
    "\n",
    "Once the NOS server is running, you can send inference requests using the NOS client SDK. \n",
    "Let's start by importing the NOS client SDK and creating a client instance. The client instance is used to send inference requests to the NOS server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ae8bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nos.client import Client, TaskType\n",
    "\n",
    "# Create a client that connects to the inference server via gRPC (50051)\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5537b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We provide helper functions to wait for the server to be ready\n",
    "# if the server is simultaneously spun up in a separate process.\n",
    "client.WaitForServer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abc3b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we can check if the server is healthy.\n",
    "client.IsHealthy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2271ff2",
   "metadata": {},
   "source": [
    "## Run Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5f164d",
   "metadata": {},
   "source": [
    "Now, we're ready to run inference using our client. First, let's load an image for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e388b67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "URL = \"https://raw.githubusercontent.com/open-mmlab/mmdetection/main/demo/demo.jpg\"\n",
    "img = Image.open(requests.get(URL, stream=True).raw).resize((640, 480))\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bfd5ef",
   "metadata": {},
   "source": [
    "## `client.Module` Interface\n",
    "\n",
    "NOS provides a convenient `client.Module` interface to get model handles for simpler remote-model execution. In this case, the following line re-uses the same `yolox/medium` model instantiated earlier as creates a handle called `yolox` that can be used by the client to get object detection results.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c8d647",
   "metadata": {},
   "source": [
    "### Run object detection with [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX)\n",
    "\n",
    "Here, we will use the `client.Module` API to run inference on the YOLOX object detection model. See the [`client.Module(...)`](https://docs.nos.run/docs/api/client/#nos.client.grpc.Client.Module) documentation for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26edddc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def visualize_bboxes(img: np.ndarray, bboxes: np.ndarray, labels: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Visualize 2D detection results on an image.\"\"\"\n",
    "    vis = np.asarray(img).copy()\n",
    "    for bbox, label in zip(bboxes.astype(np.int32), labels):\n",
    "        cv2.rectangle(vis, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)\n",
    "    return vis\n",
    "\n",
    "\n",
    "# Initiaize the yolox model as a client Module\n",
    "yolox = client.Module(\"yolox/medium\")\n",
    "\n",
    "# Run inference on the image just like a normal pytorch model\n",
    "predictions = yolox(images=img)\n",
    "for idx, (img, bboxes, scores, labels) in enumerate(zip([img], predictions[\"bboxes\"], predictions[\"scores\"], predictions[\"scores\"])):\n",
    "    display(Image.fromarray(visualize_bboxes(img, bboxes, labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b6f397",
   "metadata": {},
   "source": [
    "### Extract image-embedding with [OpenAI CLIP](https://huggingface.co/openai/clip-vit-base-patch32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc9e059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from nos.test.utils import NOS_TEST_IMAGE\n",
    "\n",
    "img = Image.open(NOS_TEST_IMAGE)\n",
    "\n",
    "clip = client.Module(\"openai/clip\")\n",
    "predictions = clip.encode_image(images=[img, img])\n",
    "predictions[\"embedding\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8b2578",
   "metadata": {},
   "source": [
    "### Extract text-embedding with [OpenAI CLIP](https://huggingface.co/openai/clip-vit-base-patch32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c9bd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clip.encode_text(texts=[\"cat\", \"dog\"])\n",
    "predictions[\"embedding\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b8aa58",
   "metadata": {},
   "source": [
    " ### Text-to-image generation with [StableDiffusionV2](https://huggingface.co/stabilityai/stable-diffusion-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8417340",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"fox jumped over the moon\", \"fox jumped over the sun\"]\n",
    "\n",
    "# Initialize the model as a client Module\n",
    "sdv2 = client.Module(\"stabilityai/stable-diffusion-2\")\n",
    "\n",
    "# Run inference on the image just like a normal pytorch model\n",
    "# predictions = sdv2(inputs={\"prompts\": prompts, \"width\": 512, \"height\": 512, \"num_images\": 1})\n",
    "predictions = sdv2(prompts=prompts, width=512, height=512, num_images=1)\n",
    "\n",
    "# Display the generated images\n",
    "for prompt, image in zip(prompts, predictions[\"images\"]):\n",
    "    print(prompt, image.size)\n",
    "    display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859f4ef6",
   "metadata": {},
   "source": [
    "### Depth prediction with [MiDaS](https://github.com/isl-org/MiDaS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53535ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "midas = client.Module(\"isl-org/MiDaS-small\")\n",
    "pred = midas(images=img)\n",
    "pred = Image.fromarray((pred[\"depths\"] * 255 / pred[\"depths\"].max()).astype(np.uint8))\n",
    "display(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22894ced-7895-4027-90e6-4122ea571dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
