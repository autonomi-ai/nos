{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e59f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4ee344",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install loguru opencv-python-headless tabulate netron\n",
    "!pip install nvidia-pyindex\n",
    "!pip install nvidia-tensorrt\n",
    "!pip install torch-tensorrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558ea5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate\n",
    "!pip install diffusers transformers tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d972a290",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update && apt-get install -y graphviz\n",
    "!pip install expecttest pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abda0b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; print(os.getenv(\"CUDA_MODULE_LOADING\", None))\n",
    "print(os.getenv(\"HF_HOME\"))\n",
    "print(os.getenv(\"TRANSFORMERS_CACHE\"))\n",
    "print(os.getenv(\"TORCH_HOME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36786c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.fx\n",
    "import torch.nn as nn\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82acf812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt\n",
    "print(tensorrt.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8323a699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_tensorrt\n",
    "print(torch_tensorrt.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4efb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_tensorrt.fx.tracer.acc_tracer import acc_normalizer, acc_ops, acc_shape_prop, acc_utils  # noqa: F401\n",
    "from torch.fx.experimental.normalize import NormalizeArgs\n",
    "\n",
    "import torch_tensorrt\n",
    "from torch_tensorrt.fx.utils import LowerPrecision\n",
    "import torch_tensorrt.fx.tracer.acc_tracer.acc_tracer as acc_tracer\n",
    "from torch_tensorrt.fx import InputTensorSpec, TRTInterpreter, TRTModule\n",
    "from torch_tensorrt.fx.tools.trt_splitter import TRTSplitter, TRTSplitterSetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de00708",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.fx.node import Argument, Target\n",
    "from torch.fx import symbolic_trace, replace_pattern\n",
    "\n",
    "from typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, Union\n",
    "import tensorrt as trt\n",
    "from torch_tensorrt.fx.converters.converter_utils import SourceIR, get_trt_tensor, get_trt_plugin\n",
    "from torch_tensorrt.fx.converter_registry import tensorrt_converter\n",
    "from torch_tensorrt.fx.tracer.acc_tracer.acc_op_properties import AccOpProperty, register_acc_op_properties\n",
    "from torch_tensorrt.fx.tracer.acc_tracer.acc_normalizer import (\n",
    "    register_acc_op,\n",
    "    register_acc_op_mapping,\n",
    "    register_custom_acc_mapper_fn,\n",
    ")\n",
    "from torch_tensorrt.fx.types import (\n",
    "    TRTNetwork,\n",
    "    TRTTensor,\n",
    ")\n",
    "from torch_tensorrt.fx.converters.impl import activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97a6358",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_tensorrt.fx.tracer.acc_tracer import acc_ops\n",
    "from torch_tensorrt.fx.converter_registry import CONVERTERS\n",
    "from torch_tensorrt.fx.tracer.acc_tracer.acc_normalizer import _acc_ops, _normalization_dict\n",
    "import torch_tensorrt.fx.converter_registry as registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd970fa8-4f42-4374-930b-547c8690c0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.ERROR)\n",
    "trt.init_libnvinfer_plugins(TRT_LOGGER, \"\")\n",
    "print(f\"Register libnvinfer plugins\")\n",
    "registry = trt.get_plugin_registry()\n",
    "print(f\"Registry: {registry}\")\n",
    "for plugin in registry.plugin_creator_list:\n",
    "    print(plugin.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d66024f",
   "metadata": {},
   "outputs": [],
   "source": [
    "registry.CONVERTERS.pop(acc_ops.expand)\n",
    "for k in registry.CONVERTERS.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d183d249",
   "metadata": {},
   "outputs": [],
   "source": [
    "for op in list(CONVERTERS.keys()):\n",
    "    if op == acc_ops.gelu:\n",
    "        CONVERTERS.pop(op)\n",
    "        print(f\"removed converter {op}\")\n",
    "\n",
    "for op in list(_acc_ops):\n",
    "    if op.__name__ == \"gelu\":\n",
    "        _acc_ops.remove(op)\n",
    "        print(f\"removed acc_op: {op}\")\n",
    "        \n",
    "for (op, target) in list(_normalization_dict.keys()):\n",
    "    if \"gelu\" in str(target) or \"GELU\" in str(target):\n",
    "        _normalization_dict.pop((op, target))\n",
    "        print(f\"removed normalization_dict op: {op}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9149a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "@register_acc_op_mapping(\n",
    "    op_and_target=(\"call_function\", torch.nn.functional.group_norm),\n",
    "    arg_replacement_tuples=[\n",
    "        (\"input\", \"input\"),\n",
    "        (\"num_groups\", \"num_groups\"),\n",
    "        (\"weight\", \"weight\"),\n",
    "        (\"bias\", \"bias\"),\n",
    "        (\"eps\", \"eps\"),\n",
    "    ],\n",
    ")\n",
    "@register_acc_op\n",
    "def group_norm(*, input, num_groups, weight=None, bias=None, eps=1e-05):\n",
    "    return torch.nn.functional.group_norm(input, num_groups, weight=weight, bias=bias, eps=eps)\n",
    "\n",
    "\n",
    "@tensorrt_converter(group_norm)\n",
    "def acc_ops_group_norm(network, target, args, kwargs, name):\n",
    "    input_val = kwargs[\"input\"]\n",
    "    weight = kwargs[\"weight\"]\n",
    "    bias = kwargs[\"bias\"]\n",
    "    shape = (input_val.shape[1],)\n",
    "    if weight is None:\n",
    "        weight = torch.ones(tuple([*input_val.shape])).to(\n",
    "            torch_dtype_from_trt(input_val.dtype)\n",
    "        )\n",
    "    weight = get_trt_tensor(network, weight, f\"{name}_weight\")\n",
    "\n",
    "    if bias is None:\n",
    "        bias = torch.zeros(tuple([*input_val.shape])).to(\n",
    "            torch_dtype_from_trt(input_val.dtype)\n",
    "        )\n",
    "    bias = get_trt_tensor(network, bias, f\"{name}_bias\")\n",
    "\n",
    "    if not isinstance(input_val, trt.tensorrt.ITensor):\n",
    "        raise RuntimeError(\n",
    "            f\"GroupNorm received input {input_val} that is not part \"\n",
    "            \"of the TensorRT region!\"\n",
    "        )\n",
    "\n",
    "    num_groups_field = trt.PluginField(\n",
    "        \"num_groups\", np.array([kwargs[\"num_groups\"]], dtype=np.int32), trt.PluginFieldType.INT32\n",
    "    )\n",
    "    eps_field = trt.PluginField(\n",
    "        \"eps\", np.array([kwargs[\"eps\"]], dtype=np.float32), trt.PluginFieldType.FLOAT32\n",
    "    )\n",
    "\n",
    "    field_collection = trt.PluginFieldCollection(\n",
    "        [eps_field, num_groups_field]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        plugin = get_trt_plugin(\"GroupNormalizationPlugin\", field_collection, \"1\", \"\")\n",
    "    except AssertionError:\n",
    "        _LOGGER.error(\n",
    "            \"Unable to find group norm plugin, fall back to TensorRT implementation.\"\n",
    "        )\n",
    "        raise RuntimeError(\n",
    "            f\"Failed to build group norm plugin.\"\n",
    "        )\n",
    "    layer = network.add_plugin_v2([input_val, weight, bias], plugin)\n",
    "    layer.name = name\n",
    "    return layer.get_output(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbdf68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3eb35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "from diffusers import DDIMScheduler\n",
    "from diffusers.pipelines.stable_diffusion import StableDiffusionPipeline\n",
    "\n",
    "# Use the DDIMScheduler scheduler here instead\n",
    "scheduler = DDIMScheduler.from_pretrained(\"stabilityai/stable-diffusion-2-1\",\n",
    "                                            subfolder=\"scheduler\")\n",
    "\n",
    "# pipe = DiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16)\n",
    "\n",
    "# pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
    "# pipe = DiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16, revision=\"fp16\")\n",
    "\n",
    "# pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\")\n",
    "pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\", torch_dtype=torch.float16 , scheduler=scheduler)\n",
    "\n",
    "# TensorRT\n",
    "# pipe = StableDiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1\",\n",
    "#                                                 custom_pipeline=\"stable_diffusion_tensorrt_txt2img\",\n",
    "#                                                 revision='fp16',\n",
    "#                                                 torch_dtype=torch.float16,\n",
    "#                                                 scheduler=scheduler,)\n",
    "# pipe.set_cached_folder(\"stabilityai/stable-diffusion-2-1\", revision='fp16',)\n",
    "\n",
    "# CUDA\n",
    "pipe = pipe.to(\"cuda\")\n",
    "\n",
    "# Custom attention\n",
    "# pipe.unet.set_default_attn_processor()\n",
    "# pipe.enable_attention_slicing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc15d804",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input = torch.randint(0, 1, (1, 14), dtype=torch.int32).to(\"cuda\")\n",
    "input2 = torch.randint(0, 1, (1, 14), dtype=torch.int32).to(\"cuda\")\n",
    "\n",
    "compile_spec = {\n",
    "    \"inputs\": [\n",
    "        torchtrt.Input(\n",
    "            input.shape,\n",
    "            dtype=input.dtype,\n",
    "            format=torch.contiguous_format,\n",
    "        ),\n",
    "        torchtrt.Input(\n",
    "            input.shape,\n",
    "            dtype=input.dtype,\n",
    "            format=torch.contiguous_format,\n",
    "        ),\n",
    "    ],\n",
    "    \"device\": torchtrt.Device(\"cuda:0\"),\n",
    "    \"enabled_precisions\": {torch.float},\n",
    "    \"truncate_long_and_double\": True,\n",
    "    \"ir\": ir,\n",
    "}\n",
    "trt_mod = torchtrt.compile(model, **compile_spec)\n",
    "\n",
    "model_outputs = model(input, input2)\n",
    "trt_model_outputs = trt_mod(input, input2)\n",
    "for key in model_outputs.keys():\n",
    "    out, trt_out = model_outputs[key], trt_model_outputs[key]\n",
    "    cos_sim = cosine_similarity(out, trt_out)\n",
    "    assertions.assertTrue(\n",
    "        cos_sim > COSINE_THRESHOLD,\n",
    "        msg=f\"HF BERT base-uncased TRT outputs don't match with the original model. Cosine sim score: {cos_sim} Threshold: {COSINE_THRESHOLD}\",\n",
    "    )\n",
    "\n",
    "# Clean up model env\n",
    "torch._dynamo.reset()\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a607e4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch._dynamo\n",
    "torch._dynamo.config.verbose = True\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "pipe.unet.to(memory_format=torch.channels_last)\n",
    "pipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\n",
    "# model = pipe.unet._orig_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adbe601",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.unet = torch_tensorrt.dynamo.compile(\n",
    "    pipe.unet,\n",
    "    inputs,\n",
    "    enabled_precisions={torch.half},\n",
    "    debug=True,\n",
    "    workspace_size=20 << 30,\n",
    "    min_block_size=3,\n",
    "    torch_executed_ops={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a44cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "# CLIP\n",
    "# inputs = [torch.randn((1, 3, 224, 224), dtype=torch.float32, device='cuda')]\n",
    "\n",
    "# UNET\n",
    "# https://github.com/stochasticai/x-stable-diffusion/blob/main/TensorRT/convert_unet_to_tensorrt.py\n",
    "# https://github.com/huggingface/diffusers/blob/666743302ff5bd1e02c204b81a80e566648d60de/examples/community/stable_diffusion_tensorrt_txt2img.py#L493\n",
    "# latents_shape = (\n",
    "#     args.batch_size * 2,\n",
    "#     4,\n",
    "#     args.img_size[0] // 8,\n",
    "#     args.img_size[1] // 8,\n",
    "# )\n",
    "# embed_shape = (args.batch_size * 2, args.max_seq_length, 768)\n",
    "# timestep_shape = (args.batch_size,)\n",
    "\n",
    "# inputs = [\n",
    "#     torch.randn(2, 4, 64, 64, dtype=torch.half, device='cuda:0'), \n",
    "#     torch.tensor([1, 3], dtype=torch.int32, device='cuda:0'), \n",
    "#     torch.randn(2, 77, 768, dtype=torch.half, device='cuda:0')\n",
    "# ]\n",
    "\n",
    "\n",
    "# TEXT ENCODER\n",
    "# inputs = (torch.randn(2, 4, 64, 64, dtype=torch.half, device='cuda'), \n",
    "#           torch.randn(1, dtype=torch.half, device='cuda'), \n",
    "#           torch.randn(2, 77, 768, dtype=torch.half, device='cuda'))\n",
    "# inputs\n",
    "\n",
    "# fp16 (2.1.0.dev20230605+cu118)\n",
    "# 4090: eager (sdv1-5): 30.6 it/s\n",
    "# 4090: eager (sdv1-5): 30.6 it/s (w/ torch.compile, mode=\"reduce-overhead\", fullgraph=True)\n",
    "\n",
    "# fp32 (2.1.0.dev20230605+cu118)\n",
    "# 4090: eager (sdv2-1): 5.6 it/s (w/ 2.0.1)\n",
    "# 4090: eager (sdv2-1): 4.34 it/s (w/ attention-slicing)\n",
    "# 4090: eager (sdv2-1): 7.2 it/s\n",
    "\n",
    "# fp16 (2.1.0.dev20230605+cu118)\n",
    "# 4090: eager (sdv2-1): 21.2 it/s\n",
    "# 4090: eager (sdv2-1): 24.5 it/s (w/ torch.compile, mode=\"reduce-overhead\", fullgraph=True)\n",
    "\n",
    "# >> Inference\n",
    "# with torch.inference_mode():\n",
    "images = pipe(prompt=[\"fox jumped over dog\"], num_inference_steps=200, num_images_per_prompt=1).images\n",
    "\n",
    "# output.last_hidden_state.shape (1, 50, 768)\n",
    "# output.pooler_output.shape  (1, 768)\n",
    "# output.hidden_states = None\n",
    "# output.attentions = None\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c71f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = pipe(prompt=[\"fox jumped over dog\"], num_inference_steps=200, num_images_per_prompt=1).images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7234d605",
   "metadata": {},
   "outputs": [],
   "source": [
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb7c81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(pipe.unet)\n",
    "emodel, _ = torch._dynamo.export(pipe, {\"prompt\": \"test prompt\"})\n",
    "emodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d33544",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch._dynamo.explain(model, *inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352a6e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import onnxscript\n",
    "# from onnxscript.onnx_opset import opset14 as op\n",
    "\n",
    "# opset_version = 14\n",
    "# custom_opset = onnxscript.values.Opset(domain=\"onnx-script\", version=1)\n",
    "# default_opset = onnxscript.values.Opset(domain=\"onnx\", version=1)\n",
    "\n",
    "# @onnxscript.script(custom_opset, default_opset)\n",
    "# def ScaledDotProductAttention(Q, K, V, attn_mask=None, dropoout_p=0.0, is_causal=False):\n",
    "#     # Efficient implementation equivalent to the following:\n",
    "#     query_shape = list(Q.size())\n",
    "#     key_shape = list(K.size())\n",
    "#     L = query_shape[len(query_shape)-2]\n",
    "#     S = key_shape[len(key_shape) - 2]\n",
    "#     attn_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0) if is_causal else attn_mask\n",
    "#     attn_mask = attn_mask.masked_fill(not attn_mask, -float('inf')) if attn_mask.dtype==torch.bool else attn_mask\n",
    "#     attn_weight = torch.softmax((Q @ K.transpose(-2, -1) / math.sqrt(Q.size(-1))) + attn_mask, dim=-1)\n",
    "#     attn_weight = torch.dropout(attn_weight, dropout_p)\n",
    "#     return attn_weight @ V    \n",
    "\n",
    "# def custom_scaled_dot_product_attention(g: jit_utils.GraphContext, Q, K, V):\n",
    "#     return g.onnxscript_op(ScaledDotProductAttention, Q, K, V).setType(V.type())\n",
    "\n",
    "# torch.onnx.register_custom_op_symbolic(\n",
    "#     symbolic_name=\"aten::scaled_dot_product_attention\",\n",
    "#     symbolic_fn=custom_scaled_dot_product_attention,\n",
    "#     opset_version=opset_version,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d919416",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRT_LOGGER = trt.Logger(trt.Logger.ERROR)\n",
    "trt.init_libnvinfer_plugins(TRT_LOGGER, \"\")\n",
    "print(f\"Register libnvinfer plugins\")\n",
    "registry = trt.get_plugin_registry()\n",
    "print(f\"Registry: {registry}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9761a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in registry.plugin_creator_list:\n",
    "    print(item.name, item.plugin_version, item.plugin_namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b121407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.fx.node import Argument, Target\n",
    "from typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, Union\n",
    "from torch_tensorrt.fx.converters.converter_utils import SourceIR\n",
    "from torch_tensorrt.fx.converter_registry import tensorrt_converter\n",
    "from torch_tensorrt.fx.tracer.acc_tracer.acc_op_properties import AccOpProperty, register_acc_op_properties\n",
    "from torch_tensorrt.fx.tracer.acc_tracer.acc_normalizer import (\n",
    "    register_acc_op,\n",
    "    register_acc_op_mapping,\n",
    "    register_custom_acc_mapper_fn,\n",
    ")\n",
    "from torch_tensorrt.fx.types import (\n",
    "    TRTNetwork,\n",
    "    TRTTensor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcabe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from torch_tensorrt.dynamo.backend._settings import CompilationSettings\n",
    "from torch_tensorrt.dynamo.backend.utils import prepare_inputs, prepare_device\n",
    "from torch_tensorrt.dynamo.backend.backends import aot_torch_tensorrt_aten_backend\n",
    "from torch_tensorrt.dynamo.backend._defaults import (\n",
    "    PRECISION,\n",
    "    DEBUG,\n",
    "    MAX_WORKSPACE_SIZE,\n",
    "    MIN_BLOCK_SIZE,\n",
    "    PASS_THROUGH_BUILD_FAILURES,\n",
    ")\n",
    "from torch._dynamo.backends.common import fake_tensor_unsupported\n",
    "from torch._functorch.aot_autograd import aot_module_simplified, make_boxed_compiler\n",
    "\n",
    "from nos.compilers.trt.backends import nos_tensorrt_backend\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# # @td.register_backend(name=\"torch_tensorrt\")\n",
    "# @fake_tensor_unsupported\n",
    "# def nos_tensorrt_backend(\n",
    "#     gm: torch.fx.GraphModule, sample_inputs: Sequence[torch.Tensor], **kwargs\n",
    "# ):\n",
    "#     DEFAULT_BACKEND = aot_torch_tensorrt_aten_backend\n",
    "#     print(gm)\n",
    "\n",
    "#     return DEFAULT_BACKEND(gm, sample_inputs, **kwargs)\n",
    "\n",
    "\n",
    "def create_backend(\n",
    "    precision: LowerPrecision = PRECISION,\n",
    "    debug: bool = DEBUG,\n",
    "    workspace_size: int = MAX_WORKSPACE_SIZE,\n",
    "    min_block_size: int = MIN_BLOCK_SIZE,\n",
    "    torch_executed_ops: Sequence[str] = set(),\n",
    "    pass_through_build_failures: bool = PASS_THROUGH_BUILD_FAILURES,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"Create torch.compile backend given specified arguments\n",
    "\n",
    "    Args:\n",
    "        precision: Model Layer precision\n",
    "        debug: Whether to print out verbose debugging information\n",
    "        workspace_size: Workspace TRT is allowed to use for the module (0 is default)\n",
    "        min_block_size: Minimum number of operators per TRT-Engine Block\n",
    "        torch_executed_ops: Sequence of operations to run in Torch, regardless of converter coverage\n",
    "        pass_through_build_failures: Whether to fail on TRT engine build errors (True) or not (False)\n",
    "    Returns:\n",
    "        Backend for torch.compile\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    settings = CompilationSettings(\n",
    "        debug=debug,\n",
    "        precision=precision,\n",
    "        workspace_size=workspace_size,\n",
    "        min_block_size=min_block_size,\n",
    "        torch_executed_ops=torch_executed_ops,\n",
    "        pass_through_build_failures=pass_through_build_failures,\n",
    "    )\n",
    "\n",
    "    return partial(\n",
    "        nos_tensorrt_backend,\n",
    "        settings=settings,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3842d613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import logging\n",
    "import collections.abc\n",
    "import torch_tensorrt\n",
    "from functools import partial\n",
    "\n",
    "from typing import Any, Sequence\n",
    "from torch_tensorrt import EngineCapability, Device\n",
    "from torch_tensorrt.fx.utils import LowerPrecision\n",
    "\n",
    "\n",
    "torch._dynamo.config.log_level = logging.INFO\n",
    "torch._dynamo.config.verbose = True\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "debug = True\n",
    "custom_backend = create_backend(\n",
    "    precision=LowerPrecision.FP16,\n",
    "    debug=debug,\n",
    "    workspace_size=MAX_WORKSPACE_SIZE,\n",
    "    min_block_size=MIN_BLOCK_SIZE,\n",
    "    torch_executed_ops=[],\n",
    ")\n",
    "\n",
    "\n",
    "# torch._dynamo.eval_frame.remove_from_cache(pipe.unet)\n",
    "with torch.inference_mode():\n",
    "    pipe.unet = torch.compile(pipe.unet, backend=custom_backend)\n",
    "\n",
    "    # Ensure compilation occurs by calling the function with provided inputs\n",
    "    pipe.unet(*args.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10eddcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 10\n",
    "with torch.inference_mode():\n",
    "    pipe.unet(*args.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f08331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def gelu_fn(x):\n",
    "#     \"\"\"\n",
    "#     https://github.com/geohot/tinygrad/blob/18892242b006785d4e92abae7c792e7874c17df9/tinygrad/tensor.py#L522\n",
    "#     \"\"\"\n",
    "#     return 0.5 * x * (1 + (x * 0.7978845608 * (1 + 0.044715 * x * x)).tanh())\n",
    "\n",
    "\n",
    "# # py/torch_tensorrt/fx/converters/impl/activation.py\n",
    "# def gelu(\n",
    "#     network: TRTNetwork,\n",
    "#     target: Target,\n",
    "#     source_ir: Optional[SourceIR],\n",
    "#     name: str,\n",
    "#     input_val: TRTTensor,\n",
    "# ):\n",
    "#     operation_type = trt.ActivationType.CLIP\n",
    "\n",
    "#     def gelu_dyn_range_fn(dyn_range):\n",
    "#         return gelu_fn(dyn_range[0]), gelu_fn(dyn_range[1])\n",
    "\n",
    "#     return convert_activation(\n",
    "#         network,\n",
    "#         target,\n",
    "#         source_ir,\n",
    "#         name,\n",
    "#         operation_type,\n",
    "#         input_val,\n",
    "#         dyn_range_fn=gelu_dyn_range_fn,\n",
    "#     )\n",
    "\n",
    "# # py/torch_tensorrt/fx/converters/nn_ops_converters.py\n",
    "# @tensorrt_converter(torch.nn.functional.gelu)\n",
    "# @tensorrt_converter(torch.nn.modules.activation.GELU)\n",
    "# def nn_ops_gelu(network, submod, args, kwargs, layer_name):\n",
    "#     # args/kwargs should have already been normalized to kwargs\n",
    "#     assert len(args) == 0\n",
    "\n",
    "#     return gelu(\n",
    "#         network=network,\n",
    "#         target=\"torch.nn.modules.activation.GELU\",\n",
    "#         source_ir=SourceIR.NN,\n",
    "#         name=layer_name,\n",
    "#         input_val=kwargs[\"input\"],\n",
    "#     )\n",
    "\n",
    "\n",
    "# # py/torch_tensorrt/fx/tracer/acc_tracer/acc_ops.py\n",
    "# @register_acc_op_properties(AccOpProperty.pointwise, AccOpProperty.unary)\n",
    "# @register_acc_op_mapping(op_and_target=(\"call_function\", torch.nn.functional.gelu))\n",
    "# @register_acc_op\n",
    "# def acc_ops_gelu(*, input, approximate='none'):\n",
    "#     assert approximate == 'none', \"Only approximate='none' supported\"\n",
    "#     return gelu_fn(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fabafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @register_acc_op_mapping(\n",
    "#     op_and_target=(\"call_function\", torch.nn.functional.group_norm),\n",
    "#     arg_replacement_tuples=[\n",
    "#         (\"input\", \"input\"),\n",
    "#         (\"num_groups\", \"num_groups\"),\n",
    "#         (\"weight\", \"weight\"),\n",
    "#         (\"bias\", \"bias\"),\n",
    "#         (\"eps\", \"eps\"),\n",
    "#     ],\n",
    "# )\n",
    "# @register_acc_op\n",
    "# def group_norm(*, input, num_groups, weight=None, bias=None, eps=1e-05):\n",
    "#     return torch.nn.functional.group_norm(\n",
    "#         input, num_groups, weight=weight, bias=bias, eps=eps\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4cd953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "\n",
    "# @register_acc_op_mapping(\n",
    "#     op_and_target=(\"call_function\", torch._C._nn.scaled_dot_product_attention),\n",
    "#     arg_replacement_tuples=[\n",
    "#         (\"query\", \"query\"),\n",
    "#         (\"key\", \"key\"),\n",
    "#         (\"value\", \"value\"),\n",
    "#         (\"attn_mask\", \"attn_mask\"),\n",
    "#         (\"dropout_p\", \"dropout_p\"),\n",
    "#         (\"is_causal\", \"is_causal\"),\n",
    "#     ],\n",
    "# )\n",
    "# @register_acc_op\n",
    "# def scaled_dot_product_attention(*, query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False):\n",
    "#     # Efficient implementation equivalent to the following:\n",
    "#     query_shape = list(query.size())\n",
    "#     key_shape = list(key.size())\n",
    "#     L = query_shape[len(query_shape)-2]\n",
    "#     S = key_shape[len(key_shape) - 2]\n",
    "#     attn_weight = (query @ key.transpose(-2, -1) / math.sqrt(query.size(-1)))\n",
    "#     if attn_mask is not None:\n",
    "#         attn_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0) if is_causal else attn_mask\n",
    "#         attn_mask = attn_mask.masked_fill(not attn_mask, -float('inf')) if attn_mask.dtype==torch.bool else attn_mask\n",
    "#         attn_weight += attn_mask\n",
    "#     attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "#     attn_weight = torch.nn.functional.dropout(attn_weight, dropout_p, training=False)\n",
    "#     return attn_weight @ value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288ce88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in registry.CONVERTERS.keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6f4a37-a7f5-4bc0-aebd-57d085e79379",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f90807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import fx\n",
    "from transformers.utils.fx import HFTracer\n",
    "\n",
    "device = \"cuda\"\n",
    "model = pipe.vae\n",
    "\n",
    "# https://github.com/huggingface/diffusers/blob/666743302ff5bd1e02c204b81a80e566648d60de/examples/community/stable_diffusion_tensorrt_txt2img.py#L493\n",
    "H, W = 512, 512\n",
    "B = 1\n",
    "sample = torch.rand(B, 3, H // 8, W // 8, dtype=torch.half, device=device)\n",
    "args = {\n",
    "    'sample': sample,\n",
    "}\n",
    "\n",
    "# Order matters\n",
    "concrete_args = {\n",
    "    \"sample_posterior\": False,\n",
    "    \"return_dict\": True,\n",
    "    \"generator\": None,\n",
    "}\n",
    "inputs = [\n",
    "    args['sample'], \n",
    " ]\n",
    "\n",
    "tracer = HFTracer()\n",
    "with torch.inference_mode():\n",
    "    traced_graph = tracer.trace(model, concrete_args=concrete_args, dummy_inputs=args)\n",
    "    traced_model = torch.fx.GraphModule(model, traced_graph)\n",
    "# print(traced_graph.print_tabular())\n",
    "\n",
    "traced_model.config = model.config\n",
    "# The model class must be stored as an attribute to allow model deserialization, which uses trace, and thus\n",
    "# _generate_dummy_input, where the model class is needed.\n",
    "traced_model.class_for_deserialization = model.__class__\n",
    "traced_model.device = model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89b9725",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(traced_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7c66bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4008ab5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, Optional, Type, Union\n",
    "import inspect\n",
    "\n",
    "from transformers.models.auto import get_values\n",
    "from transformers.models.auto.modeling_auto import MODEL_FOR_MULTIPLE_CHOICE_MAPPING_NAMES\n",
    "from transformers.utils.fx import HFTracer, _generate_random_int, _gen_constructor_wrapper\n",
    "from torch import fx\n",
    "from torch.fx import Graph, GraphModule, Proxy, Tracer\n",
    "\n",
    "\n",
    "class NOSTracer(HFTracer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def is_leaf_module(self, m: torch.nn.Module, module_qualified_name: str) -> bool:\n",
    "        print(f\"module: {m.__module__}\")\n",
    "        is_leaf = (super().is_leaf_module(m, module_qualified_name))\n",
    "        # is_leaf = is_leaf and not m.__module__.startswith(\"diffusers.models.attention_processor\")\n",
    "        # if is_leaf:\n",
    "        #     print(f\"leaf module: {m.__module__}\")\n",
    "        return is_leaf\n",
    "\n",
    "device = \"cuda\"\n",
    "model = pipe.unet\n",
    "\n",
    "# https://github.com/huggingface/diffusers/blob/666743302ff5bd1e02c204b81a80e566648d60de/examples/community/stable_diffusion_tensorrt_txt2img.py#L493\n",
    "H, W = 512, 512\n",
    "B = 1\n",
    "D = 1024  # D = (v1-5: 768, v2-1: 1024)\n",
    "unet_dim = 4 # pipe.unet.in_channels\n",
    "text_maxlen = 77\n",
    "# timestep = pipe.scheduler.timesteps[0]\n",
    "\n",
    "sample = torch.rand(2 * B, unet_dim, H // 8, W // 8, dtype=torch.half, device=device)\n",
    "timestep = torch.tensor([1, ], dtype=torch.int32, device=device)\n",
    "encoder_hidden_states = torch.rand((2 * B, text_maxlen, D), dtype=torch.half, device=device)  # 2B, 77, 1024\n",
    "\n",
    "# timestep_condition = torch.cat([noise_level_embed, text_pooler_out], dim=1)\n",
    "args = {\n",
    "    'sample': sample,\n",
    "    'timestep' : timestep,\n",
    "    'encoder_hidden_states': encoder_hidden_states,\n",
    "}\n",
    "\n",
    "# Order matters\n",
    "concrete_args = {\n",
    "    'class_labels': None,\n",
    "    'timestep_cond' : None, # timestep_condition,\n",
    "    'attention_mask': None,\n",
    "    'cross_attention_kwargs': None,\n",
    "    'added_cond_kwargs': None,\n",
    "    'down_block_additional_residuals': None,\n",
    "    'mid_block_additional_residual': None,\n",
    "    'encoder_attention_mask':None,\n",
    "    'return_dict': True,\n",
    "    'hidden_states': None,\n",
    "    'cond_proj_dim' : 1,\n",
    "    'condition': None,\n",
    "    'use_ada_layer_norm': True,\n",
    "    'input_ndim': 4,\n",
    "}\n",
    "inputs = [\n",
    "    args['sample'], \n",
    "    args['timestep'], \n",
    "    args['encoder_hidden_states']\n",
    "]\n",
    "\n",
    "\n",
    "# tracer = NOSTracer()\n",
    "# with torch.inference_mode():\n",
    "#     traced_graph = tracer.trace(model, concrete_args={**concrete_args}, dummy_inputs=args)\n",
    "#     traced_model = torch.fx.GraphModule(model, traced_graph)\n",
    "# # print(\">>\" * 80)\n",
    "# # traced_graph.print_tabular()\n",
    "\n",
    "# traced_model.config = model.config\n",
    "# # The model class must be stored as an attribute to allow model deserialization, which uses trace, and thus\n",
    "# # _generate_dummy_input, where the model class is needed.\n",
    "# traced_model.class_for_deserialization = model.__class__\n",
    "# traced_model.device = model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f3620e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers.models.clip.modeling_clip import CLIPVisionTransformer, CLIPAttention, CLIPVisionEmbeddings, CLIPEncoder\n",
    "from typing import (\n",
    "    Any,\n",
    "    Callable,\n",
    "    cast,\n",
    "    Dict,\n",
    "    Iterable,\n",
    "    Optional,\n",
    "    Sequence,\n",
    "    Set,\n",
    "    Tuple,\n",
    "    Type,\n",
    "    Union,\n",
    ")\n",
    "from torch_tensorrt.fx.tracer.acc_tracer import acc_normalizer, acc_ops, acc_shape_prop, acc_utils  # noqa: F401\n",
    "from torch_tensorrt.fx.tracer.acc_tracer.acc_tracer import _remove_assertions, _remove_exceptions, _replace_tensor_meta_with_rank, _replace_transpose_last_dims, AccRewritingTracer\n",
    "from torch.fx.experimental.normalize import NormalizeArgs\n",
    "\n",
    "\n",
    "\n",
    "def my_compiler(gm: torch.fx.GraphModule, inputs):\n",
    "    # acc_tracer is a custom fx tracer that maps nodes whose targets are PyTorch operators\n",
    "    # to acc ops.\n",
    "    # traced = acc_tracer.trace(model, concrete_args=input_kwargs)\n",
    "    with torch.inference_mode():\n",
    "        gm = acc_tracer.trace(\n",
    "            gm, inputs,\n",
    "        )\n",
    "\n",
    "    return gm\n",
    "\n",
    "\n",
    "import logging\n",
    "torch._dynamo.eval_frame.remove_from_cache(pipe.unet)\n",
    "torch._dynamo.config.log_level = logging.INFO\n",
    "torch._dynamo.config.verbose = True\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "pipe.unet.to(memory_format=torch.channels_last)\n",
    "pipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)\n",
    "# model = torch.compile(pipe.unet, backend=my_compiler)\n",
    "# model(*inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac89a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(traced_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835258ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b210b9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "NORM = math.sqrt(512)\n",
    "torch.fx.wrap('len')\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value, norm, attn_mask=None, dropout_p=0.0, is_causal=False):\n",
    "    attn_weight = (query / norm).half() @ key.transpose(-2, -1)\n",
    "    attn_weight = torch.nn.functional.softmax(attn_weight, dim=-1)\n",
    "    attn_weight = torch.nn.functional.dropout(attn_weight, dropout_p, training=False)\n",
    "    return attn_weight @ value\n",
    "\n",
    "def nn_functional_sdpa(q, k, v):\n",
    "    return torch.nn.functional.scaled_dot_product_attention(q, k, v)\n",
    "\n",
    "def ours_sdpa(q, k, v, **kwargs):\n",
    "    # B, C, K, E = q.shape\n",
    "    # norm = math.sqrt(torch.tensor([E,]).half().cuda())\n",
    "    return scaled_dot_product_attention(q, k, v, NORM)\n",
    "\n",
    "\n",
    "def replace_patterns(traced_model, patterns, new_op):\n",
    "    found = 0\n",
    "    # Go through all the nodes in the Graph\n",
    "    for idx, n in enumerate(traced_model.graph.nodes):\n",
    "        # If the target matches one of the patterns\n",
    "        if any(n.target == pattern for pattern in patterns):\n",
    "            print(\"Found: \", n.target, n.name, n.op, n.args, n.kwargs)\n",
    "            found += 1\n",
    "            # Set the insert point, add the new node, and replace all uses\n",
    "            # of `n` with the new node\n",
    "            with traced_model.graph.inserting_after(n):\n",
    "                new_node = traced_model.graph.call_function(new_op, n.args, n.kwargs)\n",
    "                n.replace_all_uses_with(new_node)\n",
    "            # Remove the old node from the graph\n",
    "            traced_model.graph.erase_node(n)\n",
    "    print(f\"Found: {found}\")\n",
    "    _ = traced_model.recompile()\n",
    "    \n",
    "sdpa_patterns = set([\n",
    "    torch._C._nn.scaled_dot_product_attention, \n",
    "    torch.nn.functional.scaled_dot_product_attention\n",
    "])\n",
    "replace_patterns(traced_model, sdpa_patterns, ours_sdpa)\n",
    "# replace_pattern(traced_model, nn_functional_sdpa, ours_sdpa)\n",
    "# print(traced_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1308f019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    return torch.nn.functional.gelu(x)\n",
    "\n",
    "def gelu_fn(x):\n",
    "    \"\"\"\n",
    "    https://github.com/geohot/tinygrad/blob/18892242b006785d4e92abae7c792e7874c17df9/tinygrad/tensor.py#L522\n",
    "    \"\"\"\n",
    "    return 0.5 * x * (1 + (x * 0.7978845608 * (1 + 0.044715 * x * x)).tanh())\n",
    "\n",
    "gelu_patterns = set([\n",
    "    torch.nn.functional.gelu\n",
    "])\n",
    "replace_patterns(traced_model, gelu_patterns, gelu_fn)\n",
    "for idx, n in enumerate(traced_model.graph.nodes):\n",
    "    print(n.name)\n",
    "#     if \"gelu\" in n.name:\n",
    "#         print(n.op, n.target, n.name, n.args, n.kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c68f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    output = model(*inputs)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a36083",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    output = traced_model()\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ddcfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 10\n",
    "with torch.inference_mode():\n",
    "    model(*inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4488ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 10\n",
    "with torch.inference_mode():\n",
    "    traced_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587bdddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    trt_compiled = torch_tensorrt.fx.compile(\n",
    "        traced_model, \n",
    "        inputs,\n",
    "        min_acc_module_size = 100, \n",
    "        max_batch_size = 2048, \n",
    "        max_workspace_size=33_554_432, \n",
    "        explicit_batch_dimension=False, \n",
    "        lower_precision=LowerPrecision.FP32, \n",
    "        verbose_log=False, \n",
    "        timing_cache_prefix='', \n",
    "        save_timing_cache=False, \n",
    "        cuda_graph_batch_size=- 1, \n",
    "        dynamic_batch=True, \n",
    "        is_aten=False, \n",
    "        use_experimental_fx_rt=False, \n",
    "        correctness_atol=0.1, \n",
    "        correctness_rtol=0.1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601cf528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_tensorrt.fx import compile\n",
    "# from torch_tensorrt.fx.tools.common_fx2trt import InputTensorSpec\n",
    "\n",
    "# # fx_only = torch.fx.symbolic_trace(model)\n",
    "# # fx_only(input)\n",
    "\n",
    "# trt_mod = compile(pipe.unet, \n",
    "#                   [InputTensorSpec.from_tensor(input) for input in inputs], \n",
    "#                   # min_acc_module_size=1, \n",
    "#                   explicit_batch_dimension=True, \n",
    "#                   dynamic_batch=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a2c7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers.models.clip.modeling_clip import CLIPVisionTransformer, CLIPAttention, CLIPVisionEmbeddings, CLIPEncoder\n",
    "from typing import (\n",
    "    Any,\n",
    "    Callable,\n",
    "    cast,\n",
    "    Dict,\n",
    "    Iterable,\n",
    "    Optional,\n",
    "    Sequence,\n",
    "    Set,\n",
    "    Tuple,\n",
    "    Type,\n",
    "    Union,\n",
    ")\n",
    "from torch_tensorrt.fx.tracer.acc_tracer import acc_normalizer, acc_ops, acc_shape_prop, acc_utils  # noqa: F401\n",
    "from torch_tensorrt.fx.tracer.acc_tracer.acc_tracer import _remove_assertions, _remove_exceptions, _replace_tensor_meta_with_rank, _replace_transpose_last_dims, AccRewritingTracer\n",
    "from torch.fx.experimental.normalize import NormalizeArgs\n",
    "\n",
    "\n",
    "# acc_tracer is a custom fx tracer that maps nodes whose targets are PyTorch operators\n",
    "# to acc ops.\n",
    "# traced = acc_tracer.trace(model, concrete_args=input_kwargs)\n",
    "with torch.inference_mode():\n",
    "    trt_traced = acc_tracer.trace(\n",
    "        traced_model, inputs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631af132",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trt_traced.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2161553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 10\n",
    "model(*inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bc5e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    output = trt_traced(*inputs)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963f066f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 10\n",
    "trt_traced(*inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81629e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitter will split the model into several submodules. The name of submodules will\n",
    "# be either `run_on_acc_{}` or `run_on_gpu_{}`. Submodules named `run_on_acc_{}` can\n",
    "# be fully lowered to TensorRT via fx2trt while submodules named `run_on_gpu_{}` has\n",
    "# unsupported ops and can't be lowered by fx2trt. We can still run `run_on_gpu_{}`\n",
    "# submodules on Gpu if ops there have cuda implementation, the naming is a bit\n",
    "# confusing and we'll improve it.\n",
    "\n",
    "settings = TRTSplitterSetting()\n",
    "# settings.use_implicit_batch_dim = False\n",
    "# settings.use_experimental_rt = False\n",
    "\n",
    "with torch.inference_mode():\n",
    "    splitter = TRTSplitter(trt_traced, inputs, settings=settings)\n",
    "splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ba4eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview functionality allows us to see what are the supported ops and unsupported\n",
    "# ops. We can optionally the dot graph which will color supported ops and unsupported\n",
    "# ops differently.\n",
    "_ = splitter.node_support_preview(dump_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07da5234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !dot -Tpdf -Gdpi=1000 node_support.dot > node_support.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc63302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image\n",
    "# import pydot\n",
    "\n",
    "# graphs = pydot.graph_from_dot_file(\"node_support.dot\")\n",
    "# Image(graphs[0].create_png())\n",
    "# from IPython.display import SVG\n",
    "\n",
    "# SVG(\"node_support.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb84e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Non Acc Nodes\")\n",
    "print(splitter.non_acc_submodule_name)\n",
    "\n",
    "# splitter.sample_input\n",
    "# splitter.split_preview()\n",
    "# dir(splitter)\n",
    "print(\"Acc Nodes\")\n",
    "print(len(splitter.acc_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5416d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitter.remove_small_acc_subgraphs?\n",
    "# dir(splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66266ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split.\n",
    "split_mod = splitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf0c2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(split_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9336f6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After split we have three submodules, _run_on_acc_0 and _run_on_gpu_1.\n",
    "print(split_mod.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d62e499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_submod_inputs(_mod, _submod, _inputs):\n",
    "    acc_inputs = None\n",
    "\n",
    "    def get_input(self, inputs):\n",
    "        nonlocal acc_inputs\n",
    "        acc_inputs = inputs\n",
    "\n",
    "    handle = _submod.register_forward_pre_hook(get_input)\n",
    "    with torch.inference_mode():\n",
    "        _mod()\n",
    "    handle.remove()\n",
    "    return acc_inputs\n",
    "\n",
    "\n",
    "# Since the model is splitted into three segments. We need to lower each TRT eligible segment.\n",
    "# If we know the model can be fully lowered, we can skip the splitter part.\n",
    "for name, _ in split_mod.named_children():\n",
    "    print(f\"Splitting {name}\")\n",
    "    if \"_run_on_acc\" in name:\n",
    "        submod = getattr(split_mod, name)\n",
    "\n",
    "        # Get submodule inputs for fx2trt\n",
    "        acc_inputs = get_submod_inputs(split_mod, submod, inputs)\n",
    "\n",
    "        # fx2trt replacement\n",
    "        interp = TRTInterpreter(\n",
    "            submod,\n",
    "            InputTensorSpec.from_tensors(acc_inputs),\n",
    "            explicit_batch_dimension=True,\n",
    "        )\n",
    "        r = interp.run(lower_precision=LowerPrecision.FP16)\n",
    "        trt_mod = TRTModule(*r)\n",
    "        setattr(split_mod, name, trt_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b544d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# from nos.constants import NOS_MODELS_DIR\n",
    "\n",
    "# model_dir = Path(NOS_MODELS_DIR, f\"cache/{MODEL_NAME}\")\n",
    "# model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# lowered_model_output = split_mod(*inputs)\n",
    "\n",
    "# Save and load model\n",
    "# W, H = 224, 224\n",
    "# model_id = MODEL_NAME.replace(\"/\", \"-\") + \"_\" + f\"{W}x{H}\" + \"_\" + \"fp16\"\n",
    "# filename = f\"{model_dir}/{model_id}.torchtrt.pt\"\n",
    "filename = \"vae_224x224.torchtrt.pt\"\n",
    "torch.save(split_mod, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1c059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading torchtrt filename: {filename}\")\n",
    "reload_trt_mod = torch.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b413f4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    reload_model_output = reload_trt_mod(*inputs)\n",
    "reload_model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e65337a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the results match\n",
    "with torch.inference_mode():\n",
    "    regular_model_output = model(*inputs)\n",
    "regular_model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4886341",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 100\n",
    "with torch.inference_mode():\n",
    "    _ = model(*inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4727827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 100\n",
    "with torch.inference_mode():\n",
    "    _ = reload_trt_mod(*inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01fbdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for o1, o2 in zip(reload_model_output, regular_model_output):\n",
    "    torch.testing.assert_close(\n",
    "        o1.cpu().float(), o2.cpu().float(), rtol=2e-02, atol=2e-02, equal_nan=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e95a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 100\n",
    "with torch.inference_mode():\n",
    "    outputs = model(*inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ddcf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monkeypatch backbone\n",
    "model.backbone = reload_trt_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21597333",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 100\n",
    "inputs_fp16 = [inp.half() for inp in inputs]\n",
    "with torch.inference_mode():\n",
    "    outputs = model(*inputs_fp16)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700aaad5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ae9e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils.fx import symbolic_trace\n",
    "\n",
    "\n",
    "# traced_model = symbolic_trace(model, input_names=[\"input_ids\", \"attention_mask\", \"position_ids\", \"output_attentions\", \"output_hidden_states\", \"return_dictd\"])\n",
    "# traced = fx.symbolic_trace(\n",
    "#     model, \n",
    "#     concrete_args={'sample': torch.zeros(2, 4, 64, 64, dtype=torch.half, device='cuda:0'), \n",
    "#                    'timestep': torch.tensor([1, 3], dtype=torch.int32, device='cuda:0'),\n",
    "#                    'encoder_hidden_states': torch.randn(2, 77, 768, dtype=torch.half, device='cuda:0'),\n",
    "#                    'timestep_cond': None, \n",
    "#                    'attention_mask': None,\n",
    "#                    'cross_attention_kwargs': None,\n",
    "#                    'added_cond_kwargs': None,\n",
    "#                    'down_block_additional_residuals': None,\n",
    "#                    'mid_block_additional_residual': None,\n",
    "#                    'encoder_attention_mask': None}\n",
    "# )\n",
    "# print(traced.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd330d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scripted_model = torch.jit.script(model.backbone)\n",
    "# compile_settings = {\n",
    "#     \"inputs\": [torch_tensorrt.Input(\n",
    "#         min_shape=[1, 3, 480, 640],\n",
    "#         opt_shape=[1, 3, 960, 1280],\n",
    "#         max_shape=[4, 3, 960, 1280],\n",
    "#     )],\n",
    "#     \"enabled_precisions\": {torch.float}  # Run with FP16 (torch.half)\n",
    "# }\n",
    "# trt_ts_module = torch_tensorrt.compile(scripted_model, compile_settings)\n",
    "# input_data = torch.randn((1, 3, 480, 640))\n",
    "# input_data = input_data.to(\"cuda\")\n",
    "# pytorch_out = model.forward(input_data)\n",
    "# torch_tensorrt_out = trt_ts_module(input_data)\n",
    "\n",
    "\n",
    "# print('PyTorch output: \\n', pytorch_out[0, :, :, 0])\n",
    "\n",
    "# trt_ts_module = torch_tensorrt.compile(model.backbone,\n",
    "#     # If the inputs to the module are plain Tensors, specify them via the `inputs` argument:\n",
    "#     inputs = [ # Provide example tensor for input shape or...\n",
    "#         torch_tensorrt.Input( # Specify input object with shape and dtype\n",
    "#             min_shape=[1, 3, 480, 640],\n",
    "#             opt_shape=[1, 3, 480, 640],\n",
    "#             max_shape=[1, 3, 960, 1280],\n",
    "#             # For static size shape=[1, 3, 224, 224]\n",
    "#             dtype=torch.float) # Datatype of input tensor. Allowed options torch.(float|half|int8|int32|bool)\n",
    "#     ],\n",
    "\n",
    "#     # For inputs containing tuples or lists of tensors, use the `input_signature` argument:\n",
    "#     # Below, we have an input consisting of a Tuple of two Tensors (Tuple[Tensor, Tensor])\n",
    "#     # input_signature = ( (torch_tensorrt.Input(shape=[1, 3, 224, 224], dtype=torch.half),\n",
    "#     #                      torch_tensorrt.Input(shape=[1, 3, 224, 224], dtype=torch.half)), ),\n",
    "\n",
    "#     enabled_precisions = {torch.float}, # Run with FP16\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
