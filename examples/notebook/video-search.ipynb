{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1JIIlkTWa2xbft5bTpzhGK1BxYL83bJNU\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# üî• Video Search Demo\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we‚Äôll use NOS to build an end-to-end semantic video search utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nos.test.utils import get_benchmark_video\n",
    "get_benchmark_video()\n",
    "FILENAME = \"test_video.mp4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"test_video.mp4\" controls  width=\"640\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "Video(FILENAME, width=640)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî• 1. Inference with ü§ó transformers (OpenAI CLIP)\n",
    "---\n",
    "\n",
    "Let's say we're using the popular OpenAI CLIP for extracting image and text embeddings, and we're using the ü§ó transformers library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Union, List\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class CLIP:\n",
    "    \"\"\"Text and image encoding using OpenAI CLIP\"\"\"\n",
    "    def __init__(self, model_name: str = \"openai/clip-vit-base-patch32\"):\n",
    "        from transformers import CLIPModel\n",
    "        from transformers import CLIPProcessor, CLIPTokenizer\n",
    "        \n",
    "        self.processor = CLIPProcessor.from_pretrained(model_name)\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(model_name)\n",
    "        \n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = CLIPModel.from_pretrained(model_name).to(device)\n",
    "        self.model.eval()\n",
    "        self.device = self.model.device\n",
    "        \n",
    "    def encode_image(self, images: Union[Image.Image, np.ndarray, List[Image.Image], List[np.ndarray]]):\n",
    "        \"\"\"Encode image into an embedding.\"\"\"\n",
    "        with torch.inference_mode():\n",
    "            inputs = self.processor(images=images, return_tensors=\"pt\").to(self.device)\n",
    "            return self.model.get_image_features(**inputs).cpu().numpy()\n",
    "\n",
    "    def encode_text(self, texts: Union[str, List[str]]) -> np.ndarray:\n",
    "        \"\"\"Encode text into an embedding.\"\"\"\n",
    "        with torch.inference_mode():\n",
    "            if isinstance(texts, str):\n",
    "                texts = [texts]\n",
    "            inputs = self.tokenizer(\n",
    "                texts,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(self.device)\n",
    "            text_features = self.model.get_text_features(**inputs)\n",
    "            return text_features.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frame Inference\n",
    "\n",
    "Let's embed the video frame by frame with the encoding scheme defined above to see the iterations per second:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nos.common import tqdm\n",
    "from nos.common.io.video.opencv import VideoReader\n",
    "\n",
    "# Load the first image\n",
    "images = VideoReader(FILENAME)\n",
    "image = next(images)\n",
    "\n",
    "# Load the Pytorch model\n",
    "clip = CLIP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Naive implementation: 284 images [00:05, 56.72 images/s]\n"
     ]
    }
   ],
   "source": [
    "for _ in tqdm(duration=5, desc=\"Naive implementation\", unit=\" images\"):\n",
    "    clip.encode_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. üîç Video Search\n",
    "\n",
    "The following snippet extracts embeddings from all the frames in the video and uses CLIP to cross-reference text queries with the image embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nos.client import Client, ask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                 | 0/6059 [00:00<?, ?images/s]"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m images \u001b[38;5;241m=\u001b[39m ({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: img} \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m tqdm(images, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Set up a model using the Module syntax introduced in 'welcome to NOS'\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241m.\u001b[39mModule(TaskType\u001b[38;5;241m.\u001b[39mIMAGE_EMBEDDING, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai/clip\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Batch inference using auto-scaled model, then normalize embeddings\u001b[39;00m\n\u001b[1;32m     12\u001b[0m video_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39mvstack(\u001b[38;5;28mlist\u001b[39m(model\u001b[38;5;241m.\u001b[39mimap(images))))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'client' is not defined"
     ]
    }
   ],
   "source": [
    "from nos.common import tqdm\n",
    "from nos.common.io.video.opencv import VideoReader\n",
    "\n",
    "# Load frames from the video lazily\n",
    "images = VideoReader(FILENAME)\n",
    "images = ({\"images\": img} for img in tqdm(images, unit=\"images\"))\n",
    "\n",
    "# Set up a model using the Module syntax introduced in 'welcome to NOS'\n",
    "model = client.Module(TaskType.IMAGE_EMBEDDING, \"openai/clip\")\n",
    "\n",
    "# Batch inference using auto-scaled model, then normalize embeddings\n",
    "video_features = torch.from_numpy(np.vstack(list(model.imap(images))))\n",
    "video_features /= video_features.norm(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "from nos.common.io import VideoReader\n",
    "\n",
    "encode_text = CLIP().encode_text\n",
    "video = VideoReader(FILENAME)\n",
    "\n",
    "def search_video(query: str, video_features: np.ndarray, topk: int = 3):\n",
    "    \"\"\"Semantic video search demo in 8 lines of code\"\"\"\n",
    "    # Encode text and normalize\n",
    "    with torch.inference_mode():\n",
    "        text_features = encode_text(texts=[query])\n",
    "        text_features = torch.from_numpy(text_features)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # Compute the similarity between the search query and each video frame\n",
    "    similarities = (video_features @ text_features.T)\n",
    "    _, best_photo_idx = similarities.topk(topk, dim=0)\n",
    "    \n",
    "    # Display the top k frames\n",
    "    results = np.hstack([video[int(frame_id)] for frame_id in best_photo_idx])\n",
    "    display(Image.fromarray(results).resize((600, 400)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Sample Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_video(\"golden gate bridge\", video_features, topk=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_video(\"alcatraz prison\", video_features, topk=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_video(\"fishermans wharf\", video_features, topk=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_video(\"golden gate park windmill\", video_features, topk=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_video(\"chinatown\", video_features, topk=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_video(\"lombard street\", video_features, topk=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_video(\"pier 39\", video_features, topk=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "search_video(\"riding the tram\", video_features, topk=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_video(\"ferry building\", video_features, topk=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
