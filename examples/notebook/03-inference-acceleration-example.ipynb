{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nos üî•: Inference Acceleration Tutorial (Advanced)\n",
    "\n",
    "**NOS** is a PyTorch library for optimizing and running lightning-fast inference of popular computer vision models. NOS inherits its name from \"Nitrous Oxide System\", the performance-enhancing system typically used in racing cars. NOS is designed to be modular and easy to extend.\n",
    "\n",
    "*Note:* We assume that you have already installed NOS. If not, please refer to the [installation instructions](https://autonomi-ai.github.io/nos/docs/QUICKSTART/) before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî• Accelerating a vanilla Pytorch Model\n",
    "\n",
    "Let's say you want to accelerate a vanilla Pytorch model such as [OpenAI CLIP](https://huggingface.co/openai/clip-vit-base-patch32). A typical implementation of the model would look like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from transformers import CLIPModel\n",
    "\n",
    "\n",
    "class CLIPVisionModel(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tself.device = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\t\tself.model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(self.device)\n",
    "\t\tself.model.eval()\n",
    "\n",
    "\tdef forward(self, images: Union[Image.Image, np.ndarray]):\n",
    "\t\treturn self.model.visual(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîå Registering the model to NOS\n",
    "\n",
    "Now that you have a vanilla Pytorch model, you can register it to NOS via the `RegisterModuleFromCls` method. This method takes in a class and registers it to NOS. By default, `RegisterModuleFromCls` expects a `__call__` method that takes in a list of inputs and returns a list of outputs. In our case, the `__call__` method is the `forward` method of the `CLIPVisionModel` class that you can override by providing the `method=\"forward\"` keyword argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nos.client import InferenceClient\n",
    "\n",
    "client = InferenceClient()\n",
    "clip = client.RegisterModuleFromCls(CLIPVisionModel, method=\"forward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have registered the CLIP model, the `clip` object returned is a NOS module that you can use to run inference directly from the client. The `clip` object has the same API as the original Pytorch model, but it is optimized for inference. You can also inspect the model's input and output schema via the `GetModelInfo` method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'inputs': [{'name': 'images', 'type': 'image'}],\n",
    "#  'outputs': [{'name': 'embeddings', 'type': 'tensor'}]}\n",
    "clip.GetModelInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Benchmarking Inference\n",
    "\n",
    "Let's run a simple benchmark to compare the performance of the original Pytorch model vs. the NOS-optimized model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üöÄ Running Inference\n",
    "\n",
    "Once you have registered the model, you can simply call the model remotely by calling `clip(...)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "images = [Image.open(\"dog.jpg\"), Image.open(\"cat.jpg\")]\n",
    "embeddings = clip(images=images)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö°Ô∏è Optimizing Inference\n",
    "\n",
    "NOS provides a convenient way to optimize the model for inference via the `Optimize` method. This method takes in a NOS module and optimizes it for inference. In this example, calling `Optimize` optimize the `forward` method explicitly as we have registered the `clip` module object with the `method=\"forward\"`. \n",
    "\n",
    "#### How does this work under the hood?\n",
    "Under the hood, NOS traces the `forward` method, lowers all potential model subgraphs to an IR, and tries its best to optimize the entire graph for inference. In the case that some subgraphs cannot be optimized, NOS will simply skip them and optimize the rest of the graph, patching the original model with the optimized subgraphs. All of this happens under the hood, so you don't have to worry about the details. Once the models are compiled and optimized, NOS caches the compilation artifacts for future re-use and returns the optimized model as a callable module.\n",
    "\n",
    "Let's do a simple benchmark to compare the performance of the original Pytorch model vs. the NOS-optimized model. Here, we will use the `timeit` module to run the benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Optimized Model Inference\n",
    "\n",
    "NOS also comes baked with a number of pre-optimized models that you can use out-of-the-box. Let's try running inference on the pre-optimized CLIP model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nos.client import TaskType\n",
    "\n",
    "# Load the model, and run inference once to warm up the model\n",
    "predictions = client.Run(TaskType.OBJECT_DETECTION_2D, \"yolox/small\", images=images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 100\n",
    "predictions = client.Run(TaskType.OBJECT_DETECTION_2D, \"yolox/small\", images=images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the optimized model, and run inference once to warm up the model\n",
    "predictions = client.Run(TaskType.OBJECT_DETECTION_2D, \"yolox/small-trt\", images=images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 100\n",
    "predictions = client.Run(TaskType.OBJECT_DETECTION_2D, \"yolox/small-trt\", images=images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
