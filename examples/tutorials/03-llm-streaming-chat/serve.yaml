images:
  llm-gpu:
    base: autonomi/nos:latest-gpu

models:
  TinyLlama/TinyLlama-1.1B-Chat-v1.0:
    model_cls: StreamingChat
    model_path: models/chat.py
    init_kwargs:
      model_name: TinyLlama/TinyLlama-1.1B-Chat-v1.0
    default_method: chat
    runtime_env: llm-gpu
    deployment:
      resources:
        device: auto
        device_memory: 3Gi
