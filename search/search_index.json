{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"\ud83c\udfe0 Home","text":"<p> Website | Docs | Tutorials | Playground | Blog | Discord </p> <p> </p> <p>NOS is a fast and flexible PyTorch inference server that runs on any cloud or AI HW.</p>"},{"location":"index.html#key-features","title":"\ud83d\udee0\ufe0f Key Features","text":"<ul> <li>\ud83d\udc69\u200d\ud83d\udcbb Easy-to-use: Built for PyTorch and designed to optimize, serve and auto-scale Pytorch models in production without compromising on developer experience.</li> <li>\ud83e\udd77 Multi-modal &amp; Multi-model: Serve multiple foundational AI models (LLMs, Diffusion, Embeddings, Speech-to-Text and Object Detection) simultaneously, in a single server.</li> <li>\u2699\ufe0f HW-aware Runtime: Deploy PyTorch models effortlessly on modern AI accelerators (NVIDIA GPUs, AWS Inferentia2, AMD - coming soon, and even CPUs).</li> <li>\u2601\ufe0f Cloud-agnostic Containers: Run on any cloud (AWS, GCP, Azure, Lambda Labs, On-Prem) with our ready-to-use inference server containers.</li> </ul>"},{"location":"index.html#whats-new","title":"\ud83d\udd25 What's New","text":"<ul> <li>[Feb 2024] \u270d\ufe0f [blog] Introducing the NOS Inferentia2 (<code>inf2</code>) runtime.</li> <li>[Jan 2024] \u270d\ufe0f [blog] Serving LLMs on a budget with SkyServe.</li> <li>[Jan 2024] \ud83d\udcda [docs] NOS x SkyPilot Integration page!</li> <li>[Jan 2024] \u270d\ufe0f [blog] Getting started with NOS tutorials is available here!</li> <li>[Dec 2023] \ud83d\udedd [repo] We open-sourced the NOS playground to help you get started with more examples built on NOS!</li> </ul>"},{"location":"index.html#quickstart","title":"\ud83d\ude80 Quickstart","text":"<p>We highly recommend that you go to our quickstart guide to get started. To install the NOS client, you can run the following command:</p> <pre><code>conda create -n nos python=3.8 -y\nconda activate nos\npip install torch-nos\n</code></pre> <p>Once the client is installed, you can start the NOS server via the NOS <code>serve</code> CLI. This will automatically detect your local environment, download the docker runtime image and spin up the NOS server:</p> <pre><code>nos serve up --http --logging-level INFO\n</code></pre> <p>You are now ready to run your first inference request with NOS! You can run any of the following commands to try things out. You can set the logging level to <code>DEBUG</code> if you want more detailed information from the server.</p>"},{"location":"index.html#what-can-nos-do","title":"\ud83d\udc69\u200d\ud83d\udcbb What can NOS do?","text":""},{"location":"index.html#chat-llm-agents-chatgpt-as-a-service","title":"\ud83d\udcac Chat / LLM Agents (ChatGPT-as-a-Service)","text":"<p>NOS provides an OpenAI-compatible server with streaming support so that you can connect your favorite OpenAI-compatible LLM client to talk to NOS.</p> <p></p> <p></p>  API / Usage gRPC API \u26a1 <pre><code>from nos.client import Client\n\nclient = Client()\n\nmodel = client.Module(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\nresponse = model.chat(message=\"Tell me a story of 1000 words with emojis\", _stream=True)\n</code></pre> REST API <pre><code>curl \\\n-X POST http://localhost:8000/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"model\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    \"messages\": [{\n        \"role\": \"user\",\n        \"content\": \"Tell me a story of 1000 words with emojis\"\n    }],\n    \"temperature\": 0.7,\n    \"stream\": true\n  }'\n</code></pre>"},{"location":"index.html#image-generation-stable-diffusion-as-a-service","title":"\ud83c\udfde\ufe0f Image Generation (Stable-Diffusion-as-a-Service)","text":"<p>Build MidJourney discord bots in seconds.</p> <p></p> <p></p>  API / Usage gRPC API \u26a1 <pre><code>from nos.client import Client\n\nclient = Client()\n\nsdxl = client.Module(\"stabilityai/stable-diffusion-xl-base-1-0\")\nimage, = sdxl(prompts=[\"hippo with glasses in a library, cartoon styling\"],\n              width=1024, height=1024, num_images=1)\n</code></pre> REST API <pre><code>curl \\\n-X POST http://localhost:8000/v1/infer \\\n-H 'Content-Type: application/json' \\\n-d '{\n    \"model_id\": \"stabilityai/stable-diffusion-xl-base-1-0\",\n    \"inputs\": {\n        \"prompts\": [\"hippo with glasses in a library, cartoon styling\"],\n        \"width\": 1024, \"height\": 1024,\n        \"num_images\": 1\n    }\n}'\n</code></pre>"},{"location":"index.html#text-image-embedding-clip-as-a-service","title":"\ud83e\udde0 Text &amp; Image Embedding (CLIP-as-a-Service)","text":"<p>Build scalable semantic search of images/videos in minutes.</p> <p></p> <p></p>  API / Usage gRPC API \u26a1 <pre><code>from nos.client import Client\n\nclient = Client()\n\nclip = client.Module(\"openai/clip-vit-base-patch32\")\ntxt_vec = clip.encode_text(texts=[\"fox jumped over the moon\"])\n</code></pre> REST API <pre><code>curl \\\n-X POST http://localhost:8000/v1/infer \\\n-H 'Content-Type: application/json' \\\n-d '{\n    \"model_id\": \"openai/clip-vit-base-patch32\",\n    \"method\": \"encode_text\",\n    \"inputs\": {\n        \"texts\": [\"fox jumped over the moon\"]\n    }\n}'\n</code></pre>"},{"location":"index.html#audio-transcription-whisper-as-a-service","title":"\ud83c\udf99\ufe0f Audio Transcription (Whisper-as-a-Service)","text":"<p>Perform real-time audio transcription using Whisper.</p> <p></p> <p></p>  API / Usage gRPC API \u26a1 <pre><code>from pathlib import Path\nfrom nos.client import Client\n\nclient = Client()\n\nmodel = client.Module(\"openai/whisper-small.en\")\nwith client.UploadFile(Path(\"audio.wav\")) as remote_path:\n  response = model(path=remote_path)\n# {\"chunks\": ...}\n</code></pre> REST API <pre><code>curl \\\n-X POST http://localhost:8000/v1/infer/file \\\n-H 'accept: application/json' \\\n-H 'Content-Type: multipart/form-data' \\\n-F 'model_id=openai/whisper-small.en' \\\n-F 'file=@audio.wav'\n</code></pre>"},{"location":"index.html#object-detection-yolox-as-a-service","title":"\ud83e\uddd0 Object Detection (YOLOX-as-a-Service)","text":"<p>Run classical computer-vision tasks in 2 lines of code.</p> <p></p> <p></p>  API / Usage gRPC API \u26a1 <pre><code>from pathlib import Path\nfrom nos.client import Client\n\nclient = Client()\n\nmodel = client.Module(\"yolox/medium\")\nresponse = model(images=[Image.open(\"image.jpg\")])\n</code></pre> REST API <pre><code>curl \\\n-X POST http://localhost:8000/v1/infer/file \\\n-H 'accept: application/json' \\\n-H 'Content-Type: multipart/form-data' \\\n-F 'model_id=yolox/medium' \\\n-F 'file=@image.jpg'\n</code></pre>"},{"location":"index.html#custom-models","title":"\u2692\ufe0f Custom models","text":"<p>Want to run models not supported by NOS? You can easily add your own models following the examples in the NOS Playground.</p>"},{"location":"index.html#license","title":"\ud83d\udcc4 License","text":"<p>This project is licensed under the Apache-2.0 License.</p>"},{"location":"index.html#telemetry","title":"\ud83d\udce1 Telemetry","text":"<p>NOS collects anonymous usage data using Sentry. This is used to help us understand how the community is using NOS and to help us prioritize features. You can opt-out of telemetry by setting <code>NOS_TELEMETRY_ENABLED=0</code>.</p>"},{"location":"index.html#contributing","title":"\ud83e\udd1d Contributing","text":"<p>We welcome contributions! Please see our contributing guide for more information.</p>"},{"location":"index.html#quick-links","title":"\ud83d\udd17  Quick Links","text":"<ul> <li>\ud83d\udcac Send us an email at support@autonomi.ai or join our Discord for help.</li> <li>\ud83d\udce3 Follow us on Twitter, and LinkedIn to keep up-to-date on our products.</li> </ul>"},{"location":"docs/CONTRIBUTING.html","title":"Contribution Guide","text":""},{"location":"docs/CONTRIBUTING.html#lint","title":"Lint","text":"<pre><code>make lint  # Runs all the linters using ruff/pre-commit\n</code></pre>"},{"location":"docs/CONTRIBUTING.html#test","title":"Test","text":"<pre><code>make test  # Runs all the basic tests using pytest\n</code></pre>"},{"location":"docs/CONTRIBUTING.html#benchmark","title":"Benchmark","text":"<pre><code>make test-benchmarks  # Runs all the benchmarks setting NOS_TEST_BENCHMARK=1\n</code></pre>"},{"location":"docs/OVERVIEW.html","title":"OVERVIEW","text":"<p> Website | Docs |  Discord </p>"},{"location":"docs/OVERVIEW.html#what-is-nos","title":"\u26a1\ufe0f What is NOS?","text":"<p>NOS (<code>torch-nos</code>) is a fast and flexible Pytorch inference server, specifically designed for optimizing and running inference of popular foundational AI models.</p>"},{"location":"docs/OVERVIEW.html#why-use-nos","title":"\ud83e\udd14 Why use NOS?","text":"<ul> <li>\ud83d\udc69\u200d\ud83d\udcbb Easy-to-use: Built for PyTorch and designed to optimize, serve and auto-scale Pytorch models in production without compromising on developer experience.</li> <li>\ud83e\udd77 Flexible: Run and serve several foundational AI models (Stable Diffusion, CLIP, Whisper) in a single place.</li> <li>\ud83d\udd0c Pluggable: Plug your front-end to NOS with out-of-the-box high-performance gRPC/REST APIs, avoiding all kinds of ML model deployment hassles.</li> <li>\ud83d\ude80 Scalable: Optimize and scale models easily for maximum HW performance without a PhD in ML, distributed systems or infrastructure.</li> <li>\ud83d\udce6 Extensible: Easily hack and add custom models, optimizations, and HW-support in a Python-first environment.</li> <li>\u2699\ufe0f HW-accelerated: Take full advantage of your underlying HW (GPUs, ASICs) without compromise.</li> <li>\u2601\ufe0f Cloud-agnostic: Run on any cloud HW (AWS, GCP, Azure, Lambda Labs, On-Prem) with our ready-to-use inference server containers.</li> </ul> <p>NOS inherits its name from Nitrous Oxide System, the performance-enhancing system typically used in racing cars. NOS is designed to be modular and easy to extend.</p>"},{"location":"docs/OVERVIEW.html#getting-started","title":"\ud83d\ude80 Getting Started","text":"<p>To quickly get started with a light-weight NOS client, run the following command:</p> <pre><code>$ conda create -n nos-py310 python=3.10\n$ conda activate nos-py310\n$ pip install torch-nos\n</code></pre> <p>In the setup above, the client pulls and utilizes the NOS docker server that runs in the background. We do expect users to have installed Docker with NVIDIA support and Docker Compose. For a more detailed quickstart, navigate to our quickstart docs.</p> <p>If you're interested in developing or contributing new models to NOS, consider installing the full NOS server via pip/conda:</p> <pre><code>$ conda create -n nos-py310 python=3.10\n$ conda activate nos-py310\n$ conda install pytorch&gt;=2.0.1 torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n$ pip install torch-nos[server]\n</code></pre>"},{"location":"docs/OVERVIEW.html#quickstart-show-me-the-code","title":"\ud83d\udd25 Quickstart / Show me the code","text":""},{"location":"docs/OVERVIEW.html#start-the-gpu-server","title":"\u26a1\ufe0f Start the GPU server","text":"<p>The quickest way to get started is to start the GPU server. The <code>--http</code> flag optionally starts an HTTP gateway server so that you can run the REST API examples. We recommend you test out the gRPC client API to get the most out-of-the-box performance.</p> <pre><code>nos serve up --http\n</code></pre> <p>This command pulls and starts the latest GPU docker server with all the NOS goodies, without you requiring to manually do any setup. You'll see a bunch of debug logs on the console, wait until you see <code>Uvicorn running on http://0.0.0.0:8000</code> before continuing to the next section. To follow the remaining examples, start a new terminal (leaving the server running in the background).</p>"},{"location":"docs/OVERVIEW.html#image-generation-stable-diffusion-as-a-service","title":"\ud83c\udfde\ufe0f Image Generation (Stable-Diffusion-as-a-Service)","text":"gRPC API \u26a1   REST API  <pre><code>from nos.client import Client\n\nclient = Client()\n\nsdxl = client.Module(\"stabilityai/stable-diffusion-xl-base-1-0\")\nimage, = sdxl(prompts=[\"fox jumped over the moon\"],\n              width=1024, height=1024, num_images=1)\n</code></pre> <pre><code>curl \\\n-X POST http://localhost:8000/v1/infer \\\n-H 'Content-Type: application/json' \\\n-d '{\n    \"model_id\": \"stabilityai/stable-diffusion-xl-base-1-0\",\n    \"inputs\": {\n        \"prompts\": [\"fox jumped over the moon\"],\n        \"width\": 1024,\n        \"height\": 1024,\n        \"num_images\": 1\n    }\n}'\n</code></pre>"},{"location":"docs/OVERVIEW.html#text-image-embedding-clip-as-a-service","title":"\ud83e\udde0 Text &amp; Image Embedding (CLIP-as-a-Service)","text":"gRPC API \u26a1   REST API  <pre><code>from nos.client import Client\n\nclient = Client()\n\nclip = client.Module(\"openai/clip-vit-base-patch32\")\ntxt_vec = clip.encode_text(texts=[\"fox jumped over the moon\"])\n</code></pre> <pre><code>curl \\\n-X POST http://localhost:8000/v1/infer \\\n-H 'Content-Type: application/json' \\\n-d '{\n    \"model_id\": \"openai/clip-vit-base-patch32\",\n    \"method\": \"encode_text\",\n    \"inputs\": {\n        \"texts\": [\"fox jumped over the moon\"]\n    }\n}'\n</code></pre>"},{"location":"docs/OVERVIEW.html#audio-transcription-whisper-as-a-service","title":"\ud83c\udf99\ufe0f Audio Transcription (Whisper-as-a-Service)","text":"gRPC API \u26a1   REST API  <pre><code>from pathlib import Path\nfrom nos.client import Client\n\nclient = Client()\n\nmodel = client.Module(\"openai/whisper-large-v2\")\nwith client.UploadFile(Path(\"audio.wav\")) as remote_path:\n  response = model(path=remote_path)\n# {\"chunks\": ...}\n</code></pre> <pre><code>curl \\\n-X POST http://localhost:8000/v1/infer/file \\\n-H 'accept: application/json' \\\n-H 'Content-Type: multipart/form-data' \\\n-F 'model_id=openai/whisper-large-v2' \\\n-F 'file=@audio.wav'\n</code></pre>"},{"location":"docs/OVERVIEW.html#object-detection-yolox-as-a-service","title":"\ud83e\uddd0 Object Detection (YOLOX-as-a-Service)","text":"gRPC API \u26a1   REST API  <pre><code>from pathlib import Path\nfrom nos.client import Client\n\nclient = Client()\n\nmodel = client.Module(\"yolox/medium\")\nresponse = model(images=[Image.open(\"image.jpg\")])\n</code></pre> <pre><code>curl \\\n-X POST http://localhost:8000/v1/infer/file \\\n-H 'accept: application/json' \\\n-H 'Content-Type: multipart/form-data' \\\n-F 'model_id=yolox/medium' \\\n-F 'file=@image.jpg'\n</code></pre>"},{"location":"docs/OVERVIEW.html#chat-llm-agents-chatgpt-as-a-service","title":"\ud83d\udcac Chat / LLM Agents (ChatGPT-as-a-Service)","text":"<p>Coming soon! Stay tuned for updates on this by signing up on Autonomi AI or join the Discord for up-to-date announcements.</p>"},{"location":"docs/OVERVIEW.html#directory-structure","title":"\ud83d\uddc2\ufe0f Directory Structure","text":"<pre><code>\u251c\u2500\u2500 docker         # Dockerfile for CPU/GPU servers\n\u251c\u2500\u2500 docs           # mkdocs documentation\n\u251c\u2500\u2500 examples       # example guides, jupyter notebooks, demos\n\u251c\u2500\u2500 makefiles      # makefiles for building/testing\n\u251c\u2500\u2500 nos\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 cli        # CLI (hub, system)\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 client     # gRPC / REST client\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 common     # common utilities\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 executors  # runtime executor (i.e. Ray)\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hub        # hub utilies\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 managers   # model manager / multiplexer\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 models     # model zoo\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 proto      # protobuf defs for NOS gRPC service\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 server     # server backend (gRPC)\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 test       # pytest utilities\n\u251c\u2500\u2500 requirements   # requirement extras (server, docs, tests)\n\u251c\u2500\u2500 scripts        # basic scripts\n\u2514\u2500\u2500 tests          # pytests (client, server, benchmark)\n</code></pre>"},{"location":"docs/OVERVIEW.html#documentation","title":"\ud83d\udcda Documentation","text":"<ul> <li>Quickstart</li> <li>Models</li> <li>Concepts: Architecture Overview, ModelSpec, ModelManager, Runtime Environments</li> <li>Demos: Building a Discord Image Generation Bot, Video Search Demo</li> </ul>"},{"location":"docs/OVERVIEW.html#roadmap","title":"\ud83d\udee3 Roadmap","text":""},{"location":"docs/OVERVIEW.html#hw-cloud-support","title":"HW / Cloud Support","text":"<ul> <li> <p> Commodity GPUs</p> <ul> <li> NVIDIA GPUs (20XX, 30XX, 40XX)</li> <li> AMD GPUs (RX 7000)</li> </ul> </li> <li> <p> Cloud GPUs</p> <ul> <li> NVIDIA (H100, A100, A10G, A30G, T4, L4)</li> <li> AMD (MI200, MI250)</li> </ul> </li> <li> <p> Cloud Service Providers (via SkyPilot)</p> <ul> <li> AWS, GCP, Azure</li> <li> Opinionated Cloud: Lambda Labs, RunPod, etc</li> </ul> </li> <li> <p> Cloud ASICs</p> <ul> <li> AWS Inferentia (Inf1/Inf2)</li> <li> Google TPU</li> <li> Coming soon! (Habana Gaudi, Tenstorrent)</li> </ul> </li> </ul>"},{"location":"docs/OVERVIEW.html#license","title":"\ud83d\udcc4 License","text":"<p>This project is licensed under the Apache-2.0 License.</p>"},{"location":"docs/OVERVIEW.html#telemetry","title":"\ud83d\udce1 Telemetry","text":"<p>NOS collects anonymous usage data using Sentry. This is used to help us understand how the community is using NOS and to help us prioritize features. You can opt-out of telemetry by setting <code>NOS_TELEMETRY_ENABLED=0</code>.</p>"},{"location":"docs/OVERVIEW.html#contributing","title":"\ud83e\udd1d Contributing","text":"<p>We welcome contributions! Please see our contributing guide for more information.</p>"},{"location":"docs/OVERVIEW.html#quick-links","title":"\ud83d\udd17  Quick Links","text":"<ul> <li>\ud83d\udcac Send us an email at support@autonomi.ai or join our Discord for help.</li> <li>\ud83d\udce3 Follow us on Twitter, and LinkedIn to keep up-to-date on our products.</li> </ul>"},{"location":"docs/benchmarking.html","title":"\ud83d\udee0\ufe0f Benchmarking + Profiling","text":"<ol> <li> <p>Memory profiling with Memray</p> <p>Ray supports memory profiling through <code>memray</code>, which is included in the server requirements. Each model that is executed should produce its own <code>$MODEL_NAME_mem_profile.bin</code> file that can be downloaded from the Ray dashboard for offline visualization. After starting the GPU server and running a few inference requests, pull up the Ray dashboard at <code>0.0.0.0:8265</code>, then go to <code>logs</code> -&gt; <code>$NODE_NAME</code> -&gt; <code>$MODEL_NAME_mem_profile.bin</code> to download the memory profile.</p> <p>Then run <pre><code>memray flamegraph $MODEL_NAME_mem_profile.bin\n</code></pre></p> </li> </ol>"},{"location":"docs/models.html","title":"Models","text":""},{"location":"docs/models.html#text-and-image-embedding-with-openai-clip","title":"Text and Image embedding with OpenAI CLIP","text":"<pre><code>import nos\nfrom nos.client import Client, TaskType\nfrom PIL import Image\nimport requests\n\nnos.init(runtime=\"gpu\")\nclient = Client()\nclient.WaitForServer()\nclient.IsHealthy()\n\nurl = \"https://raw.githubusercontent.com/open-mmlab/mmdetection/main/demo/demo.jpg\"\nimg = Image.open(requests.get(url, stream=True).raw).resize((640, 480))\n\n# single image\npredictions = client.Run(\"openai/clip\", inputs={\"images\": [img]}, method=\"image_embedding\")\nprint(predictions[\"embedding\"].shape) # 1X512\n\n# batched N=3\npredictions = client.Run(\"openai/clip\", inputs={\"images\": [img, img, img]}, method=\"image_embedding\")\nprint(predictions[\"embedding\"].shape) # 3X512\n</code></pre> <p>Text embeddings follow a similar pattern:</p> <pre><code>text_string = \"the quick brown fox jumped over the lazy dog\"\npredictions = client.Run(\"openai/clip\", inputs={\"texts\": [text_string]}, method=\"text_embedding\")\npredictions[\"embedding\"].shape\n</code></pre>"},{"location":"docs/models.html#image-generation-with-stable-diffusion","title":"Image Generation with Stable Diffusion","text":"<ul> <li>Stability AI<ul> <li>Stable Diffusion 2</li> <li>Stable Diffusion 2.1</li> </ul> </li> <li>RunwayML<ul> <li>stable-diffusion-v1.4</li> <li>stable-diffusion-v1.5</li> </ul> </li> </ul> <p>Let's generate a few images with Stable Diffusion V2: <pre><code>import nos\nfrom nos.client import Client, TaskType\nfrom PIL import Image\nimport requests\n\nnos.init(runtime=\"gpu\")\nclient = Client()\nclient.WaitForServer()\nclient.IsHealthy()\n\nprompts = [\"fox jumped over the moon\", \"fox jumped over the sun\"]\npredictions = client.Run(\"stabilityai/stable-diffusion-2\",\n                         inputs={\"prompts\": prompts, \"width\": 512, \"height\": 512, \"num_images\": 1})\n\nfor prompt, image in zip(prompts, predictions[\"images\"]):\n    print(prompt, image.size)\n    display(image)\n</code></pre></p> <p></p> <p></p>"},{"location":"docs/models.html#audio-transcription-with-whisper","title":"Audio Transcription with Whisper","text":"<p>The current interface requires that a <code>.wav</code> be serialized into base64 and passed in as a string. The following snippet demonstrates a simple youtube-&gt;text transcription flow with <code>yt_dlp</code>.</p> <pre><code>from nos.client import Client\nfrom nos.common import TaskType\nfrom yt_dlp import YoutubeDL\n\nnos.init(runtime=\"gpu\")\nclient = Client()\nclient.WaitForServer()\nclient.IsHealthy()\n\n# Short util to extract .wav files from youtube urls\ndef download_youtube_url_and_transcribe(url):\n    ydl_opts = {\n        \"format\": \"bestaudio/best\",\n        \"postprocessors\": [\n            {\n                \"key\": \"FFmpegExtractAudio\",\n                \"preferredcodec\": \"wav\",\n                \"preferredquality\": \"192\",\n            }\n        ],\n    }\n\n    with YoutubeDL(ydl_opts) as ydl:\n        # set download location to current directory\n        info_dict = ydl.extract_info(url, download=False)\n        output_filename = ydl.prepare_filename(info_dict)\n        audio_filename = output_filename.replace(\".webm\", \".wav\")\n        error_code = ydl.download([url])\n        assert error_code == 0\n\n    with open(audio_filename, \"rb\") as f:\n        audio_data = f.read()\n\n    # serialize wav to base64\n    import base64\n    audio_data_base64 = base64.b64encode(audio_data).decode(\"utf-8\")\n\n    # run transcription\n    predictions = client.Run(\"openai/whisper-tiny.en\", inputs={\"audio\" : audio_data_base64})\n    print(predictions[\"text\"])\n\nyoutube_url = \"https://www.youtube.com/watch?v=EFfJEB1jkSo\"\ndownload_youtube_url_and_transcribe(youtube_url)\n</code></pre>"},{"location":"docs/models.html#object-detection-with-yolox","title":"Object Detection with YoloX","text":"<p>NOS Supports a variety of YoloX variants for object detection including:</p> <ul> <li>nano</li> <li>tiny</li> <li>small</li> <li>medium</li> <li>large</li> <li>xlarge</li> </ul> <pre><code>import nos\nfrom nos.client import Client, TaskType\nfrom PIL import Image\nimport requests\nimport cv2\nimport numpy as np\n\nnos.init(runtime=\"gpu\")\nclient = Client()\nclient.WaitForServer()\nclient.IsHealthy()\n\nurl = \"https://raw.githubusercontent.com/open-mmlab/mmdetection/main/demo/demo.jpg\"\nimg = Image.open(requests.get(url, stream=True).raw).resize((640, 480))\n\ndef visualize_det2d(img: np.ndarray, bboxes: np.ndarray, labels: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Visualize 2D detection results on an image.\"\"\"\n    vis = np.asarray(img).copy()\n    for bbox, label in zip(bboxes.astype(np.int32), labels):\n        cv2.rectangle(vis, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)\n    return vis\n\n# Run YOLOX prediction on the image and get the prediction results as a dictionary.\n# predictions = {\"bboxes\", \"scores\", \"labels\"}.\npredictions = client.Run(\"yolox/nano\", inputs={\"images\": [img]})\nfor idx, (img, bboxes, scores, labels) in enumerate(zip([img], predictions[\"bboxes\"], predictions[\"scores\"], predictions[\"scores\"])):\n    display(Image.fromarray(visualize_det2d(img, bboxes, labels)))\n</code></pre> <p></p>"},{"location":"docs/quickstart.html","title":"\ud83d\udd25 Quickstart","text":""},{"location":"docs/quickstart.html#install-dependencies","title":"\ud83d\udee0\ufe0f Install Dependencies","text":"<p>You will need to install Docker, Nvidia Docker and Docker Compose.</p> Linux (Debian/Ubuntu) <p>On Linux, you can install Docker and Docker Compose via the following commands: <pre><code>sudo apt-get update \\\n&amp;&amp; sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin \\\n&amp;&amp; sudo systemctl restart docker\n</code></pre></p> <p>Next, let's install Nvidia Docker. This will install the NVIDIA docker and container toolkit which is required to run GPU accelerated containers. This is only required if you plan to run the NOS server with GPU support. <pre><code>sudo apt-get install nvidia-docker2 nvidia-container-toolkit-base\n</code></pre></p> <p>Finally, you should be able to run the following command without any errors and the <code>nvidia-smi</code> output: <pre><code>docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi\n</code></pre></p> <p>If you run into issues, refer to the official Nvidia install guide or just ping us on #nos-support.</p>"},{"location":"docs/quickstart.html#install-nos","title":"\ud83d\udc69\u200d\ud83d\udcbb Install NOS","text":"<p>We highly recommend doing all of the following inside of a Conda or Virtualenv environment. You can install Conda on your machine following the official guide. Create a new env: <pre><code>conda create -n nos python=3.8\nconda activate nos\n</code></pre></p> Client-only InstallationServer Installation (GPU) <pre><code>pip install torch-nos\n</code></pre> <p>If you plan to run the NOS server locally (i.e. outside docker), you will also need to install the <code>server</code> extra dependencies: <pre><code>pip install 'torch-nos[server]'\n</code></pre></p> <p>Note</p> <p>We currently only support running the NOS server on Linux with GPUs. </p> <p>Note</p> <p>Python 3.8 is currently required to run the server on MacOS due to Ray requirements. If you don't plan to run the server locally then this requirement can be relaxed.</p>"},{"location":"docs/quickstart.html#start-the-nos-backend-server","title":"\u26a1\ufe0f Start the NOS backend server","text":"<p>You can start the nos server programmatically via either the CLI or SDK:</p> Via CLIVia SDK <p>You can start the nos server via the NOS <code>serve</code> CLI: <pre><code>nos serve up\n</code></pre></p> <p>Optionally, to use the REST API, you can start an HTTP gateway proxy alongside the gRPC server: <pre><code>nos serve up --http\n</code></pre></p> <p>Note</p> <p>You can look at the full list of <code>serve</code> CLI options here. </p> <p>You can start the nos server programmatically via the NOS SDK: <pre><code>import nos\n\nnos.init(runtime=\"auto\")\n</code></pre></p> <p>We're now ready to issue our first inference request with NOS!</p>"},{"location":"docs/quickstart.html#run-inference","title":"\ud83d\ude80 Run Inference","text":"<p>Try out an inference request via the Python SDK:</p> <pre><code>from nos.client import Client, TaskType\nclient = Client()\nclient.WaitForServer()\nclient.IsHealthy()\n\nsdv2 = client.Module(\"stabilityai/stable-diffusion-2-1\")\nsdv2(prompts=[\"fox jumped over the moon\"],\n     width=512, height=512, num_images=1)\n</code></pre>"},{"location":"docs/quickstart.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"docs/quickstart.html#resource-requirements","title":"Resource Requirements","text":"<p>Most Macbook laptops don't meet the resource requirements for running most NOS models. This will cause the server to fail initialization with error messages describing resource constraints.</p>"},{"location":"docs/quickstart.html#macos-dependencies","title":"MacOS dependencies","text":"<p>There is currently an issue causing a dependency import (<code>grpcio</code>) to fail on MacOS Darwin:  <code>symbol not found in flat namespace '_kCFStreamPropertySocketNativeHandle'</code></p> <p>We're working on resolving this. In the meantime please rebuild grpcio from source with an additional flag: <pre><code>pip uninstall grpcio\nexport GRPC_PYTHON_LDFLAGS=\" -framework CoreFoundation\"\npip install grpcio --no-binary :all:\n</code></pre></p> <p>If you run into other issues after following this guide, feel free to ping us on #nos-support.</p>"},{"location":"docs/roadmap.html","title":"Roadmap","text":""},{"location":"docs/roadmap.html#hardware-support","title":"Hardware Support","text":"<p>We currently plan to support the following hardware:</p> <ul> <li>Platforms<ul> <li> Linux</li> <li> MacOS (coming soon!)</li> </ul> </li> <li> GPUs<ul> <li> NVIDIA (A100, A10G, T4, 4090, 2080)</li> <li> AMD GPUs (MI250)</li> </ul> </li> <li> Cloud Providers<ul> <li> AWS (g4/g5dn/p3/p4dn)<ul> <li> AWS Inferentia inf1/inf2</li> <li> Intel Habana Gaudi</li> </ul> </li> <li> GCP (g2/a1/n1)<ul> <li> Google TPUv3</li> </ul> </li> </ul> </li> </ul>"},{"location":"docs/roadmap.html#model-hub","title":"Model Hub","text":"<ul> <li>Text-to-Image<ul> <li> Stable Diffusion v1.5</li> <li> Stable Diffusion v2.0</li> </ul> </li> <li>Image/Text-to-Vec<ul> <li> OpenAI CLIP ViT</li> <li> Laion CLIP ViT</li> </ul> </li> <li>Object Detection<ul> <li> FasterRCNN</li> <li> EfficientDet (OpenMMLab)</li> <li> FasterRCNN (OpenMMLab)</li> </ul> </li> </ul>"},{"location":"docs/roadmap.html#model-hub-coming-soon","title":"Model Hub: Coming Soon","text":"<ul> <li>Audio-to-Text<ul> <li> OpenAI Whisper</li> <li> WhisperX</li> </ul> </li> <li>Object Tracking<ul> <li> MMtracking (OpenMMLab)</li> </ul> </li> </ul>"},{"location":"docs/roadmap.html#model-optimizations","title":"Model Optimizations","text":"<ul> <li>NVIDIA TensorRT</li> <li>Facebook AITemplate</li> </ul>"},{"location":"docs/support.html","title":"\u2753 FAQ / Support","text":"<p>\ud83e\uddea NOS is currently in beta. This means that the API is subject to change and may not be available at all times. We will do our best to keep this documentation up to date, but if you notice any errors or inconsistencies, please let us know.</p> <p>All of the support channels are listed in the table below. If you have any questions, please feel free to ask them in the appropriate channel.</p> Channel Description GitHub Issues For client-side bug reports and feature requests. Discord For general questions and discussions. Email For all support related inquiries."},{"location":"docs/api/client.html","title":"nos.client","text":""},{"location":"docs/api/client.html#nos-inference-service","title":"NOS Inference Service","text":"<p>In this section, we expect that you have already installed NOS and have already started the server.</p>"},{"location":"docs/api/client.html#nos.client.grpc.Client","title":"nos.client.grpc.Client","text":"<p>Main gRPC client for NOS inference service.</p> <p>Parameters:</p> <ul> <li> <code>address</code>               (<code>str</code>, default:                   <code>DEFAULT_GRPC_ADDRESS</code> )           \u2013            <p>Address for the gRPC server.</p> </li> </ul> Usage <pre><code>&gt;&gt;&gt; client = Client(address=\"localhost:50051\")  # create client\n&gt;&gt;&gt; client.WaitForServer()  # wait for server to start\n&gt;&gt;&gt; client.CheckCompatibility()  # check compatibility with server\n\n&gt;&gt;&gt; client.ListModels()  # list all models registered\n\n&gt;&gt;&gt; img = Image.open(\"test.jpg\")\n&gt;&gt;&gt; visual_model = client.Module(\"openai/clip-vit-base-patch32\")  # instantiate CLIP module\n&gt;&gt;&gt; visual_model(images=img)  # predict with CLIP\n\n&gt;&gt;&gt; fastrcnn_model = client.Module(\"torchvision/fasterrcnn-mobilenet-v3-large-320-fpn\")  # instantiate FasterRCNN module\n&gt;&gt;&gt; fastrcnn_model(images=img)\n</code></pre> Source code in <code>nos/client/grpc.py</code> <pre><code>class Client:\n    \"\"\"Main gRPC client for NOS inference service.\n\n    Parameters:\n        address (str): Address for the gRPC server.\n\n    Usage:\n        ```py\n\n        &gt;&gt;&gt; client = Client(address=\"localhost:50051\")  # create client\n        &gt;&gt;&gt; client.WaitForServer()  # wait for server to start\n        &gt;&gt;&gt; client.CheckCompatibility()  # check compatibility with server\n\n        &gt;&gt;&gt; client.ListModels()  # list all models registered\n\n        &gt;&gt;&gt; img = Image.open(\"test.jpg\")\n        &gt;&gt;&gt; visual_model = client.Module(\"openai/clip-vit-base-patch32\")  # instantiate CLIP module\n        &gt;&gt;&gt; visual_model(images=img)  # predict with CLIP\n\n        &gt;&gt;&gt; fastrcnn_model = client.Module(\"torchvision/fasterrcnn-mobilenet-v3-large-320-fpn\")  # instantiate FasterRCNN module\n        &gt;&gt;&gt; fastrcnn_model(images=img)\n        ```\n    \"\"\"\n\n    def __init__(self, address: str = DEFAULT_GRPC_ADDRESS):\n        \"\"\"Initializes the gRPC client.\n\n        Args:\n            address (str): Address for the gRPC server. Defaults to DEFAULT_GRPC_ADDRESS.\n        \"\"\"\n        self.address: str = address\n        self._channel: grpc.Channel = None\n        self._stub: nos_service_pb2_grpc.InferenceServiceStub = None\n        self._uuid: str = secrets.token_hex(4)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Returns the string representation of the client.\n\n        Returns:\n            str: String representation of the client.\n        \"\"\"\n        return f\"Client(address={self.address})\"\n\n    def __getstate__(self) -&gt; ClientState:\n        \"\"\"Returns the state of the client for serialization purposes.\n\n        Returns:\n            ClientState: State of the client.\n        \"\"\"\n        return ClientState(address=self.address)\n\n    def __setstate__(self, state: ClientState) -&gt; None:\n        \"\"\"Sets the state of the client for de-serialization purposes.\n\n        Args:\n            state (ClientState): State of the client.\n        Returns:\n            None (NoneType): Nothing.\n        \"\"\"\n        self.address = state.address\n        self._channel = None\n        self._stub = None\n\n    @property\n    def stub(self) -&gt; nos_service_pb2_grpc.InferenceServiceStub:\n        \"\"\"Returns the gRPC stub.\n\n        Note: The stub is created on-demand for serialization purposes,\n        as we don't want to create a channel until we actually need it.\n        This is especially useful for pickling/un-pickling the client.\n\n        Returns:\n            nos_service_pb2_grpc.InferenceServiceStub: gRPC stub.\n        Raises:\n            NosClientException: If the server fails to respond to the connection request.\n        \"\"\"\n        if not self._stub:\n            options = [\n                (\"grpc.max_message_length\", GRPC_MAX_MESSAGE_LENGTH),\n                (\"grpc.max_send_message_length\", GRPC_MAX_MESSAGE_LENGTH),\n                (\"grpc.max_receive_message_length\", GRPC_MAX_MESSAGE_LENGTH),\n            ]\n            self._channel = grpc.insecure_channel(self.address, options)\n            try:\n                self._stub = nos_service_pb2_grpc.InferenceServiceStub(self._channel)\n            except Exception as e:\n                raise ServerReadyException(f\"Failed to connect to server ({e})\", e)\n        assert self._channel\n        assert self._stub\n        return self._stub\n\n    def IsHealthy(self) -&gt; bool:\n        \"\"\"Check if the gRPC server is healthy.\n\n        Returns:\n            bool: True if the server is running, False otherwise.\n        Raises:\n            NosClientException: If the server fails to respond to the ping.\n        \"\"\"\n        try:\n            response: nos_service_pb2.PingResponse = self.stub.Ping(empty_pb2.Empty())\n            return response.status == \"ok\"\n        except grpc.RpcError as e:\n            raise ServerReadyException(f\"Failed to ping server (details={e.details()})\", e)\n\n    def WaitForServer(self, timeout: int = 60, retry_interval: int = 5) -&gt; None:\n        \"\"\"Ping the gRPC server for health.\n\n        Args:\n            timeout (int, optional): Timeout in seconds. Defaults to 60.\n            retry_interval (int, optional): Retry interval in seconds. Defaults to 5.\n        Returns:\n            bool: True if the server is running, False otherwise.\n        Raises:\n            NosClientException: If the server fails to respond to the ping or times out.\n        \"\"\"\n        st = time.time()\n        while time.time() - st &lt;= timeout:\n            try:\n                return self.IsHealthy()\n            except Exception:\n                elapsed = time.time() - st\n                if int(elapsed) &gt; 10:\n                    logger.warning(\"Waiting for server to start... (elapsed={:.0f}s)\".format(time.time() - st))\n                time.sleep(retry_interval)\n        default_msg = \"\"\"\\n If you are running the server in docker, make sure the server sets `NOS_GRPC_HOST=[::]` and the client sets `NOS_GRPC_HOST=&lt;server-container-name&gt;` in their environment variables.\"\"\"\n        raise ServerReadyException(f\"Failed to ping server. {default_msg}\")\n\n    def GetServiceVersion(self) -&gt; str:\n        \"\"\"Get service version.\n\n        Returns:\n            str: Service version (e.g. 0.0.4).\n        Raises:\n            NosClientException: If the server fails to respond to the request.\n        \"\"\"\n        try:\n            response: nos_service_pb2.ServiceInfoResponse = self.stub.GetServiceInfo(empty_pb2.Empty())\n            return response.version\n        except grpc.RpcError as e:\n            raise ServerReadyException(f\"Failed to get service info (details={e.details()})\", e)\n\n    def GetServiceRuntime(self) -&gt; str:\n        \"\"\"Get service runtime.\n\n        Returns:\n            str: Service runtime (e.g. cpu, gpu, local).\n        Raises:\n            NosClientException: If the server fails to respond to the request.\n        \"\"\"\n        try:\n            response: nos_service_pb2.ServiceInfoResponse = self.stub.GetServiceInfo(empty_pb2.Empty())\n            return response.runtime\n        except grpc.RpcError as e:\n            raise ServerReadyException(f\"Failed to get service info (details={e.details()})\", e)\n\n    def CheckCompatibility(self) -&gt; bool:\n        \"\"\"Check if the service version is compatible with the client.\n\n        Returns:\n            bool: True if the service version is compatible, False otherwise.\n        Raises:\n            NosClientException: If the server fails to respond to the request.\n        \"\"\"\n        # TODO (spillai): For now, we enforce strict version matching\n        # until we have tests for client-server compatibility.\n        is_compatible = self.GetServiceVersion() == __version__\n        if not is_compatible:\n            raise ClientException(\n                f\"Client-Server version mismatch (client={__version__}, server={self.GetServiceVersion()})\"\n            )\n        return is_compatible\n\n    def ListModels(self) -&gt; List[ModelSpec]:\n        \"\"\"List all models.\n\n        Returns:\n            List[ModelSpec]: List of ModelInfo (name, task).\n        Raises:\n            NosClientException: If the server fails to respond to the request.\n        \"\"\"\n        try:\n            response: nos_service_pb2.GenericResponse = self.stub.ListModels(empty_pb2.Empty())\n            models: List[str] = loads(response.response_bytes)\n            return list(models)\n        except grpc.RpcError as e:\n            raise ClientException(f\"Failed to list models (details={e.details()})\", e)\n\n    def LoadModel(self, model_id: str, num_replicas: int = 1) -&gt; None:\n        \"\"\"Load a model.\n\n        Args:\n            model_id (str): Name of the model to load.\n            num_replicas (int, optional): Number of replicas to load. Defaults to 1.\n        Raises:\n            NosClientException: If the server fails to respond to the request.\n        \"\"\"\n        try:\n            self.stub.LoadModel(\n                nos_service_pb2.GenericRequest(request_bytes=dumps({\"id\": model_id, \"num_replicas\": num_replicas}))\n            )\n        except grpc.RpcError as e:\n            raise ClientException(f\"Failed to load model (details={e.details()})\", e)\n\n    @lru_cache()  # noqa: B019\n    def _get_model_catalog(self) -&gt; ModelSpecMetadataCatalog:\n        \"\"\"Get the model catalog and cache.\n\n        Returns:\n            Dict[str, ModelSpec]: Model catalog (name, task).\n        Raises:\n            NosClientException: If the server fails to respond to the request.\n        \"\"\"\n        try:\n            response: nos_service_pb2.GenericResponse = self.stub.GetModelCatalog(empty_pb2.Empty())\n            ModelSpecMetadataCatalog._instance = loads(response.response_bytes)\n            return ModelSpecMetadataCatalog.get()\n        except grpc.RpcError as e:\n            raise ClientException(f\"Failed to get model catalog (details={e.details()})\", e)\n\n    def GetModelInfo(self, model_id: str) -&gt; ModelSpec:\n        \"\"\"Get the relevant model information from the model name.\n\n        Note: This may be possible only after initialization, as we need to inspect the\n        HW to understand the configurable image resolutions, batch sizes etc.\n\n        Args:\n            spec (ModelSpec): Model information.\n        \"\"\"\n        try:\n            # Update the model catalog so that the metadata is cached on the client-side\n            _ = self._get_model_catalog()\n            # Get the model spec separately\n            response: nos_service_pb2.GenericResponse = self.stub.GetModelInfo(\n                wrappers_pb2.StringValue(value=model_id)\n            )\n            model_spec: ModelSpec = loads(response.response_bytes)\n            return model_spec\n        except grpc.RpcError as e:\n            raise ClientException(f\"Failed to get model info (details={(e.details())})\", e)\n\n    @lru_cache(maxsize=8)  # noqa: B019\n    def Module(self, model_id: str, shm: bool = False) -&gt; \"Module\":\n        \"\"\"Instantiate a model module.\n\n        Args:\n            model_id (str): Name of the model to init.\n            shm (bool, optional): Enable shared memory transport. Defaults to False.\n        Returns:\n            Module: Inference module.\n        \"\"\"\n        return Module(model_id, self, shm=shm)\n\n    @lru_cache(maxsize=8)  # noqa: B019\n    def ModuleFromSpec(self, spec: ModelSpec, shm: bool = False) -&gt; \"Module\":\n        \"\"\"Instantiate a model module from a model spec.\n\n        Args:\n            spec (ModelSpec): Model specification.\n            shm (bool, optional): Enable shared memory transport. Defaults to False.\n        Returns:\n            Module: Inference module.\n        \"\"\"\n        return Module(spec.task, spec.name, self, shm=shm)\n\n    def ModuleFromCls(self, cls: Callable, shm: bool = False) -&gt; \"Module\":\n        raise NotImplementedError(\"ModuleFromCls not implemented yet.\")\n\n    def _upload_file(self, path: Path, chunk_size: int = 4 * MB_BYTES) -&gt; Path:\n        \"\"\"Upload a file to the server.\n\n        Args:\n            path (Path): Path to the file to be uploaded.\n        Returns:\n            path: Temporary remote path of the uploaded file.\n        Raises:\n            NosClientException: If the server fails to respond to the request.\n        \"\"\"\n        try:\n            response = None\n            with path.open(\"rb\") as f:\n                for cidx, chunk in enumerate(iter(lambda: f.read(chunk_size), b\"\")):\n                    response: nos_service_pb2.GenericResponse = self.stub.UploadFile(\n                        iter(\n                            [\n                                nos_service_pb2.GenericRequest(\n                                    request_bytes=dumps(\n                                        {\"chunk_bytes\": chunk, \"chunk_index\": cidx, \"filename\": str(path)}\n                                    )\n                                )\n                            ]\n                        )\n                    )\n            return Path(loads(response.response_bytes)[\"filename\"])\n        except grpc.RpcError as e:\n            raise ClientException(f\"Failed to upload file (details={e.details()})\", e)\n\n    def _delete_file(self, path: Path) -&gt; None:\n        \"\"\"Delete a file from the server.\n\n        Args:\n            path (Path): Path to the file to be deleted.\n        Raises:\n            NosClientException: If the server fails to respond to the request.\n        \"\"\"\n        try:\n            self.stub.DeleteFile(nos_service_pb2.GenericRequest(request_bytes=dumps({\"filename\": str(path)})))\n        except grpc.RpcError as e:\n            raise ClientException(f\"Failed to delete file (details={e.details()})\", e)\n\n    @contextlib.contextmanager\n    def UploadFile(self, path: Path, chunk_size: int = 4 * MB_BYTES) -&gt; Path:\n        \"\"\"Upload a file to the server, and delete it after use.\"\"\"\n        path = path.absolute()\n        if not path.exists():\n            raise FileNotFoundError(f\"File not found [path={path}]\")\n        try:\n            logger.debug(f\"Uploading file [path={path}]\")\n            remote_path: Path = self._upload_file(path, chunk_size=chunk_size)\n            logger.debug(f\"Uploaded file [path={path}, remote_path={remote_path}]\")\n            yield remote_path\n            logger.debug(f\"Deleting file [path={path}, remote_path={remote_path}]\")\n        except Exception as e:\n            logger.error(f\"Failed to upload file [path={path}, e={e}]\")\n        finally:\n            logger.debug(f\"Deleting file [path={path}, remote_path={remote_path}]\")\n            try:\n                self._delete_file(path)\n            except Exception as e:\n                logger.error(f\"Failed to delete file [path={path}, remote_path={remote_path}, e={e}]\")\n            logger.debug(f\"Deleted file [path={path}, remote_path={remote_path}]\")\n\n    def Run(\n        self,\n        model_id: str,\n        inputs: Dict[str, Any],\n        method: str = None,\n        shm: bool = False,\n    ) -&gt; nos_service_pb2.GenericResponse:\n        \"\"\"Run module.\n\n        Args:\n            model_id (str):\n                Model identifier (e.g. openai/clip-vit-base-patch32).\n            inputs (Dict[str, Any]): Inputs to the model (\"images\", \"texts\", \"prompts\" etc) as\n                defined in the ModelSpec.signature.\n            method (str, optional): Method to call on the model. Defaults to None.\n            stream (bool, optional): Stream the response. Defaults to False.\n            shm (bool, optional): Enable shared memory transport. Defaults to False.\n        Returns:\n            nos_service_pb2.GenericResponse: Inference response.\n        Raises:\n            NosClientException: If the server fails to respond to the request.\n        \"\"\"\n        module: Module = self.Module(model_id, shm=shm)\n        return module(**inputs, _method=method)\n\n    def Stream(\n        self,\n        model_id: str,\n        inputs: Dict[str, Any],\n        method: str = None,\n        shm: bool = False,\n    ) -&gt; Iterable[nos_service_pb2.GenericResponse]:\n        \"\"\"Run module in streaming mode.\"\"\"\n        assert shm is False, \"Shared memory transport is not supported for streaming yet.\"\n        module: Module = self.Module(model_id, shm=shm)\n        return module(**inputs, _method=method, _stream=True)\n</code></pre>"},{"location":"docs/api/client.html#nos.client.grpc.Client.__init__","title":"__init__","text":"<pre><code>__init__(address: str = DEFAULT_GRPC_ADDRESS)\n</code></pre> <p>Initializes the gRPC client.</p> <p>Parameters:</p> <ul> <li> <code>address</code>               (<code>str</code>, default:                   <code>DEFAULT_GRPC_ADDRESS</code> )           \u2013            <p>Address for the gRPC server. Defaults to DEFAULT_GRPC_ADDRESS.</p> </li> </ul> Source code in <code>nos/client/grpc.py</code> <pre><code>def __init__(self, address: str = DEFAULT_GRPC_ADDRESS):\n    \"\"\"Initializes the gRPC client.\n\n    Args:\n        address (str): Address for the gRPC server. Defaults to DEFAULT_GRPC_ADDRESS.\n    \"\"\"\n    self.address: str = address\n    self._channel: grpc.Channel = None\n    self._stub: nos_service_pb2_grpc.InferenceServiceStub = None\n    self._uuid: str = secrets.token_hex(4)\n</code></pre>"},{"location":"docs/api/client.html#nos.client.grpc.Client.IsHealthy","title":"IsHealthy","text":"<pre><code>IsHealthy() -&gt; bool\n</code></pre> <p>Check if the gRPC server is healthy.</p> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the server is running, False otherwise.</p> </li> </ul> <p>Raises:     NosClientException: If the server fails to respond to the ping.</p> Source code in <code>nos/client/grpc.py</code> <pre><code>def IsHealthy(self) -&gt; bool:\n    \"\"\"Check if the gRPC server is healthy.\n\n    Returns:\n        bool: True if the server is running, False otherwise.\n    Raises:\n        NosClientException: If the server fails to respond to the ping.\n    \"\"\"\n    try:\n        response: nos_service_pb2.PingResponse = self.stub.Ping(empty_pb2.Empty())\n        return response.status == \"ok\"\n    except grpc.RpcError as e:\n        raise ServerReadyException(f\"Failed to ping server (details={e.details()})\", e)\n</code></pre>"},{"location":"docs/api/client.html#nos.client.grpc.Client.WaitForServer","title":"WaitForServer","text":"<pre><code>WaitForServer(timeout: int = 60, retry_interval: int = 5) -&gt; None\n</code></pre> <p>Ping the gRPC server for health.</p> <p>Parameters:</p> <ul> <li> <code>timeout</code>               (<code>int</code>, default:                   <code>60</code> )           \u2013            <p>Timeout in seconds. Defaults to 60.</p> </li> <li> <code>retry_interval</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Retry interval in seconds. Defaults to 5.</p> </li> </ul> <p>Returns:     bool: True if the server is running, False otherwise. Raises:     NosClientException: If the server fails to respond to the ping or times out.</p> Source code in <code>nos/client/grpc.py</code> <pre><code>def WaitForServer(self, timeout: int = 60, retry_interval: int = 5) -&gt; None:\n    \"\"\"Ping the gRPC server for health.\n\n    Args:\n        timeout (int, optional): Timeout in seconds. Defaults to 60.\n        retry_interval (int, optional): Retry interval in seconds. Defaults to 5.\n    Returns:\n        bool: True if the server is running, False otherwise.\n    Raises:\n        NosClientException: If the server fails to respond to the ping or times out.\n    \"\"\"\n    st = time.time()\n    while time.time() - st &lt;= timeout:\n        try:\n            return self.IsHealthy()\n        except Exception:\n            elapsed = time.time() - st\n            if int(elapsed) &gt; 10:\n                logger.warning(\"Waiting for server to start... (elapsed={:.0f}s)\".format(time.time() - st))\n            time.sleep(retry_interval)\n    default_msg = \"\"\"\\n If you are running the server in docker, make sure the server sets `NOS_GRPC_HOST=[::]` and the client sets `NOS_GRPC_HOST=&lt;server-container-name&gt;` in their environment variables.\"\"\"\n    raise ServerReadyException(f\"Failed to ping server. {default_msg}\")\n</code></pre>"},{"location":"docs/api/client.html#nos.client.grpc.Client.GetServiceVersion","title":"GetServiceVersion","text":"<pre><code>GetServiceVersion() -&gt; str\n</code></pre> <p>Get service version.</p> <p>Returns:</p> <ul> <li> <code>str</code> (              <code>str</code> )          \u2013            <p>Service version (e.g. 0.0.4).</p> </li> </ul> <p>Raises:     NosClientException: If the server fails to respond to the request.</p> Source code in <code>nos/client/grpc.py</code> <pre><code>def GetServiceVersion(self) -&gt; str:\n    \"\"\"Get service version.\n\n    Returns:\n        str: Service version (e.g. 0.0.4).\n    Raises:\n        NosClientException: If the server fails to respond to the request.\n    \"\"\"\n    try:\n        response: nos_service_pb2.ServiceInfoResponse = self.stub.GetServiceInfo(empty_pb2.Empty())\n        return response.version\n    except grpc.RpcError as e:\n        raise ServerReadyException(f\"Failed to get service info (details={e.details()})\", e)\n</code></pre>"},{"location":"docs/api/client.html#nos.client.grpc.Client.CheckCompatibility","title":"CheckCompatibility","text":"<pre><code>CheckCompatibility() -&gt; bool\n</code></pre> <p>Check if the service version is compatible with the client.</p> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if the service version is compatible, False otherwise.</p> </li> </ul> <p>Raises:     NosClientException: If the server fails to respond to the request.</p> Source code in <code>nos/client/grpc.py</code> <pre><code>def CheckCompatibility(self) -&gt; bool:\n    \"\"\"Check if the service version is compatible with the client.\n\n    Returns:\n        bool: True if the service version is compatible, False otherwise.\n    Raises:\n        NosClientException: If the server fails to respond to the request.\n    \"\"\"\n    # TODO (spillai): For now, we enforce strict version matching\n    # until we have tests for client-server compatibility.\n    is_compatible = self.GetServiceVersion() == __version__\n    if not is_compatible:\n        raise ClientException(\n            f\"Client-Server version mismatch (client={__version__}, server={self.GetServiceVersion()})\"\n        )\n    return is_compatible\n</code></pre>"},{"location":"docs/api/client.html#nos.client.grpc.Client.ListModels","title":"ListModels","text":"<pre><code>ListModels() -&gt; List[ModelSpec]\n</code></pre> <p>List all models.</p> <p>Returns:</p> <ul> <li> <code>List[ModelSpec]</code>           \u2013            <p>List[ModelSpec]: List of ModelInfo (name, task).</p> </li> </ul> <p>Raises:     NosClientException: If the server fails to respond to the request.</p> Source code in <code>nos/client/grpc.py</code> <pre><code>def ListModels(self) -&gt; List[ModelSpec]:\n    \"\"\"List all models.\n\n    Returns:\n        List[ModelSpec]: List of ModelInfo (name, task).\n    Raises:\n        NosClientException: If the server fails to respond to the request.\n    \"\"\"\n    try:\n        response: nos_service_pb2.GenericResponse = self.stub.ListModels(empty_pb2.Empty())\n        models: List[str] = loads(response.response_bytes)\n        return list(models)\n    except grpc.RpcError as e:\n        raise ClientException(f\"Failed to list models (details={e.details()})\", e)\n</code></pre>"},{"location":"docs/api/client.html#nos.client.grpc.Client.GetModelInfo","title":"GetModelInfo","text":"<pre><code>GetModelInfo(model_id: str) -&gt; ModelSpec\n</code></pre> <p>Get the relevant model information from the model name.</p> <p>Note: This may be possible only after initialization, as we need to inspect the HW to understand the configurable image resolutions, batch sizes etc.</p> <p>Parameters:</p> <ul> <li> <code>spec</code>               (<code>ModelSpec</code>)           \u2013            <p>Model information.</p> </li> </ul> Source code in <code>nos/client/grpc.py</code> <pre><code>def GetModelInfo(self, model_id: str) -&gt; ModelSpec:\n    \"\"\"Get the relevant model information from the model name.\n\n    Note: This may be possible only after initialization, as we need to inspect the\n    HW to understand the configurable image resolutions, batch sizes etc.\n\n    Args:\n        spec (ModelSpec): Model information.\n    \"\"\"\n    try:\n        # Update the model catalog so that the metadata is cached on the client-side\n        _ = self._get_model_catalog()\n        # Get the model spec separately\n        response: nos_service_pb2.GenericResponse = self.stub.GetModelInfo(\n            wrappers_pb2.StringValue(value=model_id)\n        )\n        model_spec: ModelSpec = loads(response.response_bytes)\n        return model_spec\n    except grpc.RpcError as e:\n        raise ClientException(f\"Failed to get model info (details={(e.details())})\", e)\n</code></pre>"},{"location":"docs/api/client.html#nos.client.grpc.Client.Module","title":"Module  <code>cached</code>","text":"<pre><code>Module(model_id: str, shm: bool = False) -&gt; Module\n</code></pre> <p>Instantiate a model module.</p> <p>Parameters:</p> <ul> <li> <code>model_id</code>               (<code>str</code>)           \u2013            <p>Name of the model to init.</p> </li> <li> <code>shm</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Enable shared memory transport. Defaults to False.</p> </li> </ul> <p>Returns:     Module: Inference module.</p> Source code in <code>nos/client/grpc.py</code> <pre><code>@lru_cache(maxsize=8)  # noqa: B019\ndef Module(self, model_id: str, shm: bool = False) -&gt; \"Module\":\n    \"\"\"Instantiate a model module.\n\n    Args:\n        model_id (str): Name of the model to init.\n        shm (bool, optional): Enable shared memory transport. Defaults to False.\n    Returns:\n        Module: Inference module.\n    \"\"\"\n    return Module(model_id, self, shm=shm)\n</code></pre>"},{"location":"docs/api/client.html#nos.client.grpc.Client.ModuleFromSpec","title":"ModuleFromSpec  <code>cached</code>","text":"<pre><code>ModuleFromSpec(spec: ModelSpec, shm: bool = False) -&gt; Module\n</code></pre> <p>Instantiate a model module from a model spec.</p> <p>Parameters:</p> <ul> <li> <code>spec</code>               (<code>ModelSpec</code>)           \u2013            <p>Model specification.</p> </li> <li> <code>shm</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Enable shared memory transport. Defaults to False.</p> </li> </ul> <p>Returns:     Module: Inference module.</p> Source code in <code>nos/client/grpc.py</code> <pre><code>@lru_cache(maxsize=8)  # noqa: B019\ndef ModuleFromSpec(self, spec: ModelSpec, shm: bool = False) -&gt; \"Module\":\n    \"\"\"Instantiate a model module from a model spec.\n\n    Args:\n        spec (ModelSpec): Model specification.\n        shm (bool, optional): Enable shared memory transport. Defaults to False.\n    Returns:\n        Module: Inference module.\n    \"\"\"\n    return Module(spec.task, spec.name, self, shm=shm)\n</code></pre>"},{"location":"docs/api/client.html#nos.client.grpc.Client.Run","title":"Run","text":"<pre><code>Run(model_id: str, inputs: Dict[str, Any], method: str = None, shm: bool = False) -&gt; GenericResponse\n</code></pre> <p>Run module.</p> <p>Parameters:</p> <ul> <li> <code>model_id</code>               (<code>str</code>)           \u2013            <p>Model identifier (e.g. openai/clip-vit-base-patch32).</p> </li> <li> <code>inputs</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>Inputs to the model (\"images\", \"texts\", \"prompts\" etc) as defined in the ModelSpec.signature.</p> </li> <li> <code>method</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Method to call on the model. Defaults to None.</p> </li> <li> <code>stream</code>               (<code>bool</code>)           \u2013            <p>Stream the response. Defaults to False.</p> </li> <li> <code>shm</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Enable shared memory transport. Defaults to False.</p> </li> </ul> <p>Returns:     nos_service_pb2.GenericResponse: Inference response. Raises:     NosClientException: If the server fails to respond to the request.</p> Source code in <code>nos/client/grpc.py</code> <pre><code>def Run(\n    self,\n    model_id: str,\n    inputs: Dict[str, Any],\n    method: str = None,\n    shm: bool = False,\n) -&gt; nos_service_pb2.GenericResponse:\n    \"\"\"Run module.\n\n    Args:\n        model_id (str):\n            Model identifier (e.g. openai/clip-vit-base-patch32).\n        inputs (Dict[str, Any]): Inputs to the model (\"images\", \"texts\", \"prompts\" etc) as\n            defined in the ModelSpec.signature.\n        method (str, optional): Method to call on the model. Defaults to None.\n        stream (bool, optional): Stream the response. Defaults to False.\n        shm (bool, optional): Enable shared memory transport. Defaults to False.\n    Returns:\n        nos_service_pb2.GenericResponse: Inference response.\n    Raises:\n        NosClientException: If the server fails to respond to the request.\n    \"\"\"\n    module: Module = self.Module(model_id, shm=shm)\n    return module(**inputs, _method=method)\n</code></pre>"},{"location":"docs/api/client.html#nos.client.grpc.Module","title":"nos.client.grpc.Module  <code>dataclass</code>","text":"<p>Inference module for remote model execution.</p> Usage <pre><code># Create client\n&gt;&gt;&gt; client = Client()\n# Instantiate new task module with specific model name\n&gt;&gt;&gt; model = client.Module(\"openai/clip-vit-base-patch32\")\n# Predict with model using `__call__`\n&gt;&gt;&gt; predictions = model({\"images\": img})\n</code></pre> Source code in <code>nos/client/grpc.py</code> <pre><code>@dataclass\nclass Module:\n    \"\"\"Inference module for remote model execution.\n\n    Usage:\n        ```python\n        # Create client\n        &gt;&gt;&gt; client = Client()\n        # Instantiate new task module with specific model name\n        &gt;&gt;&gt; model = client.Module(\"openai/clip-vit-base-patch32\")\n        # Predict with model using `__call__`\n        &gt;&gt;&gt; predictions = model({\"images\": img})\n        ```\n    \"\"\"\n\n    id: str\n    \"\"\"Model identifier (e.g. openai/clip-vit-base-patch32).\"\"\"\n    _client: Client\n    \"\"\"gRPC client.\"\"\"\n    shm: bool = False\n    \"\"\"Enable shared memory transport.\"\"\"\n    _spec: ModelSpec = field(init=False)\n    \"\"\"Model specification for this module.\"\"\"\n    _shm_objects: Dict[str, Any] = field(init=False, default_factory=dict)\n    \"\"\"Shared memory data.\"\"\"\n\n    def __post_init__(self):\n        \"\"\"Initialize the spec.\"\"\"\n        self._spec = self._client.GetModelInfo(self.id)\n        assert self._spec.id == self.id\n        if not NOS_SHM_ENABLED or not self.shm:\n            # Note (spillai): Shared memory caveats.\n            # - only supported for numpy arrays\n            # - registered once per module\n            # - can not handle shm objects while calling multiple methods cleanly\n            #   (i.e. expects the same method to be called for a module)\n            self._shm_objects = None  # disables shm, and avoids registering/unregistering\n\n        # Patch the module with methods from model spec signature\n        for method in self._spec.signature.keys():\n            if hasattr(self, method):\n                # If the method to patch is __call__ just log a debug message and skip,\n                # otherwise log a warning so that the user is warned that the method is skipped.\n                log = logger.debug if method == \"__call__\" else logger.warning\n                log(f\"Module ({self.id}) already has method ({method}), skipping ...\")\n                continue\n\n            assert self._spec.signature[method].method == method\n            # Patch the module with the partial method only if the default method is\n            # not the same as the method being patched i.e., there's no need to pass\n            # `method`` to the partial method since it's already the default method.\n            if self._spec.default_method != method:\n                method_partial = partial(self.__call__, _method=method)\n            else:\n                method_partial = self.__call__\n            setattr(self, method, method_partial)\n            logger.debug(f\"Module ({self.id}) patched [method={method}].\")\n        logger.debug(f\"Module ({self.id}) initialized [spec={self._spec}, shm={self.shm}].\")\n\n    @property\n    def stub(self) -&gt; nos_service_pb2_grpc.InferenceServiceStub:\n        return self._client.stub\n\n    @property\n    def client_id(self) -&gt; str:\n        \"\"\"Correlation ID for this module.\"\"\"\n        return self._client._uuid\n\n    @cached_property\n    def object_id(self) -&gt; str:\n        \"\"\"Unique object ID for this module.\"\"\"\n        return f\"{self._spec.id}_{secrets.token_hex(4)}\"\n\n    @cached_property\n    def namespace(self) -&gt; str:\n        \"\"\"Unique namespace for this module.\"\"\"\n        return f\"{self.client_id}/{self.object_id}\"\n\n    def _encode(self, inputs: Dict[str, Any], method: str = None) -&gt; Dict[str, Any]:\n        \"\"\"Encode the inputs dictionary for transmission.\n        TODO (spillai)\n            - Support middlewares for encoding/decoding.\n            - Validate inputs/outputs with spec signature.\n            - Support shared memory transport.\n            - SerDe before/after transmission.\n        Args:\n            inputs (Dict[str, Any]): Inputs to the model (\"images\", \"texts\", \"prompts\" etc) as\n                defined in the ModelSpec.signature.\n        Returns:\n            Dict[str, Any]: Encoded inputs.\n        \"\"\"\n        # Validate data with spec signature\n        if method is None:\n            method = self._spec.default_method\n        if method not in self._spec.signature:\n            raise InferenceException(f\"Method {method} not found in spec signature.\")\n        sig: FunctionSignature = self._spec.signature[method]\n        inputs = FunctionSignature.validate(inputs, sig.parameters)\n\n        # Encode List[np.ndarray] as stacked np.ndarray (B, H, W, C)\n        for k, v in inputs.items():\n            if isinstance(v, Image.Image):\n                inputs[k] = np.asarray(v)\n            elif isinstance(v, list) and isinstance(v[0], Image.Image):\n                inputs[k] = np.stack([np.asarray(_v) for _v in v], axis=0)\n            elif isinstance(v, list) and isinstance(v[0], np.ndarray):\n                inputs[k] = np.stack(v, axis=0)\n\n        # Optionally, create/register shm and copy over numpy arrays to shm\n        if self._shm_objects is not None:\n            # If inputs are already registered, check if they've changed\n            # If they've changed, unregister and re-register.\n            # Checks: 1) keys match, 2) inputs are np.ndarray, 3) shapes match\n            if len(self._shm_objects):\n                valid = inputs.keys() == self._shm_objects.keys()\n                for k, v in inputs.items():\n                    try:\n                        valid &amp;= isinstance(v, np.ndarray)\n                        if valid and isinstance(v, np.ndarray):\n                            valid &amp;= v.shape == self._shm_objects[k].shape\n                    except Exception:\n                        valid = False\n                if not valid:\n                    logger.debug(\n                        \"\"\"Inputs are inconsistent with previously registered shared memory objects, unregistering ...\"\"\"\n                    )\n                    registered_str = [(k, type(v), v.shape) for k, v in self._shm_objects.items()]\n                    inputs_str = [\n                        (k, type(v), v.shape if isinstance(v, np.ndarray) else None) for k, v in inputs.items()\n                    ]\n                    logger.debug(\n                        f\"\"\"Unregistering due to inconsistent shapes ... [registered={registered_str}, \"\"\"\n                        f\"\"\"inputs={inputs_str}]\"\"\"\n                    )\n                    self.UnregisterSystemSharedMemory()\n\n            # Register system shared memory for inputs, if not already registered\n            if not len(self._shm_objects):\n                self.RegisterSystemSharedMemory(inputs)\n\n        # Copy data from numpy array to shared memory\n        if self._shm_objects is not None and len(self._shm_objects):\n            inputs = SharedMemoryTransportManager.copy(self._shm_objects, inputs)\n\n        # Pickle the data for transmission\n        return {k: dumps(v) for k, v in inputs.items()}\n\n    def _decode(self, response_bytes: bytes) -&gt; Any:\n        \"\"\"Decode the response bytes.\"\"\"\n        return loads(response_bytes)\n\n    def __del__(self):\n        \"\"\"Delete the shared memory.\"\"\"\n        if self._shm_objects is not None:\n            self.UnregisterSystemSharedMemory()\n\n    def GetModelInfo(self) -&gt; ModelSpec:\n        \"\"\"Get the relevant model information from the model name.\"\"\"\n        return self._spec\n\n    def Load(self, num_replicas: int = 1) -&gt; None:\n        \"\"\"Load the model.\"\"\"\n        return self._client.LoadModel(self.id, num_replicas=num_replicas)\n\n    def RegisterSystemSharedMemory(self, inputs: Dict[str, Any]) -&gt; None:\n        \"\"\"Register system shared memory for inputs.\n\n        Args:\n            inputs (Dict[str, Any]): Inputs to the model (\"images\", \"texts\", \"prompts\" etc) as\n                defined in the ModelSpec.signature. For example, {\"images\": np.ndarray}.\n        \"\"\"\n        # Create shared memory request\n        # We convert the numpy arrays to TensorSpec(s) to let the\n        # server know the shape and dtype of the underlying shm data.\n        if not NOS_SHM_ENABLED:\n            logger.warning(\"Shared memory is not enabled, skipping.\")\n            return\n\n        shm_request = {}\n        for k, v in inputs.items():\n            if isinstance(v, np.ndarray):\n                shm_request[k] = TensorSpec(v.shape, dtype=str(v.dtype))\n        if not len(shm_request):\n            logger.debug(f\"Skipping shm registration, no numpy arrays found in inputs [inputs={inputs}]\")\n            return\n        logger.debug(f\"Registering shm [request={shm_request}]\")\n\n        # Request shared memory, fail gracefully if not supported\n        try:\n            # Clear the cached object_id and namespace so that they are re-initialized\n            if \"object_id\" in self.__dict__:  # noqa: WPS421\n                del self.object_id\n                del self.namespace\n            response = self.stub.RegisterSystemSharedMemory(\n                nos_service_pb2.GenericRequest(request_bytes=dumps(shm_request)),\n                metadata=[(\"client_id\", self.client_id), (\"object_id\", self.object_id)],\n            )\n\n            # Register the shared memory objects by name on the client\n            # Note (spillai): This calls __setstate__ on the SharedMemoryNumpyObject\n            self._shm_objects = loads(response.response_bytes)\n            logger.debug(f\"Registered shm [namespace={self.namespace}, objects={self._shm_objects}]\")\n        except grpc.RpcError as e:\n            logger.debug(f\"Failed to register shm [request={shm_request}, e={e.details()}], skipping.\")\n            self._shm_objects = None\n\n    def UnregisterSystemSharedMemory(self) -&gt; None:\n        \"\"\"Unregister system shared memory.\"\"\"\n        if self._shm_objects is None:\n            logger.warning(\"Shared memory is not enabled, skipping.\")\n            return\n\n        if len(self._shm_objects):\n            logger.debug(\n                f\"Unregistering shm [namespace={self.namespace}, objects={[(k, v) for k, v in self._shm_objects.items()]}\"\n            )\n\n            # Close the shared memory objects\n            shm_objects_name_map = {k: v.name for k, v in self._shm_objects.items()}\n            for _k, v in self._shm_objects.items():\n                v.close()\n\n            # Unregister the shared memory objects on the server\n            try:\n                self.stub.UnregisterSystemSharedMemory(\n                    nos_service_pb2.GenericRequest(request_bytes=dumps(shm_objects_name_map)),\n                    metadata=[(\"client_id\", self.client_id), (\"object_id\", self.object_id)],\n                )\n                # Delete the shared memory objects after safely closing (client-side) and unregistering them (server-side).\n                self._shm_objects = {}\n                logger.debug(f\"Unregistered shm [{self._shm_objects}]\")\n            except grpc.RpcError as e:\n                logger.error(f\"Failed to unregister shm [{self._shm_objects}], error: {e.details()}\")\n                raise ClientException(f\"Failed to unregister shm [{self._shm_objects}]\", e)\n\n    def __call__(self, _method: str = None, _stream: bool = False, **inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Call the instantiated module/model.\n\n        Args:\n            _method (str, optional): Method to call on the model. Defaults to None.\n            _stream (bool, optional): Stream the response. Defaults to False.\n            **inputs (Dict[str, Any]): Inputs to the model (\"images\", \"texts\", \"prompts\" etc) as\n                defined in ModelSpec.signature.\n        Returns:\n            Dict[str, Any]: Inference response.\n        Raises:\n            NosInputValidationException: If the inputs are inconsistent with the spec signature.\n            NosInferenceException: If the server fails to respond to the request.\n            NosClientException: If the outputs cannot be decoded.\n\n        Note: While encoding the inputs, we check if the input dictionary is consistent\n        with inputs/outputs defined in `spec.signature` and only then encode it.\n        \"\"\"\n        # Encode the inputs\n        st = time.perf_counter()\n        try:\n            inputs = self._encode(inputs, method=_method)\n        except Exception as e:\n            logger.error(f\"Failed to encode inputs [model={self.id}, method={_method}, inputs={inputs}, e={e}]\")\n            raise InputValidationException(\n                f\"Failed to encode inputs [model={self.id}, method={_method}, inputs={inputs}, e={e}]\", e\n            )\n        if NOS_PROFILING_ENABLED:\n            logger.debug(f\"Encoded inputs [model={self.id}, elapsed={(time.perf_counter() - st) * 1e3:.1f}ms]\")\n\n        # Prepare the request\n        request = nos_service_pb2.GenericRequest(\n            request_bytes=dumps(\n                {\n                    \"id\": self._spec.id,\n                    \"method\": _method,\n                    \"inputs\": inputs,\n                }\n            )\n        )\n        try:\n            # Execute the request\n            st = time.perf_counter()\n            logger.debug(f\"Executing request [model={self.id}]\")\n            if not _stream:\n                response: nos_service_pb2.GenericResponse = self.stub.Run(request)\n            else:\n                response: Iterable[nos_service_pb2.GenericResponse] = self.stub.Stream(request)\n            if NOS_PROFILING_ENABLED:\n                logger.debug(f\"Executed request [model={self.id}, elapsed={(time.perf_counter() - st) * 1e3:.1f}ms]\")\n        except grpc.RpcError as e:\n            logger.error(f\"Run() failed [details={e.details()}, inputs={inputs.keys()}]\")\n            raise InferenceException(f\"Run() failed [model={self.id}, details={e.details()}]\", e)\n\n        # Decode / stream the response\n        st = time.perf_counter()\n        try:\n            if not _stream:\n                response = self._decode(response.response_bytes)\n            else:\n                return _StreamingModuleResponse(response, self._decode)\n        except Exception as e:\n            logger.error(f\"Failed to decode response [model={self.id}, e={e}]\")\n            raise ClientException(f\"Failed to decode response [model={self.id}, e={e}]\", e)\n        if NOS_PROFILING_ENABLED:\n            logger.debug(f\"Decoded response [model={self.id}, elapsed={(time.perf_counter() - st) * 1e3:.1f}ms]\")\n        return response\n</code></pre>"},{"location":"docs/api/client.html#nos.client.grpc.Module.__init__","title":"__init__","text":"<pre><code>__init__(id: str, _client: Client, shm: bool = False) -&gt; None\n</code></pre>"},{"location":"docs/api/client.html#nos.client.grpc.Module.GetModelInfo","title":"GetModelInfo","text":"<pre><code>GetModelInfo() -&gt; ModelSpec\n</code></pre> <p>Get the relevant model information from the model name.</p> Source code in <code>nos/client/grpc.py</code> <pre><code>def GetModelInfo(self) -&gt; ModelSpec:\n    \"\"\"Get the relevant model information from the model name.\"\"\"\n    return self._spec\n</code></pre>"},{"location":"docs/api/client.html#nos.client.grpc.Module.__call__","title":"__call__","text":"<pre><code>__call__(_method: str = None, _stream: bool = False, **inputs: Dict[str, Any]) -&gt; Dict[str, Any]\n</code></pre> <p>Call the instantiated module/model.</p> <p>Parameters:</p> <ul> <li> <code>_method</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Method to call on the model. Defaults to None.</p> </li> <li> <code>_stream</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Stream the response. Defaults to False.</p> </li> <li> <code>**inputs</code>               (<code>Dict[str, Any]</code>, default:                   <code>{}</code> )           \u2013            <p>Inputs to the model (\"images\", \"texts\", \"prompts\" etc) as defined in ModelSpec.signature.</p> </li> </ul> <p>Returns:     Dict[str, Any]: Inference response. Raises:     NosInputValidationException: If the inputs are inconsistent with the spec signature.     NosInferenceException: If the server fails to respond to the request.     NosClientException: If the outputs cannot be decoded.</p> <p>Note: While encoding the inputs, we check if the input dictionary is consistent with inputs/outputs defined in <code>spec.signature</code> and only then encode it.</p> Source code in <code>nos/client/grpc.py</code> <pre><code>def __call__(self, _method: str = None, _stream: bool = False, **inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Call the instantiated module/model.\n\n    Args:\n        _method (str, optional): Method to call on the model. Defaults to None.\n        _stream (bool, optional): Stream the response. Defaults to False.\n        **inputs (Dict[str, Any]): Inputs to the model (\"images\", \"texts\", \"prompts\" etc) as\n            defined in ModelSpec.signature.\n    Returns:\n        Dict[str, Any]: Inference response.\n    Raises:\n        NosInputValidationException: If the inputs are inconsistent with the spec signature.\n        NosInferenceException: If the server fails to respond to the request.\n        NosClientException: If the outputs cannot be decoded.\n\n    Note: While encoding the inputs, we check if the input dictionary is consistent\n    with inputs/outputs defined in `spec.signature` and only then encode it.\n    \"\"\"\n    # Encode the inputs\n    st = time.perf_counter()\n    try:\n        inputs = self._encode(inputs, method=_method)\n    except Exception as e:\n        logger.error(f\"Failed to encode inputs [model={self.id}, method={_method}, inputs={inputs}, e={e}]\")\n        raise InputValidationException(\n            f\"Failed to encode inputs [model={self.id}, method={_method}, inputs={inputs}, e={e}]\", e\n        )\n    if NOS_PROFILING_ENABLED:\n        logger.debug(f\"Encoded inputs [model={self.id}, elapsed={(time.perf_counter() - st) * 1e3:.1f}ms]\")\n\n    # Prepare the request\n    request = nos_service_pb2.GenericRequest(\n        request_bytes=dumps(\n            {\n                \"id\": self._spec.id,\n                \"method\": _method,\n                \"inputs\": inputs,\n            }\n        )\n    )\n    try:\n        # Execute the request\n        st = time.perf_counter()\n        logger.debug(f\"Executing request [model={self.id}]\")\n        if not _stream:\n            response: nos_service_pb2.GenericResponse = self.stub.Run(request)\n        else:\n            response: Iterable[nos_service_pb2.GenericResponse] = self.stub.Stream(request)\n        if NOS_PROFILING_ENABLED:\n            logger.debug(f\"Executed request [model={self.id}, elapsed={(time.perf_counter() - st) * 1e3:.1f}ms]\")\n    except grpc.RpcError as e:\n        logger.error(f\"Run() failed [details={e.details()}, inputs={inputs.keys()}]\")\n        raise InferenceException(f\"Run() failed [model={self.id}, details={e.details()}]\", e)\n\n    # Decode / stream the response\n    st = time.perf_counter()\n    try:\n        if not _stream:\n            response = self._decode(response.response_bytes)\n        else:\n            return _StreamingModuleResponse(response, self._decode)\n    except Exception as e:\n        logger.error(f\"Failed to decode response [model={self.id}, e={e}]\")\n        raise ClientException(f\"Failed to decode response [model={self.id}, e={e}]\", e)\n    if NOS_PROFILING_ENABLED:\n        logger.debug(f\"Decoded response [model={self.id}, elapsed={(time.perf_counter() - st) * 1e3:.1f}ms]\")\n    return response\n</code></pre>"},{"location":"docs/api/executors.html","title":"Executors","text":""},{"location":"docs/api/executors.html#nos.executors.ray","title":"nos.executors.ray","text":"<p>Ray executor for NOS.</p> <p>This module provides a Ray executor for NOS. The Ray executor is a singleton instance that can be used to start a Ray head, connect to an existing Ray cluster, and submit tasks to Ray. We use Ray as a backend for distributed computation and containerize them in docker to isolate the environment.</p>"},{"location":"docs/api/executors.html#nos.executors.ray.RayRuntimeSpec","title":"RayRuntimeSpec  <code>dataclass</code>","text":"Source code in <code>nos/executors/ray.py</code> <pre><code>@dataclass\nclass RayRuntimeSpec:\n    namespace: str = NOS_RAY_NS\n    \"\"\"Namespace for Ray runtime.\"\"\"\n    runtime_env: str = NOS_RAY_ENV\n    \"\"\"Runtime environment for Ray runtime.\"\"\"\n</code></pre>"},{"location":"docs/api/executors.html#nos.executors.ray.RayRuntimeSpec.namespace","title":"namespace  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>namespace: str = NOS_RAY_NS\n</code></pre> <p>Namespace for Ray runtime.</p>"},{"location":"docs/api/executors.html#nos.executors.ray.RayRuntimeSpec.runtime_env","title":"runtime_env  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>runtime_env: str = NOS_RAY_ENV\n</code></pre> <p>Runtime environment for Ray runtime.</p>"},{"location":"docs/api/executors.html#nos.executors.ray.RayExecutor","title":"RayExecutor  <code>dataclass</code>","text":"<p>Executor for Ray.</p> Source code in <code>nos/executors/ray.py</code> <pre><code>@dataclass\nclass RayExecutor(metaclass=SingletonMetaclass):\n    \"\"\"Executor for Ray.\"\"\"\n\n    spec: RayRuntimeSpec = RayRuntimeSpec()\n    \"\"\"Runtime spec for Ray.\"\"\"\n\n    @classmethod\n    def get(cls) -&gt; \"RayExecutor\":\n        \"\"\"Get the singleton instance of RayExecutor.\"\"\"\n        return cls()\n\n    def is_initialized(self) -&gt; bool:\n        \"\"\"Check if Ray is initialized.\"\"\"\n        return ray.is_initialized()\n\n    def init(self, max_attempts: int = 5, timeout: int = 60, retry_interval: int = 5) -&gt; None:\n        \"\"\"Initialize Ray exector.\n\n        This implementation forces Ray to start a new cluster instance via\n        `ray.init(address=\"local\")` and then connecting to the\n        server via `ray.init(address=\"auto\")`. The first call to\n        `ray.init(address=\"auto\")` raises a `ConnectionError` and proceeds to\n        force-start a new ray cluster instance, followed by a second call to\n        `ray.init(address=\"auto\")` which successfully connects to the server.\n\n        In the case of a Ray cluster already running, the first call to\n        `ray.init(address=\"auto\")` will successfully connect to the server.\n\n        Args:\n            max_attempts: Number of retries to attempt to connect to an existing\n            timeout: Time to wait for Ray to start. Defaults to 60 seconds.\n            retry_interval: Time to wait between retries. Defaults to 5 seconds.\n        \"\"\"\n        # Ignore predefined RAY_ADDRESS environment variable.\n        if \"RAY_ADDRESS\" in os.environ:\n            del os.environ[\"RAY_ADDRESS\"]\n\n        st = time.time()\n        attempt = 0\n\n        # Attempt to connect to an existing ray cluster.\n        # Allow upto 5 attempts, or if timeout of 60 seconds is reached.\n        console = rich.console.Console()\n        while time.time() - st &lt;= timeout and attempt &lt; max_attempts:\n            # Attempt to connect to an existing ray cluster in the background.\n            try:\n                with console.status(\n                    \"[bold green] InferenceExecutor :: Connecting to backend ... [/bold green]\"\n                ) as status:\n                    logger.debug(f\"Connecting to executor: namespace={self.spec.namespace}\")\n                    ray.init(\n                        address=\"auto\",\n                        namespace=self.spec.namespace,\n                        ignore_reinit_error=True,\n                        logging_level=\"error\",\n                    )\n                    status.stop()\n                    console.print(\"[bold green] \u2713 InferenceExecutor :: Connected to backend. [/bold green]\")\n                    logger.debug(\n                        f\"Connected to executor: namespace={self.spec.namespace} (time={time.time() - st:.2f}s)\"\n                    )\n                return True\n            except ConnectionError as exc:\n                # If Ray head is not running (this results in a ConnectionError),\n                # start it in a background subprocess.\n                if attempt &gt; 0:\n                    logger.error(\n                        f\"Failed to connect to InferenceExecutor.\\n\"\n                        f\"{exc}\\n\"\n                        f\"Retrying {attempt}/{max_attempts} after {retry_interval}s...\"\n                    )\n                    time.sleep(retry_interval)\n                else:\n                    logger.debug(\"No executor found, starting a new one\")\n                    self.start()\n                attempt += 1\n                continue\n        logger.error(f\"Failed to connect to InferenceExecutor: namespace={self.spec.namespace}.\")\n        return False\n\n    def start(self) -&gt; None:\n        \"\"\"Force-start a local instance of Ray head.\"\"\"\n        level = getattr(logging, LOGGING_LEVEL)\n\n        start_t = time.time()\n        console = rich.console.Console()\n        console.print(\"[bold green] \u2713 InferenceExecutor :: Backend initializing (as daemon) ... [/bold green]\")\n        try:\n            logger.debug(f\"Starting executor: namespace={self.spec.namespace}\")\n            ray.init(\n                _node_name=\"nos-executor\",\n                address=\"local\",\n                namespace=self.spec.namespace,\n                object_store_memory=NOS_RAY_OBJECT_STORE_MEMORY,\n                ignore_reinit_error=False,\n                include_dashboard=NOS_RAY_DASHBOARD_ENABLED,\n                configure_logging=True,\n                logging_level=logging.ERROR,\n                log_to_driver=level &lt;= logging.ERROR,\n                dashboard_host=\"0.0.0.0\" if NOS_RAY_DASHBOARD_ENABLED else None,\n            )\n            logger.debug(f\"Started executor: namespace={self.spec.namespace} (time={time.time() - start_t:.2f}s)\")\n        except ConnectionError as exc:\n            logger.error(f\"Failed to start executor: exc={exc}.\")\n            raise RuntimeError(f\"Failed to start executor: exc={exc}.\")\n        console.print(\n            f\"[bold green] \u2713 InferenceExecutor :: Backend initialized (elapsed={time.time() - start_t:.1f}s). [/bold green]\"\n        )\n        logger.debug(f\"Started executor: namespace={self.spec.namespace} (time={time.time() - start_t}s)\")\n\n    def stop(self) -&gt; None:\n        \"\"\"Stop Ray head.\"\"\"\n        console = rich.console.Console()\n        console.print(\"[bold green] InferenceExecutor :: Backend stopping ... [/bold green]\")\n        try:\n            logger.debug(f\"Stopping executor: namespace={self.spec.namespace}\")\n            ray.shutdown()\n            logger.debug(f\"Stopped executor: namespace={self.spec.namespace}\")\n        except Exception as exc:\n            logger.error(f\"Failed to stop executor: exc={exc}.\")\n            raise RuntimeError(f\"Failed to stop executor: exc={exc}.\")\n        console.print(\"[bold green] \u2713 InferenceExecutor :: Backend stopped. [/bold green]\")\n\n    @property\n    def pid(self) -&gt; Optional[int]:\n        \"\"\"Get PID of Ray head.\"\"\"\n        for proc in psutil.process_iter(attrs=[\"pid\", \"name\"]):\n            if proc.name() == \"raylet\":\n                return proc.pid\n        return None\n\n    @cached_property\n    def jobs(self) -&gt; \"RayJobExecutor\":\n        \"\"\"Get the ray jobs executor.\"\"\"\n        return RayJobExecutor()\n</code></pre>"},{"location":"docs/api/executors.html#nos.executors.ray.RayExecutor.spec","title":"spec  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>spec: RayRuntimeSpec = RayRuntimeSpec()\n</code></pre> <p>Runtime spec for Ray.</p>"},{"location":"docs/api/executors.html#nos.executors.ray.RayExecutor.pid","title":"pid  <code>property</code>","text":"<pre><code>pid: Optional[int]\n</code></pre> <p>Get PID of Ray head.</p>"},{"location":"docs/api/executors.html#nos.executors.ray.RayExecutor.jobs","title":"jobs  <code>cached</code> <code>property</code>","text":"<pre><code>jobs: RayJobExecutor\n</code></pre> <p>Get the ray jobs executor.</p>"},{"location":"docs/api/executors.html#nos.executors.ray.RayExecutor.get","title":"get  <code>classmethod</code>","text":"<pre><code>get() -&gt; RayExecutor\n</code></pre> <p>Get the singleton instance of RayExecutor.</p> Source code in <code>nos/executors/ray.py</code> <pre><code>@classmethod\ndef get(cls) -&gt; \"RayExecutor\":\n    \"\"\"Get the singleton instance of RayExecutor.\"\"\"\n    return cls()\n</code></pre>"},{"location":"docs/api/executors.html#nos.executors.ray.RayExecutor.is_initialized","title":"is_initialized","text":"<pre><code>is_initialized() -&gt; bool\n</code></pre> <p>Check if Ray is initialized.</p> Source code in <code>nos/executors/ray.py</code> <pre><code>def is_initialized(self) -&gt; bool:\n    \"\"\"Check if Ray is initialized.\"\"\"\n    return ray.is_initialized()\n</code></pre>"},{"location":"docs/api/executors.html#nos.executors.ray.RayExecutor.init","title":"init","text":"<pre><code>init(max_attempts: int = 5, timeout: int = 60, retry_interval: int = 5) -&gt; None\n</code></pre> <p>Initialize Ray exector.</p> <p>This implementation forces Ray to start a new cluster instance via <code>ray.init(address=\"local\")</code> and then connecting to the server via <code>ray.init(address=\"auto\")</code>. The first call to <code>ray.init(address=\"auto\")</code> raises a <code>ConnectionError</code> and proceeds to force-start a new ray cluster instance, followed by a second call to <code>ray.init(address=\"auto\")</code> which successfully connects to the server.</p> <p>In the case of a Ray cluster already running, the first call to <code>ray.init(address=\"auto\")</code> will successfully connect to the server.</p> <p>Parameters:</p> <ul> <li> <code>max_attempts</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Number of retries to attempt to connect to an existing</p> </li> <li> <code>timeout</code>               (<code>int</code>, default:                   <code>60</code> )           \u2013            <p>Time to wait for Ray to start. Defaults to 60 seconds.</p> </li> <li> <code>retry_interval</code>               (<code>int</code>, default:                   <code>5</code> )           \u2013            <p>Time to wait between retries. Defaults to 5 seconds.</p> </li> </ul> Source code in <code>nos/executors/ray.py</code> <pre><code>def init(self, max_attempts: int = 5, timeout: int = 60, retry_interval: int = 5) -&gt; None:\n    \"\"\"Initialize Ray exector.\n\n    This implementation forces Ray to start a new cluster instance via\n    `ray.init(address=\"local\")` and then connecting to the\n    server via `ray.init(address=\"auto\")`. The first call to\n    `ray.init(address=\"auto\")` raises a `ConnectionError` and proceeds to\n    force-start a new ray cluster instance, followed by a second call to\n    `ray.init(address=\"auto\")` which successfully connects to the server.\n\n    In the case of a Ray cluster already running, the first call to\n    `ray.init(address=\"auto\")` will successfully connect to the server.\n\n    Args:\n        max_attempts: Number of retries to attempt to connect to an existing\n        timeout: Time to wait for Ray to start. Defaults to 60 seconds.\n        retry_interval: Time to wait between retries. Defaults to 5 seconds.\n    \"\"\"\n    # Ignore predefined RAY_ADDRESS environment variable.\n    if \"RAY_ADDRESS\" in os.environ:\n        del os.environ[\"RAY_ADDRESS\"]\n\n    st = time.time()\n    attempt = 0\n\n    # Attempt to connect to an existing ray cluster.\n    # Allow upto 5 attempts, or if timeout of 60 seconds is reached.\n    console = rich.console.Console()\n    while time.time() - st &lt;= timeout and attempt &lt; max_attempts:\n        # Attempt to connect to an existing ray cluster in the background.\n        try:\n            with console.status(\n                \"[bold green] InferenceExecutor :: Connecting to backend ... [/bold green]\"\n            ) as status:\n                logger.debug(f\"Connecting to executor: namespace={self.spec.namespace}\")\n                ray.init(\n                    address=\"auto\",\n                    namespace=self.spec.namespace,\n                    ignore_reinit_error=True,\n                    logging_level=\"error\",\n                )\n                status.stop()\n                console.print(\"[bold green] \u2713 InferenceExecutor :: Connected to backend. [/bold green]\")\n                logger.debug(\n                    f\"Connected to executor: namespace={self.spec.namespace} (time={time.time() - st:.2f}s)\"\n                )\n            return True\n        except ConnectionError as exc:\n            # If Ray head is not running (this results in a ConnectionError),\n            # start it in a background subprocess.\n            if attempt &gt; 0:\n                logger.error(\n                    f\"Failed to connect to InferenceExecutor.\\n\"\n                    f\"{exc}\\n\"\n                    f\"Retrying {attempt}/{max_attempts} after {retry_interval}s...\"\n                )\n                time.sleep(retry_interval)\n            else:\n                logger.debug(\"No executor found, starting a new one\")\n                self.start()\n            attempt += 1\n            continue\n    logger.error(f\"Failed to connect to InferenceExecutor: namespace={self.spec.namespace}.\")\n    return False\n</code></pre>"},{"location":"docs/api/executors.html#nos.executors.ray.RayExecutor.start","title":"start","text":"<pre><code>start() -&gt; None\n</code></pre> <p>Force-start a local instance of Ray head.</p> Source code in <code>nos/executors/ray.py</code> <pre><code>def start(self) -&gt; None:\n    \"\"\"Force-start a local instance of Ray head.\"\"\"\n    level = getattr(logging, LOGGING_LEVEL)\n\n    start_t = time.time()\n    console = rich.console.Console()\n    console.print(\"[bold green] \u2713 InferenceExecutor :: Backend initializing (as daemon) ... [/bold green]\")\n    try:\n        logger.debug(f\"Starting executor: namespace={self.spec.namespace}\")\n        ray.init(\n            _node_name=\"nos-executor\",\n            address=\"local\",\n            namespace=self.spec.namespace,\n            object_store_memory=NOS_RAY_OBJECT_STORE_MEMORY,\n            ignore_reinit_error=False,\n            include_dashboard=NOS_RAY_DASHBOARD_ENABLED,\n            configure_logging=True,\n            logging_level=logging.ERROR,\n            log_to_driver=level &lt;= logging.ERROR,\n            dashboard_host=\"0.0.0.0\" if NOS_RAY_DASHBOARD_ENABLED else None,\n        )\n        logger.debug(f\"Started executor: namespace={self.spec.namespace} (time={time.time() - start_t:.2f}s)\")\n    except ConnectionError as exc:\n        logger.error(f\"Failed to start executor: exc={exc}.\")\n        raise RuntimeError(f\"Failed to start executor: exc={exc}.\")\n    console.print(\n        f\"[bold green] \u2713 InferenceExecutor :: Backend initialized (elapsed={time.time() - start_t:.1f}s). [/bold green]\"\n    )\n    logger.debug(f\"Started executor: namespace={self.spec.namespace} (time={time.time() - start_t}s)\")\n</code></pre>"},{"location":"docs/api/executors.html#nos.executors.ray.RayExecutor.stop","title":"stop","text":"<pre><code>stop() -&gt; None\n</code></pre> <p>Stop Ray head.</p> Source code in <code>nos/executors/ray.py</code> <pre><code>def stop(self) -&gt; None:\n    \"\"\"Stop Ray head.\"\"\"\n    console = rich.console.Console()\n    console.print(\"[bold green] InferenceExecutor :: Backend stopping ... [/bold green]\")\n    try:\n        logger.debug(f\"Stopping executor: namespace={self.spec.namespace}\")\n        ray.shutdown()\n        logger.debug(f\"Stopped executor: namespace={self.spec.namespace}\")\n    except Exception as exc:\n        logger.error(f\"Failed to stop executor: exc={exc}.\")\n        raise RuntimeError(f\"Failed to stop executor: exc={exc}.\")\n    console.print(\"[bold green] \u2713 InferenceExecutor :: Backend stopped. [/bold green]\")\n</code></pre>"},{"location":"docs/api/executors.html#nos.executors.ray.RayJobExecutor","title":"RayJobExecutor  <code>dataclass</code>","text":"<p>Ray job executor.</p> Source code in <code>nos/executors/ray.py</code> <pre><code>@dataclass\nclass RayJobExecutor(metaclass=SingletonMetaclass):\n    \"\"\"Ray job executor.\"\"\"\n\n    client: JobSubmissionClient = field(init=False)\n    \"\"\"Job submission client.\"\"\"\n\n    def __post_init__(self):\n        \"\"\"Post-initialization.\"\"\"\n        if not ray.is_initialized():\n            raise RuntimeError(\"Ray executor is not initialized.\")\n        self.client = JobSubmissionClient(NOS_RAY_JOB_CLIENT_ADDRESS)\n\n    def submit(self, *args, **kwargs) -&gt; str:\n        \"\"\"Submit a job to Ray.\"\"\"\n        job_id = self.client.submit_job(*args, **kwargs)\n        logger.debug(f\"Submitted job with id: {job_id}\")\n        return job_id\n\n    def list(self) -&gt; List[str]:\n        \"\"\"List all jobs.\"\"\"\n        return self.client.list_jobs()\n\n    def info(self, job_id: str) -&gt; str:\n        \"\"\"Get info for a job.\"\"\"\n        return self.client.get_job_info(job_id)\n\n    def status(self, job_id: str) -&gt; str:\n        \"\"\"Get status for a job.\"\"\"\n        return self.client.get_job_status(job_id)\n\n    def logs(self, job_id: str) -&gt; str:\n        \"\"\"Get logs for a job.\"\"\"\n        return self.client.get_job_logs(job_id)\n\n    def wait(self, job_id: str, timeout: int = 600, retry_interval: int = 5) -&gt; str:\n        \"\"\"Wait for a job to complete.\"\"\"\n        status = None\n        st = time.time()\n        while time.time() - st &lt; timeout:\n            status = self.status(job_id)\n            if str(status) == \"SUCCEEDED\":\n                logger.debug(f\"Training job completed [job_id={job_id}, status={status}]\")\n                return status\n            else:\n                logger.debug(f\"Training job not completed yet [job_id={job_id}, status={status}]\")\n                time.sleep(retry_interval)\n        logger.warning(f\"Training job timed out [job_id={job_id}, status={status}]\")\n        return status\n</code></pre>"},{"location":"docs/api/executors.html#nos.executors.ray.RayJobExecutor.client","title":"client  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>client: JobSubmissionClient = field(init=False)\n</code></pre> <p>Job submission client.</p>"},{"location":"docs/api/executors.html#nos.executors.ray.RayJobExecutor.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Post-initialization.</p> Source code in <code>nos/executors/ray.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Post-initialization.\"\"\"\n    if not ray.is_initialized():\n        raise RuntimeError(\"Ray executor is not initialized.\")\n    self.client = JobSubmissionClient(NOS_RAY_JOB_CLIENT_ADDRESS)\n</code></pre>"},{"location":"docs/api/executors.html#nos.executors.ray.RayJobExecutor.submit","title":"submit","text":"<pre><code>submit(*args, **kwargs) -&gt; str\n</code></pre> <p>Submit a job to Ray.</p> Source code in <code>nos/executors/ray.py</code> <pre><code>def submit(self, *args, **kwargs) -&gt; str:\n    \"\"\"Submit a job to Ray.\"\"\"\n    job_id = self.client.submit_job(*args, **kwargs)\n    logger.debug(f\"Submitted job with id: {job_id}\")\n    return job_id\n</code></pre>"},{"location":"docs/api/executors.html#nos.executors.ray.RayJobExecutor.list","title":"list","text":"<pre><code>list() -&gt; List[str]\n</code></pre> <p>List all jobs.</p> Source code in <code>nos/executors/ray.py</code> <pre><code>def list(self) -&gt; List[str]:\n    \"\"\"List all jobs.\"\"\"\n    return self.client.list_jobs()\n</code></pre>"},{"location":"docs/api/executors.html#nos.executors.ray.RayJobExecutor.info","title":"info","text":"<pre><code>info(job_id: str) -&gt; str\n</code></pre> <p>Get info for a job.</p> Source code in <code>nos/executors/ray.py</code> <pre><code>def info(self, job_id: str) -&gt; str:\n    \"\"\"Get info for a job.\"\"\"\n    return self.client.get_job_info(job_id)\n</code></pre>"},{"location":"docs/api/executors.html#nos.executors.ray.RayJobExecutor.status","title":"status","text":"<pre><code>status(job_id: str) -&gt; str\n</code></pre> <p>Get status for a job.</p> Source code in <code>nos/executors/ray.py</code> <pre><code>def status(self, job_id: str) -&gt; str:\n    \"\"\"Get status for a job.\"\"\"\n    return self.client.get_job_status(job_id)\n</code></pre>"},{"location":"docs/api/executors.html#nos.executors.ray.RayJobExecutor.logs","title":"logs","text":"<pre><code>logs(job_id: str) -&gt; str\n</code></pre> <p>Get logs for a job.</p> Source code in <code>nos/executors/ray.py</code> <pre><code>def logs(self, job_id: str) -&gt; str:\n    \"\"\"Get logs for a job.\"\"\"\n    return self.client.get_job_logs(job_id)\n</code></pre>"},{"location":"docs/api/executors.html#nos.executors.ray.RayJobExecutor.wait","title":"wait","text":"<pre><code>wait(job_id: str, timeout: int = 600, retry_interval: int = 5) -&gt; str\n</code></pre> <p>Wait for a job to complete.</p> Source code in <code>nos/executors/ray.py</code> <pre><code>def wait(self, job_id: str, timeout: int = 600, retry_interval: int = 5) -&gt; str:\n    \"\"\"Wait for a job to complete.\"\"\"\n    status = None\n    st = time.time()\n    while time.time() - st &lt; timeout:\n        status = self.status(job_id)\n        if str(status) == \"SUCCEEDED\":\n            logger.debug(f\"Training job completed [job_id={job_id}, status={status}]\")\n            return status\n        else:\n            logger.debug(f\"Training job not completed yet [job_id={job_id}, status={status}]\")\n            time.sleep(retry_interval)\n    logger.warning(f\"Training job timed out [job_id={job_id}, status={status}]\")\n    return status\n</code></pre>"},{"location":"docs/api/executors.html#nos.executors.ray.init","title":"init","text":"<pre><code>init(*args, **kwargs) -&gt; bool\n</code></pre> <p>Initialize Ray executor.</p> Source code in <code>nos/executors/ray.py</code> <pre><code>def init(*args, **kwargs) -&gt; bool:\n    \"\"\"Initialize Ray executor.\"\"\"\n    logger.debug(f\"Initializing executor: args={args}, kwargs={kwargs}\")\n    exector = RayExecutor.get()\n    return exector.init(*args, **kwargs)\n</code></pre>"},{"location":"docs/api/hub.html","title":"nos.hub","text":""},{"location":"docs/api/hub.html#nos.hub.Hub","title":"nos.hub.Hub","text":"<p>Registry for models.</p> Source code in <code>nos/hub/__init__.py</code> <pre><code>class Hub:\n    \"\"\"Registry for models.\"\"\"\n\n    _instance: Optional[\"Hub\"] = None\n    \"\"\"Singleton instance.\"\"\"\n    _registry: Dict[str, ModelSpec] = {}\n    \"\"\"Model specifications lookup for all models registered.\"\"\"\n\n    @classmethod\n    def get(cls: \"Hub\") -&gt; \"Hub\":\n        \"\"\"Get the singleton instance.\n\n        Returns:\n            Hub: Singleton instance.\n        \"\"\"\n        if cls._instance is None:\n            cls._instance = cls()\n            # Register models / Populate the registry\n            import nos.models  # noqa: F401, E402\n        return cls._instance\n\n    def __contains__(self, model_id: str) -&gt; bool:\n        return model_id in self.get()._registry\n\n    @classmethod\n    def list(cls, private: bool = False) -&gt; List[str]:\n        \"\"\"List models in the registry.\n\n        Args:\n            private (bool): Whether to include private models.\n\n        Returns:\n            List[str]: List of model names.\n        \"\"\"\n        if private:\n            raise NotImplementedError(\"Private models not supported.\")\n        return [k for k in cls.get()._registry.keys()]\n\n    @classmethod\n    def load_spec(cls, model_id: str) -&gt; ModelSpec:\n        \"\"\"Load model spec from the registry.\n\n        Args:\n            model_id (str): Model identifier (e.g. `openai/clip-vit-base-patch32`).\n\n        Returns:\n            ModelSpec: Model specification.\n        \"\"\"\n        try:\n            return cls.get()._registry[model_id]\n        except KeyError:\n            raise KeyError(f\"Unavailable model (name={model_id}).\")\n\n    @classmethod\n    def load(cls, model_id: str) -&gt; Any:\n        \"\"\"Instantiate model from the registry.\n\n        Args:\n            model_id (str): Model identifier (e.g. `openai/clip-vit-base-patch32`).\n\n        Returns:\n            Any: Instantiated model.\n        \"\"\"\n        spec: ModelSpec = cls.load_spec(model_id)\n        # Note (spillai): Loading the default signature here is OK\n        # since all the signatures have the same `func_or_cls`.\n        sig: FunctionSignature = spec.default_signature\n        return sig.func_or_cls(*sig.init_args, **sig.init_kwargs)\n\n    @classmethod\n    def register(\n        cls,\n        model_id: str,\n        task: TaskType,\n        func_or_cls: Callable,\n        init_args: Tuple[Any] = (),\n        init_kwargs: Dict[str, Any] = {},  # noqa: B006\n        method: str = \"__call__\",\n        inputs: Dict[str, Any] = {},  # noqa: B006\n        outputs: Union[Any, Dict[str, Any], None] = None,  # noqa: B006\n        resources: ModelResources = None,\n        **kwargs,\n    ) -&gt; ModelSpec:\n        \"\"\"Model registry decorator.\n\n        Args:\n            model_id (str): Model identifier (e.g. `openai/clip-vit-base-patch32`).\n            task (TaskType): Task type (e.g. `TaskType.OBJECT_DETECTION_2D`).\n            func_or_cls (Type[Any]): Model function or class.\n            **kwargs: Additional keyword arguments.\n        Returns:\n            ModelSpec: Model specification.\n        \"\"\"\n        logger.debug(\n            f\"\"\"Registering model [model={model_id}, task={task}, func_or_cls={func_or_cls}, \"\"\"\n            f\"\"\"inputs={inputs}, outputs={outputs}, \"\"\"\n            f\"\"\"init_args={init_args}, init_kwargs={init_kwargs}, method={method}]\"\"\"\n        )\n\n        # Create signature\n        signature: Dict[str, FunctionSignature] = {\n            method: FunctionSignature(\n                func_or_cls,\n                method=method,\n                init_args=init_args,\n                init_kwargs=init_kwargs,\n                input_annotations=inputs,\n                output_annotations=outputs,\n            ),\n        }\n\n        # Get hub instance\n        hub = cls.get()\n\n        # Add metadata for the model\n        catalog = ModelSpecMetadataCatalog.get()\n        spec = ModelSpec(model_id, signature=signature)\n        logger.debug(f\"Created model spec [id={model_id}, spec={spec}]\")\n\n        # Add task metadata for the model\n        if task is not None:\n            metadata = ModelSpecMetadata(model_id, method, task)\n            catalog._metadata_catalog[f\"{model_id}/{method}\"] = metadata\n\n        # Add model resources\n        if resources is not None:\n            catalog._resources_catalog[f\"{model_id}/{method}\"] = resources\n\n        # Register model id to model spec registry\n        if model_id not in hub._registry:\n            hub._registry[model_id] = spec\n            logger.debug(f\"Registered model to hub registry [id={model_id}, spec={spec}]\")\n\n        # Add another signature if the model is already registered\n        else:\n            _spec = hub._registry[model_id]\n            if method not in _spec.signature:\n                logger.debug(\n                    f\"Adding task signature [model={model_id}, task={task}, method={method}, sig={spec.signature}]\"\n                )\n                _spec.signature[method] = spec.signature[method]\n                catalog._metadata_catalog[f\"{model_id}/{method}\"] = spec.metadata(method)\n            else:\n                logger.debug(\n                    f\"Task signature already registered [model={model_id}, task={task}, method={method}, sig={spec.signature}]\"\n                )\n\n        logger.debug(f\"Registered model [id={model_id}, spec={spec}]\")\n        return spec\n\n    @classmethod\n    def register_spec(cls, spec: ModelSpec, task: TaskType = None, resources: ModelResources = None) -&gt; ModelSpec:\n        \"\"\"Register model spec to the registry.\n\n        Args:\n            spec (ModelSpec): Model specification.\n            task (TaskType): Task type (e.g. `TaskType.OBJECT_DETECTION_2D`).\n            resources (ModelResources): Model resources.\n        Returns:\n            ModelSpec: Model specification.\n        \"\"\"\n        logger.debug(f\"Registering model spec [id={spec.id}, spec={spec}]\")\n\n        # Get hub instance\n        hub = cls.get()\n\n        # Add metadata for the model\n        catalog = ModelSpecMetadataCatalog.get()\n\n        # Add task metadata for the model\n        if task is not None:\n            metadata = ModelSpecMetadata(spec.id, spec.default_method, task)\n            catalog._metadata_catalog[f\"{spec.id}/{spec.default_method}\"] = metadata\n\n        # Add model resources\n        if resources is not None:\n            catalog._resources_catalog[f\"{spec.id}/{spec.default_method}\"] = resources\n\n        # Register model id to model spec registry\n        if spec.id not in hub._registry:\n            hub._registry[spec.id] = spec\n            logger.debug(f\"Registered model spec [id={spec.id}, spec={spec}]\")\n        return spec\n\n    @classmethod\n    def register_from_yaml(cls, filename: str) -&gt; List[Any]:\n        \"\"\"Register models from a catalog YAML.\n\n        Args:\n            filename (str): Path to the deployment config YAML file.\n        Returns:\n            List[ModelSpec]: List of model specifications.\n        \"\"\"\n\n        @dataclass\n        class _ModelImportConfig:\n            \"\"\"Model import configuration.\"\"\"\n\n            id: str\n            \"\"\"Model identifier.\"\"\"\n            runtime_env: str\n            \"\"\"Runtime environment.\"\"\"\n            model_path: str\n            \"\"\"Model path.\"\"\"\n            model_cls: Callable\n            \"\"\"Model class name.\"\"\"\n            default_method: str\n            \"\"\"Default model method name.\"\"\"\n            init_args: Tuple[Any, ...] = field(default_factory=tuple)\n            \"\"\"Arguments to initialize the model instance.\"\"\"\n            init_kwargs: Dict[str, Any] = field(default_factory=dict)\n            \"\"\"Keyword arguments to initialize the model instance.\"\"\"\n            deployment: ModelDeploymentSpec = field(default_factory=ModelDeploymentSpec)\n            \"\"\"Model deployment specification.\"\"\"\n\n            @model_validator(mode=\"before\")\n            @classmethod\n            def _validate_model_cls_import(cls, values):\n                \"\"\"Validate the model.\"\"\"\n                import importlib\n                import importlib.util\n                import sys\n\n                model_cls_name = values.kwargs.get(\"model_cls\")\n                model_path = values.kwargs.get(\"model_path\")\n\n                # Check if model_path is a valid path relative to the directory containing\n                # the catalog.yaml file.\n                model_path = Path(filename).parent / model_path\n                if not model_path.exists():\n                    logger.error(f\"Invalid model path provided, model_path={model_path}.\")\n                    raise FileNotFoundError(f\"Invalid model path provided, model_path={model_path}.\")\n\n                # Check if the model_cls is importable from the model_path\n                try:\n                    # Load `model_cls` from the `model_path`\n                    logger.debug(f\"Loading model class [model_cls={model_cls_name}, model_path={model_path}].\")\n                    sys.path.append(str(model_path.parent))\n                    logger.debug(f\"Added model path to sys.path [model_path={model_path.parent}].\")\n                    spec = importlib.util.spec_from_file_location(model_cls_name, model_path)\n                    logger.debug(f\"Loaded spec from file location [spec={spec}].\")\n                    module = importlib.util.module_from_spec(spec)\n                    logger.debug(f\"Loaded module from spec [module={module}].\")\n                    spec.loader.exec_module(module)\n                    logger.debug(f\"Executed module [module={module}].\")\n                    model_cls = getattr(module, model_cls_name)\n                    logger.debug(f\"Loaded model class [model_cls={model_cls}].\")\n                except Exception as e:\n                    import traceback\n\n                    tback_str = traceback.format_exc()\n                    logger.error(\n                        f\"Failed to import model class, model_cls={model_cls_name}, model_path={model_path}, e={e}\\n\\n{tback_str}.\"\n                    )\n                    raise ValueError(\n                        f\"Invalid model class provided, model_cls={model_cls_name}, model_path={model_path}, e={e}\\n\\n{tback_str}.\"\n                    )\n\n                values.kwargs.update(model_cls=model_cls)\n                return values\n\n        # Check if the file exists and has a YAML extension\n        path = Path(filename)\n        logger.debug(f\"Loading deployment configuration from {path}\")\n        if not path.exists():\n            raise FileNotFoundError(f\"YAML file {path.absolute()} does not exist\")\n        if not (path.name.endswith(\".yaml\") or path.name.endswith(\".yml\")):\n            raise ValueError(f\"YAML file {path.absolute()} must have a .yaml or .yml extension\")\n\n        # Load the YAML file\n        with path.open(\"r\") as f:\n            data = yaml.safe_load(f)\n        if \"models\" not in data:\n            raise ValueError(\"Missing `models` specification in the YAML file\")\n\n        # Service the models\n        services: List[ModelServiceSpec] = []\n        for model_id, mconfig in data[\"models\"].items():\n            # Check if the model is already registered\n            logger.debug(f\"Checking if model is already registered [id={model_id}].\")\n            try:\n                spec: ModelSpec = cls.load_spec(model_id)\n                deployment: ModelDeploymentSpec = ModelDeploymentSpec(**mconfig.get(\"deployment\", {}))\n                logger.debug(f\"Model already registered [id={model_id}, spec={spec}, deployment={deployment}]\")\n                services.append(ModelServiceSpec(model=spec, deployment=deployment))\n                logger.debug(f\"Registered service [id={model_id}, svc={services[-1]}]\")\n                continue\n            except KeyError as e:\n                logger.error(f\"Failed to load model spec, model_id={model_id}, e={e}\")\n\n            # If the model_id is not previously registered, register it\n            # Add the model id to the config\n            mconfig.update({\"id\": model_id})\n\n            # Generate the model spec from the config\n            try:\n                mconfig = _ModelImportConfig(**mconfig)\n            except (ValidationError, PydanticUserError) as e:\n                raise ValueError(f\"Invalid model config provided, filename={filename}, e={e}\")\n\n            # Register the model as a custom model\n            spec = ModelSpec.from_cls(\n                mconfig.model_cls,\n                method=mconfig.default_method,\n                init_args=mconfig.init_args,\n                init_kwargs=mconfig.init_kwargs,\n                model_id=mconfig.id,\n            )\n            cls.register_spec(spec, task=TaskType.CUSTOM, resources=mconfig.deployment.resources)\n            services.append(ModelServiceSpec(model=spec, deployment=mconfig.deployment))\n            logger.debug(f\"Registered service [id={model_id}, svc={services[-1]}]\")\n        return services\n\n    @classmethod\n    def register_from_catalog(cls):\n        \"\"\"Register models from the catalog.\n\n        The current workflow for registering models is as follows:\n         - Load all .yaml files from the environment variable `NOS_HUB_CATALOG_PATH`\n         - Register models from each of the catalog files (via `register_from_yaml`)\n\n            `register_from_catalog` -&gt; `register_from_yaml` -&gt; `register`\n\n        Raises:\n            TypeError: If `NOS_HUB_CATALOG_PATH` is not a string.\n            FileNotFoundError: If a specified catalog file does not exist.\n        \"\"\"\n        import os\n\n        warn_msg = \"register_from_catalog will be deprecated soon, use register_from_yaml instead.\"\n        logger.warning(warn_msg)\n\n        NOS_HUB_CATALOG_PATH = os.getenv(\"NOS_HUB_CATALOG_PATH\", \"\")\n        if not isinstance(NOS_HUB_CATALOG_PATH, str):\n            raise TypeError(f\"NOS_HUB_CATALOG_PATH must be a string, got {type(NOS_HUB_CATALOG_PATH)}\")\n\n        logger.debug(\"Loading hub models from catalog.\")\n        paths: List[Path] = [\n            Path(filename)\n            for filename in NOS_HUB_CATALOG_PATH.split(\":\")\n            if filename.endswith(\".yaml\") or filename.endswith(\".yml\")\n        ]\n        logger.debug(f\"Found {len(paths)} catalog files.\")\n\n        specs = []\n        for path in paths:\n            logger.debug(f\"Loading catalog file {path}.\")\n            if not path.exists():\n                raise FileNotFoundError(f\"Catalog file {path} does not exist.\")\n            _specs = cls.register_from_yaml(str(path))\n            specs.extend(_specs)\n        logger.debug(f\"Registered {len(specs)} models from catalog.\")\n</code></pre>"},{"location":"docs/api/hub.html#nos.hub.Hub.get","title":"get  <code>classmethod</code>","text":"<pre><code>get() -&gt; Hub\n</code></pre> <p>Get the singleton instance.</p> <p>Returns:</p> <ul> <li> <code>Hub</code> (              <code>Hub</code> )          \u2013            <p>Singleton instance.</p> </li> </ul> Source code in <code>nos/hub/__init__.py</code> <pre><code>@classmethod\ndef get(cls: \"Hub\") -&gt; \"Hub\":\n    \"\"\"Get the singleton instance.\n\n    Returns:\n        Hub: Singleton instance.\n    \"\"\"\n    if cls._instance is None:\n        cls._instance = cls()\n        # Register models / Populate the registry\n        import nos.models  # noqa: F401, E402\n    return cls._instance\n</code></pre>"},{"location":"docs/api/hub.html#nos.hub.Hub.list","title":"list  <code>classmethod</code>","text":"<pre><code>list(private: bool = False) -&gt; List[str]\n</code></pre> <p>List models in the registry.</p> <p>Parameters:</p> <ul> <li> <code>private</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to include private models.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[str]</code>           \u2013            <p>List[str]: List of model names.</p> </li> </ul> Source code in <code>nos/hub/__init__.py</code> <pre><code>@classmethod\ndef list(cls, private: bool = False) -&gt; List[str]:\n    \"\"\"List models in the registry.\n\n    Args:\n        private (bool): Whether to include private models.\n\n    Returns:\n        List[str]: List of model names.\n    \"\"\"\n    if private:\n        raise NotImplementedError(\"Private models not supported.\")\n    return [k for k in cls.get()._registry.keys()]\n</code></pre>"},{"location":"docs/api/hub.html#nos.hub.Hub.load_spec","title":"load_spec  <code>classmethod</code>","text":"<pre><code>load_spec(model_id: str) -&gt; ModelSpec\n</code></pre> <p>Load model spec from the registry.</p> <p>Parameters:</p> <ul> <li> <code>model_id</code>               (<code>str</code>)           \u2013            <p>Model identifier (e.g. <code>openai/clip-vit-base-patch32</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ModelSpec</code> (              <code>ModelSpec</code> )          \u2013            <p>Model specification.</p> </li> </ul> Source code in <code>nos/hub/__init__.py</code> <pre><code>@classmethod\ndef load_spec(cls, model_id: str) -&gt; ModelSpec:\n    \"\"\"Load model spec from the registry.\n\n    Args:\n        model_id (str): Model identifier (e.g. `openai/clip-vit-base-patch32`).\n\n    Returns:\n        ModelSpec: Model specification.\n    \"\"\"\n    try:\n        return cls.get()._registry[model_id]\n    except KeyError:\n        raise KeyError(f\"Unavailable model (name={model_id}).\")\n</code></pre>"},{"location":"docs/api/hub.html#nos.hub.Hub.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(model_id: str) -&gt; Any\n</code></pre> <p>Instantiate model from the registry.</p> <p>Parameters:</p> <ul> <li> <code>model_id</code>               (<code>str</code>)           \u2013            <p>Model identifier (e.g. <code>openai/clip-vit-base-patch32</code>).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code> (              <code>Any</code> )          \u2013            <p>Instantiated model.</p> </li> </ul> Source code in <code>nos/hub/__init__.py</code> <pre><code>@classmethod\ndef load(cls, model_id: str) -&gt; Any:\n    \"\"\"Instantiate model from the registry.\n\n    Args:\n        model_id (str): Model identifier (e.g. `openai/clip-vit-base-patch32`).\n\n    Returns:\n        Any: Instantiated model.\n    \"\"\"\n    spec: ModelSpec = cls.load_spec(model_id)\n    # Note (spillai): Loading the default signature here is OK\n    # since all the signatures have the same `func_or_cls`.\n    sig: FunctionSignature = spec.default_signature\n    return sig.func_or_cls(*sig.init_args, **sig.init_kwargs)\n</code></pre>"},{"location":"docs/api/hub.html#nos.hub.Hub.register","title":"register  <code>classmethod</code>","text":"<pre><code>register(model_id: str, task: TaskType, func_or_cls: Callable, init_args: Tuple[Any] = (), init_kwargs: Dict[str, Any] = {}, method: str = '__call__', inputs: Dict[str, Any] = {}, outputs: Union[Any, Dict[str, Any], None] = None, resources: ModelResources = None, **kwargs) -&gt; ModelSpec\n</code></pre> <p>Model registry decorator.</p> <p>Parameters:</p> <ul> <li> <code>model_id</code>               (<code>str</code>)           \u2013            <p>Model identifier (e.g. <code>openai/clip-vit-base-patch32</code>).</p> </li> <li> <code>task</code>               (<code>TaskType</code>)           \u2013            <p>Task type (e.g. <code>TaskType.OBJECT_DETECTION_2D</code>).</p> </li> <li> <code>func_or_cls</code>               (<code>Type[Any]</code>)           \u2013            <p>Model function or class.</p> </li> <li> <code>**kwargs</code>           \u2013            <p>Additional keyword arguments.</p> </li> </ul> <p>Returns:     ModelSpec: Model specification.</p> Source code in <code>nos/hub/__init__.py</code> <pre><code>@classmethod\ndef register(\n    cls,\n    model_id: str,\n    task: TaskType,\n    func_or_cls: Callable,\n    init_args: Tuple[Any] = (),\n    init_kwargs: Dict[str, Any] = {},  # noqa: B006\n    method: str = \"__call__\",\n    inputs: Dict[str, Any] = {},  # noqa: B006\n    outputs: Union[Any, Dict[str, Any], None] = None,  # noqa: B006\n    resources: ModelResources = None,\n    **kwargs,\n) -&gt; ModelSpec:\n    \"\"\"Model registry decorator.\n\n    Args:\n        model_id (str): Model identifier (e.g. `openai/clip-vit-base-patch32`).\n        task (TaskType): Task type (e.g. `TaskType.OBJECT_DETECTION_2D`).\n        func_or_cls (Type[Any]): Model function or class.\n        **kwargs: Additional keyword arguments.\n    Returns:\n        ModelSpec: Model specification.\n    \"\"\"\n    logger.debug(\n        f\"\"\"Registering model [model={model_id}, task={task}, func_or_cls={func_or_cls}, \"\"\"\n        f\"\"\"inputs={inputs}, outputs={outputs}, \"\"\"\n        f\"\"\"init_args={init_args}, init_kwargs={init_kwargs}, method={method}]\"\"\"\n    )\n\n    # Create signature\n    signature: Dict[str, FunctionSignature] = {\n        method: FunctionSignature(\n            func_or_cls,\n            method=method,\n            init_args=init_args,\n            init_kwargs=init_kwargs,\n            input_annotations=inputs,\n            output_annotations=outputs,\n        ),\n    }\n\n    # Get hub instance\n    hub = cls.get()\n\n    # Add metadata for the model\n    catalog = ModelSpecMetadataCatalog.get()\n    spec = ModelSpec(model_id, signature=signature)\n    logger.debug(f\"Created model spec [id={model_id}, spec={spec}]\")\n\n    # Add task metadata for the model\n    if task is not None:\n        metadata = ModelSpecMetadata(model_id, method, task)\n        catalog._metadata_catalog[f\"{model_id}/{method}\"] = metadata\n\n    # Add model resources\n    if resources is not None:\n        catalog._resources_catalog[f\"{model_id}/{method}\"] = resources\n\n    # Register model id to model spec registry\n    if model_id not in hub._registry:\n        hub._registry[model_id] = spec\n        logger.debug(f\"Registered model to hub registry [id={model_id}, spec={spec}]\")\n\n    # Add another signature if the model is already registered\n    else:\n        _spec = hub._registry[model_id]\n        if method not in _spec.signature:\n            logger.debug(\n                f\"Adding task signature [model={model_id}, task={task}, method={method}, sig={spec.signature}]\"\n            )\n            _spec.signature[method] = spec.signature[method]\n            catalog._metadata_catalog[f\"{model_id}/{method}\"] = spec.metadata(method)\n        else:\n            logger.debug(\n                f\"Task signature already registered [model={model_id}, task={task}, method={method}, sig={spec.signature}]\"\n            )\n\n    logger.debug(f\"Registered model [id={model_id}, spec={spec}]\")\n    return spec\n</code></pre>"},{"location":"docs/api/hub.html#nos.hub.Hub.register_spec","title":"register_spec  <code>classmethod</code>","text":"<pre><code>register_spec(spec: ModelSpec, task: TaskType = None, resources: ModelResources = None) -&gt; ModelSpec\n</code></pre> <p>Register model spec to the registry.</p> <p>Parameters:</p> <ul> <li> <code>spec</code>               (<code>ModelSpec</code>)           \u2013            <p>Model specification.</p> </li> <li> <code>task</code>               (<code>TaskType</code>, default:                   <code>None</code> )           \u2013            <p>Task type (e.g. <code>TaskType.OBJECT_DETECTION_2D</code>).</p> </li> <li> <code>resources</code>               (<code>ModelResources</code>, default:                   <code>None</code> )           \u2013            <p>Model resources.</p> </li> </ul> <p>Returns:     ModelSpec: Model specification.</p> Source code in <code>nos/hub/__init__.py</code> <pre><code>@classmethod\ndef register_spec(cls, spec: ModelSpec, task: TaskType = None, resources: ModelResources = None) -&gt; ModelSpec:\n    \"\"\"Register model spec to the registry.\n\n    Args:\n        spec (ModelSpec): Model specification.\n        task (TaskType): Task type (e.g. `TaskType.OBJECT_DETECTION_2D`).\n        resources (ModelResources): Model resources.\n    Returns:\n        ModelSpec: Model specification.\n    \"\"\"\n    logger.debug(f\"Registering model spec [id={spec.id}, spec={spec}]\")\n\n    # Get hub instance\n    hub = cls.get()\n\n    # Add metadata for the model\n    catalog = ModelSpecMetadataCatalog.get()\n\n    # Add task metadata for the model\n    if task is not None:\n        metadata = ModelSpecMetadata(spec.id, spec.default_method, task)\n        catalog._metadata_catalog[f\"{spec.id}/{spec.default_method}\"] = metadata\n\n    # Add model resources\n    if resources is not None:\n        catalog._resources_catalog[f\"{spec.id}/{spec.default_method}\"] = resources\n\n    # Register model id to model spec registry\n    if spec.id not in hub._registry:\n        hub._registry[spec.id] = spec\n        logger.debug(f\"Registered model spec [id={spec.id}, spec={spec}]\")\n    return spec\n</code></pre>"},{"location":"docs/api/hub.html#nos.hub.Hub.register_from_yaml","title":"register_from_yaml  <code>classmethod</code>","text":"<pre><code>register_from_yaml(filename: str) -&gt; List[Any]\n</code></pre> <p>Register models from a catalog YAML.</p> <p>Parameters:</p> <ul> <li> <code>filename</code>               (<code>str</code>)           \u2013            <p>Path to the deployment config YAML file.</p> </li> </ul> <p>Returns:     List[ModelSpec]: List of model specifications.</p> Source code in <code>nos/hub/__init__.py</code> <pre><code>@classmethod\ndef register_from_yaml(cls, filename: str) -&gt; List[Any]:\n    \"\"\"Register models from a catalog YAML.\n\n    Args:\n        filename (str): Path to the deployment config YAML file.\n    Returns:\n        List[ModelSpec]: List of model specifications.\n    \"\"\"\n\n    @dataclass\n    class _ModelImportConfig:\n        \"\"\"Model import configuration.\"\"\"\n\n        id: str\n        \"\"\"Model identifier.\"\"\"\n        runtime_env: str\n        \"\"\"Runtime environment.\"\"\"\n        model_path: str\n        \"\"\"Model path.\"\"\"\n        model_cls: Callable\n        \"\"\"Model class name.\"\"\"\n        default_method: str\n        \"\"\"Default model method name.\"\"\"\n        init_args: Tuple[Any, ...] = field(default_factory=tuple)\n        \"\"\"Arguments to initialize the model instance.\"\"\"\n        init_kwargs: Dict[str, Any] = field(default_factory=dict)\n        \"\"\"Keyword arguments to initialize the model instance.\"\"\"\n        deployment: ModelDeploymentSpec = field(default_factory=ModelDeploymentSpec)\n        \"\"\"Model deployment specification.\"\"\"\n\n        @model_validator(mode=\"before\")\n        @classmethod\n        def _validate_model_cls_import(cls, values):\n            \"\"\"Validate the model.\"\"\"\n            import importlib\n            import importlib.util\n            import sys\n\n            model_cls_name = values.kwargs.get(\"model_cls\")\n            model_path = values.kwargs.get(\"model_path\")\n\n            # Check if model_path is a valid path relative to the directory containing\n            # the catalog.yaml file.\n            model_path = Path(filename).parent / model_path\n            if not model_path.exists():\n                logger.error(f\"Invalid model path provided, model_path={model_path}.\")\n                raise FileNotFoundError(f\"Invalid model path provided, model_path={model_path}.\")\n\n            # Check if the model_cls is importable from the model_path\n            try:\n                # Load `model_cls` from the `model_path`\n                logger.debug(f\"Loading model class [model_cls={model_cls_name}, model_path={model_path}].\")\n                sys.path.append(str(model_path.parent))\n                logger.debug(f\"Added model path to sys.path [model_path={model_path.parent}].\")\n                spec = importlib.util.spec_from_file_location(model_cls_name, model_path)\n                logger.debug(f\"Loaded spec from file location [spec={spec}].\")\n                module = importlib.util.module_from_spec(spec)\n                logger.debug(f\"Loaded module from spec [module={module}].\")\n                spec.loader.exec_module(module)\n                logger.debug(f\"Executed module [module={module}].\")\n                model_cls = getattr(module, model_cls_name)\n                logger.debug(f\"Loaded model class [model_cls={model_cls}].\")\n            except Exception as e:\n                import traceback\n\n                tback_str = traceback.format_exc()\n                logger.error(\n                    f\"Failed to import model class, model_cls={model_cls_name}, model_path={model_path}, e={e}\\n\\n{tback_str}.\"\n                )\n                raise ValueError(\n                    f\"Invalid model class provided, model_cls={model_cls_name}, model_path={model_path}, e={e}\\n\\n{tback_str}.\"\n                )\n\n            values.kwargs.update(model_cls=model_cls)\n            return values\n\n    # Check if the file exists and has a YAML extension\n    path = Path(filename)\n    logger.debug(f\"Loading deployment configuration from {path}\")\n    if not path.exists():\n        raise FileNotFoundError(f\"YAML file {path.absolute()} does not exist\")\n    if not (path.name.endswith(\".yaml\") or path.name.endswith(\".yml\")):\n        raise ValueError(f\"YAML file {path.absolute()} must have a .yaml or .yml extension\")\n\n    # Load the YAML file\n    with path.open(\"r\") as f:\n        data = yaml.safe_load(f)\n    if \"models\" not in data:\n        raise ValueError(\"Missing `models` specification in the YAML file\")\n\n    # Service the models\n    services: List[ModelServiceSpec] = []\n    for model_id, mconfig in data[\"models\"].items():\n        # Check if the model is already registered\n        logger.debug(f\"Checking if model is already registered [id={model_id}].\")\n        try:\n            spec: ModelSpec = cls.load_spec(model_id)\n            deployment: ModelDeploymentSpec = ModelDeploymentSpec(**mconfig.get(\"deployment\", {}))\n            logger.debug(f\"Model already registered [id={model_id}, spec={spec}, deployment={deployment}]\")\n            services.append(ModelServiceSpec(model=spec, deployment=deployment))\n            logger.debug(f\"Registered service [id={model_id}, svc={services[-1]}]\")\n            continue\n        except KeyError as e:\n            logger.error(f\"Failed to load model spec, model_id={model_id}, e={e}\")\n\n        # If the model_id is not previously registered, register it\n        # Add the model id to the config\n        mconfig.update({\"id\": model_id})\n\n        # Generate the model spec from the config\n        try:\n            mconfig = _ModelImportConfig(**mconfig)\n        except (ValidationError, PydanticUserError) as e:\n            raise ValueError(f\"Invalid model config provided, filename={filename}, e={e}\")\n\n        # Register the model as a custom model\n        spec = ModelSpec.from_cls(\n            mconfig.model_cls,\n            method=mconfig.default_method,\n            init_args=mconfig.init_args,\n            init_kwargs=mconfig.init_kwargs,\n            model_id=mconfig.id,\n        )\n        cls.register_spec(spec, task=TaskType.CUSTOM, resources=mconfig.deployment.resources)\n        services.append(ModelServiceSpec(model=spec, deployment=mconfig.deployment))\n        logger.debug(f\"Registered service [id={model_id}, svc={services[-1]}]\")\n    return services\n</code></pre>"},{"location":"docs/api/hub.html#nos.hub.Hub.register_from_catalog","title":"register_from_catalog  <code>classmethod</code>","text":"<pre><code>register_from_catalog()\n</code></pre> <p>Register models from the catalog.</p> The current workflow for registering models is as follows <ul> <li>Load all .yaml files from the environment variable <code>NOS_HUB_CATALOG_PATH</code></li> <li>Register models from each of the catalog files (via <code>register_from_yaml</code>)</li> </ul> <p><code>register_from_catalog</code> -&gt; <code>register_from_yaml</code> -&gt; <code>register</code></p> <p>Raises:</p> <ul> <li> <code>TypeError</code>             \u2013            <p>If <code>NOS_HUB_CATALOG_PATH</code> is not a string.</p> </li> <li> <code>FileNotFoundError</code>             \u2013            <p>If a specified catalog file does not exist.</p> </li> </ul> Source code in <code>nos/hub/__init__.py</code> <pre><code>@classmethod\ndef register_from_catalog(cls):\n    \"\"\"Register models from the catalog.\n\n    The current workflow for registering models is as follows:\n     - Load all .yaml files from the environment variable `NOS_HUB_CATALOG_PATH`\n     - Register models from each of the catalog files (via `register_from_yaml`)\n\n        `register_from_catalog` -&gt; `register_from_yaml` -&gt; `register`\n\n    Raises:\n        TypeError: If `NOS_HUB_CATALOG_PATH` is not a string.\n        FileNotFoundError: If a specified catalog file does not exist.\n    \"\"\"\n    import os\n\n    warn_msg = \"register_from_catalog will be deprecated soon, use register_from_yaml instead.\"\n    logger.warning(warn_msg)\n\n    NOS_HUB_CATALOG_PATH = os.getenv(\"NOS_HUB_CATALOG_PATH\", \"\")\n    if not isinstance(NOS_HUB_CATALOG_PATH, str):\n        raise TypeError(f\"NOS_HUB_CATALOG_PATH must be a string, got {type(NOS_HUB_CATALOG_PATH)}\")\n\n    logger.debug(\"Loading hub models from catalog.\")\n    paths: List[Path] = [\n        Path(filename)\n        for filename in NOS_HUB_CATALOG_PATH.split(\":\")\n        if filename.endswith(\".yaml\") or filename.endswith(\".yml\")\n    ]\n    logger.debug(f\"Found {len(paths)} catalog files.\")\n\n    specs = []\n    for path in paths:\n        logger.debug(f\"Loading catalog file {path}.\")\n        if not path.exists():\n            raise FileNotFoundError(f\"Catalog file {path} does not exist.\")\n        _specs = cls.register_from_yaml(str(path))\n        specs.extend(_specs)\n    logger.debug(f\"Registered {len(specs)} models from catalog.\")\n</code></pre>"},{"location":"docs/api/hub.html#nos.hub.config.NosHubConfig","title":"nos.hub.config.NosHubConfig  <code>dataclass</code>","text":"<p>NOS Hub configuration.</p> Source code in <code>nos/hub/config.py</code> <pre><code>@dataclass(frozen=True)\nclass NosHubConfig:\n    \"\"\"NOS Hub configuration.\"\"\"\n\n    namespace: str\n    \"\"\"Namespace (repository, organization).\"\"\"\n    name: str\n    \"\"\"Model name.\"\"\"\n</code></pre>"},{"location":"docs/api/hub.html#nos.hub.config.NosHubConfig.namespace","title":"namespace  <code>instance-attribute</code>","text":"<pre><code>namespace: str\n</code></pre> <p>Namespace (repository, organization).</p>"},{"location":"docs/api/hub.html#nos.hub.config.NosHubConfig.name","title":"name  <code>instance-attribute</code>","text":"<pre><code>name: str\n</code></pre> <p>Model name.</p>"},{"location":"docs/api/hub.html#nos.hub.config.TorchHubConfig","title":"nos.hub.config.TorchHubConfig  <code>dataclass</code>","text":"<p>PyTorch Hub configuration.</p> Source code in <code>nos/hub/config.py</code> <pre><code>@dataclass(frozen=True)\nclass TorchHubConfig:\n    \"\"\"PyTorch Hub configuration.\"\"\"\n\n    repo: str\n    \"\"\"Repository name (e.g. pytorch/vision).\"\"\"\n    model_name: str\n    \"\"\"Model name (e.g. resnet18).\"\"\"\n    checkpoint: str = None\n    \"\"\"Checkpoint name (e.g. resnet18-5c106cde.pth).\"\"\"\n</code></pre>"},{"location":"docs/api/hub.html#nos.hub.config.TorchHubConfig.repo","title":"repo  <code>instance-attribute</code>","text":"<pre><code>repo: str\n</code></pre> <p>Repository name (e.g. pytorch/vision).</p>"},{"location":"docs/api/hub.html#nos.hub.config.TorchHubConfig.model_name","title":"model_name  <code>instance-attribute</code>","text":"<pre><code>model_name: str\n</code></pre> <p>Model name (e.g. resnet18).</p>"},{"location":"docs/api/hub.html#nos.hub.config.TorchHubConfig.checkpoint","title":"checkpoint  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>checkpoint: str = None\n</code></pre> <p>Checkpoint name (e.g. resnet18-5c106cde.pth).</p>"},{"location":"docs/api/hub.html#nos.hub.config.HuggingFaceHubConfig","title":"nos.hub.config.HuggingFaceHubConfig  <code>dataclass</code>","text":"<p>HuggingFace Hub configuration.</p> Source code in <code>nos/hub/config.py</code> <pre><code>@dataclass(frozen=True)\nclass HuggingFaceHubConfig:\n    \"\"\"HuggingFace Hub configuration.\"\"\"\n\n    model_name: str\n    \"\"\"Model name (e.g. bert-base-uncased).\"\"\"\n    checkpoint: str = None\n    \"\"\"Checkpoint name (e.g. bert-base-uncased-pytorch_model.bin).\"\"\"\n</code></pre>"},{"location":"docs/api/hub.html#nos.hub.config.HuggingFaceHubConfig.model_name","title":"model_name  <code>instance-attribute</code>","text":"<pre><code>model_name: str\n</code></pre> <p>Model name (e.g. bert-base-uncased).</p>"},{"location":"docs/api/hub.html#nos.hub.config.HuggingFaceHubConfig.checkpoint","title":"checkpoint  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>checkpoint: str = None\n</code></pre> <p>Checkpoint name (e.g. bert-base-uncased-pytorch_model.bin).</p>"},{"location":"docs/api/managers.html","title":"Managers","text":""},{"location":"docs/api/managers.html#nos.managers.model.ModelHandle","title":"nos.managers.model.ModelHandle  <code>dataclass</code>","text":"<p>Model handles for distributed model execution.</p> Usage <pre><code># Initialize a model handle\n&gt;&gt; model = ModelHandle(spec, num_replicas=1)\n\n# Call the task immediately\n&gt;&gt; response = model(**model_inputs)\n\n# Call a method on the model handle\n&gt;&gt; response = model.process_images(**model_inputs)\n\n# Submit a task to the model handle,\n# this will add results to the queue\n&gt;&gt; model.submit(**model_inputs)\n# Fetch the next result from the queue\n&gt;&gt; response = model.get()\n\n# Submit a task to a specific model handle method\n&gt;&gt; model.submit(**model_inputs, _method=\"process_images\")\n\n# Submit a task to the model handle,\n# this will add results to the queue\n&gt;&gt; model_handle.submit(**model_inputs)\n# Fetch the next result from the queue\n&gt;&gt; response = model_handle.get()\n\n# Cleanup model resources\n&gt;&gt; model_handle.cleanup()\n</code></pre> Source code in <code>nos/managers/model.py</code> <pre><code>@dataclass\nclass ModelHandle:\n    \"\"\"Model handles for distributed model execution.\n\n    Usage:\n        ```python\n        # Initialize a model handle\n        &gt;&gt; model = ModelHandle(spec, num_replicas=1)\n\n        # Call the task immediately\n        &gt;&gt; response = model(**model_inputs)\n\n        # Call a method on the model handle\n        &gt;&gt; response = model.process_images(**model_inputs)\n\n        # Submit a task to the model handle,\n        # this will add results to the queue\n        &gt;&gt; model.submit(**model_inputs)\n        # Fetch the next result from the queue\n        &gt;&gt; response = model.get()\n\n        # Submit a task to a specific model handle method\n        &gt;&gt; model.submit(**model_inputs, _method=\"process_images\")\n\n        # Submit a task to the model handle,\n        # this will add results to the queue\n        &gt;&gt; model_handle.submit(**model_inputs)\n        # Fetch the next result from the queue\n        &gt;&gt; response = model_handle.get()\n\n        # Cleanup model resources\n        &gt;&gt; model_handle.cleanup()\n        ```\n    \"\"\"\n\n    spec: ModelSpec\n    \"\"\"Model specification.\"\"\"\n    deployment: ModelDeploymentSpec = field(default_factory=ModelDeploymentSpec)\n    \"\"\"Number of replicas.\"\"\"\n    _actors: List[Union[ray.remote, ray.actor.ActorHandle]] = field(init=False, default=None)\n    \"\"\"Ray actor handle.\"\"\"\n    _actor_pool: ActorPool = field(init=False, default=None)\n    \"\"\"Ray actor pool.\"\"\"\n    _actor_options: Dict[str, Any] = field(init=False, default=None)\n    \"\"\"Ray actor options.\"\"\"\n\n    def __post_init__(self):\n        \"\"\"Initialize the actor handles.\"\"\"\n        self._actor_options = self._get_actor_options(self.spec, self.deployment)\n        self._actors = [self._get_actor() for _ in range(self.deployment.num_replicas)]\n        self._actor_pool = ActorPool(self._actors)\n\n        # Patch the model handle with methods from the model spec signature\n        for method in self.spec.signature:\n            # Note (spillai): We do not need to patch the __call__ method\n            # since it is already re-directed in the model handle.\n            if hasattr(self, method):\n                logger.debug(f\"Model handle ({self}) already has method ({method}), skipping ....\")\n                continue\n\n            # Methods:\n            #   &gt;&gt; handle.process_images: ModelHandlePartial\n            #   &gt;&gt; handle.process_images(images=...) =&gt; handle.__call__(images=..., _method=\"process_images\")\n            #   &gt;&gt; handle.process_images.submit(images=...) =&gt; handle.submit(images=..., _method=\"process_images\")\n            setattr(self, method, ModelHandlePartial(self, method))\n\n    def __repr__(self) -&gt; str:\n        assert len(self._actors) == self.num_replicas\n        opts_str = \", \".join([f\"{k}={v}\" for k, v in self._actor_options.items()])\n        return f\"ModelHandle(name={self.spec.name}, replicas={len(self._actors)}, opts=({opts_str}))\"\n\n    @property\n    def num_replicas(self) -&gt; int:\n        \"\"\"Get the number of replicas.\"\"\"\n        return self.deployment.num_replicas\n\n    @classmethod\n    def _get_actor_options(cls, spec: ModelSpec, deployment: ModelDeploymentSpec) -&gt; Dict[str, Any]:\n        \"\"\"Get actor options from model specification.\"\"\"\n        # TOFIX (spillai): When considering CPU-only models with num_cpus specified,\n        # OMP_NUM_THREADS will be set to the number of CPUs requested. Otherwise,\n        # if num_cpus is not specified, OMP_NUM_THREADS will default to 1.\n        # Instead, for now, we manually set the environment variable in `InferenceServiceRuntime`\n        # to the number of CPUs threads available.\n\n        # If deployment resources are not specified, get the model resources from the catalog\n        if deployment.resources is None:\n            try:\n                catalog = ModelSpecMetadataCatalog.get()\n                resources: ModelResources = catalog._resources_catalog[f\"{spec.id}/{spec.default_method}\"]\n            except Exception:\n                resources = ModelResources()\n                logger.debug(f\"Failed to get model resources [model={spec.id}, method={spec.default_method}]\")\n\n        # Otherwise, use the deployment resources provided\n        else:\n            resources = deployment.resources\n\n        # For GPU models, we need to set the number of fractional GPUs to use\n        if (resources.device == \"auto\" or resources.device == \"gpu\") and torch.cuda.is_available():\n            try:\n                # TODO (spillai): This needs to be resolved differently for\n                # multi-node clusters.\n                # Determine the current device id by checking the number of GPUs used.\n                total, available = ray.cluster_resources(), ray.available_resources()\n                gpus_used = total[\"GPU\"] - available[\"GPU\"]\n                device_id = int(gpus_used)\n\n                if isinstance(resources.device_memory, str) and resources.device_memory == \"auto\":\n                    gpu_frac = 1.0 / NOS_MAX_CONCURRENT_MODELS\n                    actor_opts = {\"num_gpus\": gpu_frac}\n                elif isinstance(resources.device_memory, int):\n                    # Fractional GPU memory needed within the current device\n                    device_memory = torch.cuda.get_device_properties(device_id).total_memory\n                    gpu_frac = float(resources.device_memory) / device_memory\n                    gpu_frac = round(gpu_frac * 10) / 10.0\n\n                    # Fractional GPU used for the current device\n                    gpu_frac_used = gpus_used - int(gpus_used)\n                    gpu_frac_avail = (1 - gpu_frac_used) * device_memory\n                    logger.debug(\n                        f\"\"\"actor_opts [model={spec.id}, \"\"\"\n                        f\"\"\"mem={humanize.naturalsize(resources.device_memory, binary=True)}, device={device_id}, device_mem={humanize.naturalsize(device_memory, binary=True)}, \"\"\"\n                        f\"\"\"gpu_frac={gpu_frac}, gpu_frac_avail={gpu_frac_avail}, \"\"\"\n                        f\"\"\"gpu_frac_used={gpu_frac_used}]\"\"\"\n                    )\n                    if gpu_frac &gt; gpu_frac_avail:\n                        logger.debug(\n                            f\"Insufficient GPU memory for model [model={spec.id}, \"\n                            f\"method={spec.default_method}, gpu_frac={gpu_frac}, \"\n                            f\"gpu_frac_avail={gpu_frac_avail}, gpu_frac_used={gpu_frac_used}]\"\n                        )\n                        if device_id == torch.cuda.device_count() - 1:\n                            # TOFIX (spillai): evict models to make space for the current model\n                            logger.debug(\"All GPUs are fully utilized, this may result in undesirable behavior.\")\n                    actor_opts = {\"num_gpus\": gpu_frac}\n                else:\n                    raise ValueError(f\"Invalid device memory: {resources.device_memory}\")\n            except Exception as exc:\n                logger.debug(f\"Failed to get GPU memory [e={exc}].\")\n                actor_opts = {\"num_gpus\": 1.0 / NOS_MAX_CONCURRENT_MODELS}\n\n        elif resources.device == \"cpu\":\n            actor_opts = {\"num_cpus\": resources.cpus, \"memory\": resources.memory}\n\n        else:\n            actor_opts = {\"num_cpus\": resources.cpus, \"memory\": resources.memory}\n\n        if spec.runtime_env is not None:\n            logger.debug(\"Using custom runtime environment, this may take a while to build.\")\n            actor_opts[\"runtime_env\"] = RuntimeEnv(**spec.runtime_env.model_dump())\n        logger.debug(f\"Actor options [id={spec.id}, opts={actor_opts}]\")\n\n        return actor_opts\n\n    def _get_actor(self) -&gt; Union[ray.remote, ray.actor.ActorHandle]:\n        \"\"\"Get an actor handle from model specification.\n\n        Returns:\n            Union[ray.remote, ray.actor.ActorHandle]: Ray actor handle.\n        \"\"\"\n        # TODO (spillai): Use the auto-tuned model spec to instantiate an\n        # actor the desired memory requirements. Fractional GPU amounts\n        # will need to be calculated from the target HW and model spec\n        # (i.e. 0.5 on A100 vs. T4 are different).\n        # NOTE (spillai): Using default signature here is OK, since\n        # all the signatures for a model spec have the same `func_or_cls`.\n        model_cls = self.spec.default_signature.func_or_cls\n\n        # Get the actor options from the model spec\n        actor_options = self._actor_options\n        actor_cls = ray.remote(**actor_options)(model_cls)\n\n        # Check if the model class has the required method\n        logger.debug(\n            f\"Creating actor [actor={actor_cls}, opts={actor_options}, cls={model_cls}, init_args={self.spec.default_signature.init_args}, init_kwargs={self.spec.default_signature.init_kwargs}]\"\n        )\n        actor = actor_cls.remote(*self.spec.default_signature.init_args, **self.spec.default_signature.init_kwargs)\n\n        # Note: Only check if default signature method is implemented\n        # even though other methods may be implemented and used.\n        if not hasattr(actor, self.spec.default_method):\n            raise NotImplementedError(f\"Model class {model_cls} does not have {self.spec.default_method} implemented.\")\n        logger.debug(f\"Creating actor [actor={actor}, opts={actor_options}, cls={model_cls}]\")\n\n        # Add some memory logs to this actor\n        if NOS_MEMRAY_ENABLED:\n            # Replace all non-alphanumeric characters with underscores\n            actor_name = re.sub(r\"\\W+\", \"_\", str(actor))\n            log_name = Path(NOS_RAY_LOGS_DIR) / f\"{actor_name}_mem_profile.bin\"\n            if log_name.exists():\n                log_name.unlink()\n            try:\n                memray.Tracker(log_name).__enter__()\n            except Exception:\n                logger.error(\"Failed to iniitialize memray tracker.\")\n        return actor\n\n    def __call__(self, *args: Any, **kwargs: Any) -&gt; Any:\n        \"\"\"Call the task immediately.\n\n        Args:\n            *args: Model arguments.\n            **kwargs: Model keyword arguments\n                (except for special `_method` keyword that is\n                used to call different class methods).\n        Returns:\n            Model response.\n        \"\"\"\n        assert len(self._actors) &gt;= 1, \"Model should have atleast one replica.\"\n        if self.num_replicas &gt; 1:\n            logger.warning(\"Model has &gt;1 replicas, use `.submit()` instead to fully utilize them.\")\n\n        method: str = kwargs.pop(\"_method\", self.spec.default_method)\n        # TODO (spillai): We should be able to determine if the output\n        # is an iterable or not from the signature, and set the default\n        stream: bool = kwargs.pop(\"_stream\", False)\n        actor_method_func = getattr(self._actors[0], method)\n        if not stream:\n            response_ref: ray.ObjectRef = actor_method_func.remote(**kwargs)\n            return ray.get(response_ref)\n        else:\n            response_refs: Iterable[ray.ObjectRef] = actor_method_func.options(num_returns=\"streaming\").remote(\n                **kwargs\n            )\n            return _StreamingModelHandleResponse(response_refs)\n\n    def scale(self, num_replicas: Union[int, str] = 1) -&gt; \"ModelHandle\":\n        \"\"\"Scale the model handle to a new number of replicas.\n\n        Args:\n            num_replicas (int or str): Number of replicas, or set to \"auto\" to\n                automatically scale the model to the number of GPUs available.\n        \"\"\"\n        if isinstance(num_replicas, str) and num_replicas == \"auto\":\n            raise NotImplementedError(\"Automatic scaling not implemented.\")\n        if not isinstance(num_replicas, int):\n            raise ValueError(f\"Invalid replicas: {num_replicas}\")\n\n        # Check if there are any pending futures\n        if self._actor_pool.has_next():\n            logger.warning(f\"Pending futures detected, this may result in dropped queue items [name={self.spec.name}]\")\n        logger.debug(f\"Waiting for pending futures to complete before scaling [name={self.spec.name}].\")\n        logger.debug(f\"Scaling model [name={self.spec.name}].\")\n\n        if num_replicas == len(self._actors):\n            logger.debug(f\"Model already scaled appropriately [name={self.spec.name}, replicas={num_replicas}].\")\n            return self\n        elif num_replicas &gt; len(self._actors):\n            self._actors += [self._get_actor() for _ in range(num_replicas - len(self._actors))]\n            logger.debug(f\"Scaling up model [name={self.spec.name}, replicas={num_replicas}].\")\n        else:\n            actors_to_remove = self._actors[num_replicas:]\n            for actor in actors_to_remove:\n                ray.kill(actor)\n            self._actors = self._actors[:num_replicas]\n\n            logger.debug(f\"Scaling down model [name={self.spec.name}, replicas={num_replicas}].\")\n\n        # Update repicas and queue size\n        self.deployment.num_replicas = num_replicas\n\n        # Re-create the actor pool\n        logger.debug(f\"Removing actor pool [replicas={len(self._actors)}].\")\n        del self._actor_pool\n        self._actor_pool = None\n\n        # Re-create the actor pool\n        logger.debug(f\"Re-creating actor pool [name={self.spec.name}, replicas={num_replicas}].\")\n        self._actor_pool = ActorPool(self._actors)\n        assert len(self._actors) == num_replicas, \"Model scaling failed.\"\n        gc.collect()\n        return self\n\n    def submit(self, *args: Any, **kwargs: Any) -&gt; ray.ObjectRef:\n        \"\"\"Submit a task to the actor pool.\n\n        Note (spillai): Caveats for `.submit()` with custom methods:\n            ModelHandles have a single result queue that add\n            results asynchronously on task completion. Calling `submit()`\n            with different methods interchangably will result in\n            the results queue being populated with results from\n            different methods. In other words, it is advised to\n            use `submit()` with the same method for a given model\n            and then use `get()` to fetch all the results, before\n            calling `submit()` with a different method.\n\n        Args:\n            *args: Model arguments.\n            **kwargs: Model keyword arguments\n                (except for special `_method` keyword that is\n                used to call different class methods).\n\n        Returns:\n            ray.ObjectRef: Ray object reference as a string.\n        \"\"\"\n        # Submit the task to the actor pool, leveraging all replicas\n        method: str = kwargs.pop(\"_method\", self.spec.default_method)\n        # TODO (spillai): We should be able to determine if the output\n        # is an iterable or not from the signature, and set the default\n        stream: bool = kwargs.pop(\"_stream\", False)\n        remote_opts = {\"num_returns\": \"streaming\"} if stream else {}\n        if not self._actor_pool._idle_actors:\n            logger.warning(f\"Actor pool is full, this may result in dropped queue items [name={self.spec.name}]\")\n        future_ref = self._actor_pool.submit(\n            lambda a, v: getattr(a, method).options(**remote_opts).remote(**v), kwargs\n        )\n        logger.info(f\"Submitted task [name={self.spec.name}, method={method}, kwargs={kwargs}]\")\n        return future_ref\n\n    def cleanup(self) -&gt; None:\n        \"\"\"Kill all the actor handles and garbage collect.\"\"\"\n        for actor_handle in self._actors:\n            ray.kill(actor_handle)\n        self._actors = []\n        gc.collect()\n\n    def get(self, future_ref: ray.ObjectRef = None, timeout: int = None) -&gt; Any:\n        \"\"\"Get the result future.\"\"\"\n        return self._actor_pool.get(future_ref)\n\n    async def async_get(self, future_ref: ray.ObjectRef = None, timeout: int = None) -&gt; Any:\n        \"\"\"Get the result future asynchronously.\"\"\"\n        return await self._actor_pool.async_get(future_ref)\n</code></pre>"},{"location":"docs/api/managers.html#nos.managers.model.ModelHandle.spec","title":"spec  <code>instance-attribute</code>","text":"<pre><code>spec: ModelSpec\n</code></pre> <p>Model specification.</p>"},{"location":"docs/api/managers.html#nos.managers.model.ModelHandle.deployment","title":"deployment  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>deployment: ModelDeploymentSpec = field(default_factory=ModelDeploymentSpec)\n</code></pre> <p>Number of replicas.</p>"},{"location":"docs/api/managers.html#nos.managers.model.ModelHandle.num_replicas","title":"num_replicas  <code>property</code>","text":"<pre><code>num_replicas: int\n</code></pre> <p>Get the number of replicas.</p>"},{"location":"docs/api/managers.html#nos.managers.model.ModelHandle.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Initialize the actor handles.</p> Source code in <code>nos/managers/model.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Initialize the actor handles.\"\"\"\n    self._actor_options = self._get_actor_options(self.spec, self.deployment)\n    self._actors = [self._get_actor() for _ in range(self.deployment.num_replicas)]\n    self._actor_pool = ActorPool(self._actors)\n\n    # Patch the model handle with methods from the model spec signature\n    for method in self.spec.signature:\n        # Note (spillai): We do not need to patch the __call__ method\n        # since it is already re-directed in the model handle.\n        if hasattr(self, method):\n            logger.debug(f\"Model handle ({self}) already has method ({method}), skipping ....\")\n            continue\n\n        # Methods:\n        #   &gt;&gt; handle.process_images: ModelHandlePartial\n        #   &gt;&gt; handle.process_images(images=...) =&gt; handle.__call__(images=..., _method=\"process_images\")\n        #   &gt;&gt; handle.process_images.submit(images=...) =&gt; handle.submit(images=..., _method=\"process_images\")\n        setattr(self, method, ModelHandlePartial(self, method))\n</code></pre>"},{"location":"docs/api/managers.html#nos.managers.model.ModelHandle.__call__","title":"__call__","text":"<pre><code>__call__(*args: Any, **kwargs: Any) -&gt; Any\n</code></pre> <p>Call the task immediately.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Model arguments.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Model keyword arguments (except for special <code>_method</code> keyword that is used to call different class methods).</p> </li> </ul> <p>Returns:     Model response.</p> Source code in <code>nos/managers/model.py</code> <pre><code>def __call__(self, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"Call the task immediately.\n\n    Args:\n        *args: Model arguments.\n        **kwargs: Model keyword arguments\n            (except for special `_method` keyword that is\n            used to call different class methods).\n    Returns:\n        Model response.\n    \"\"\"\n    assert len(self._actors) &gt;= 1, \"Model should have atleast one replica.\"\n    if self.num_replicas &gt; 1:\n        logger.warning(\"Model has &gt;1 replicas, use `.submit()` instead to fully utilize them.\")\n\n    method: str = kwargs.pop(\"_method\", self.spec.default_method)\n    # TODO (spillai): We should be able to determine if the output\n    # is an iterable or not from the signature, and set the default\n    stream: bool = kwargs.pop(\"_stream\", False)\n    actor_method_func = getattr(self._actors[0], method)\n    if not stream:\n        response_ref: ray.ObjectRef = actor_method_func.remote(**kwargs)\n        return ray.get(response_ref)\n    else:\n        response_refs: Iterable[ray.ObjectRef] = actor_method_func.options(num_returns=\"streaming\").remote(\n            **kwargs\n        )\n        return _StreamingModelHandleResponse(response_refs)\n</code></pre>"},{"location":"docs/api/managers.html#nos.managers.model.ModelHandle.scale","title":"scale","text":"<pre><code>scale(num_replicas: Union[int, str] = 1) -&gt; ModelHandle\n</code></pre> <p>Scale the model handle to a new number of replicas.</p> <p>Parameters:</p> <ul> <li> <code>num_replicas</code>               (<code>int or str</code>, default:                   <code>1</code> )           \u2013            <p>Number of replicas, or set to \"auto\" to automatically scale the model to the number of GPUs available.</p> </li> </ul> Source code in <code>nos/managers/model.py</code> <pre><code>def scale(self, num_replicas: Union[int, str] = 1) -&gt; \"ModelHandle\":\n    \"\"\"Scale the model handle to a new number of replicas.\n\n    Args:\n        num_replicas (int or str): Number of replicas, or set to \"auto\" to\n            automatically scale the model to the number of GPUs available.\n    \"\"\"\n    if isinstance(num_replicas, str) and num_replicas == \"auto\":\n        raise NotImplementedError(\"Automatic scaling not implemented.\")\n    if not isinstance(num_replicas, int):\n        raise ValueError(f\"Invalid replicas: {num_replicas}\")\n\n    # Check if there are any pending futures\n    if self._actor_pool.has_next():\n        logger.warning(f\"Pending futures detected, this may result in dropped queue items [name={self.spec.name}]\")\n    logger.debug(f\"Waiting for pending futures to complete before scaling [name={self.spec.name}].\")\n    logger.debug(f\"Scaling model [name={self.spec.name}].\")\n\n    if num_replicas == len(self._actors):\n        logger.debug(f\"Model already scaled appropriately [name={self.spec.name}, replicas={num_replicas}].\")\n        return self\n    elif num_replicas &gt; len(self._actors):\n        self._actors += [self._get_actor() for _ in range(num_replicas - len(self._actors))]\n        logger.debug(f\"Scaling up model [name={self.spec.name}, replicas={num_replicas}].\")\n    else:\n        actors_to_remove = self._actors[num_replicas:]\n        for actor in actors_to_remove:\n            ray.kill(actor)\n        self._actors = self._actors[:num_replicas]\n\n        logger.debug(f\"Scaling down model [name={self.spec.name}, replicas={num_replicas}].\")\n\n    # Update repicas and queue size\n    self.deployment.num_replicas = num_replicas\n\n    # Re-create the actor pool\n    logger.debug(f\"Removing actor pool [replicas={len(self._actors)}].\")\n    del self._actor_pool\n    self._actor_pool = None\n\n    # Re-create the actor pool\n    logger.debug(f\"Re-creating actor pool [name={self.spec.name}, replicas={num_replicas}].\")\n    self._actor_pool = ActorPool(self._actors)\n    assert len(self._actors) == num_replicas, \"Model scaling failed.\"\n    gc.collect()\n    return self\n</code></pre>"},{"location":"docs/api/managers.html#nos.managers.model.ModelHandle.submit","title":"submit","text":"<pre><code>submit(*args: Any, **kwargs: Any) -&gt; ObjectRef\n</code></pre> <p>Submit a task to the actor pool.</p> <p>Note (spillai): Caveats for <code>.submit()</code> with custom methods:     ModelHandles have a single result queue that add     results asynchronously on task completion. Calling <code>submit()</code>     with different methods interchangably will result in     the results queue being populated with results from     different methods. In other words, it is advised to     use <code>submit()</code> with the same method for a given model     and then use <code>get()</code> to fetch all the results, before     calling <code>submit()</code> with a different method.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Model arguments.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Model keyword arguments (except for special <code>_method</code> keyword that is used to call different class methods).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ObjectRef</code>           \u2013            <p>ray.ObjectRef: Ray object reference as a string.</p> </li> </ul> Source code in <code>nos/managers/model.py</code> <pre><code>def submit(self, *args: Any, **kwargs: Any) -&gt; ray.ObjectRef:\n    \"\"\"Submit a task to the actor pool.\n\n    Note (spillai): Caveats for `.submit()` with custom methods:\n        ModelHandles have a single result queue that add\n        results asynchronously on task completion. Calling `submit()`\n        with different methods interchangably will result in\n        the results queue being populated with results from\n        different methods. In other words, it is advised to\n        use `submit()` with the same method for a given model\n        and then use `get()` to fetch all the results, before\n        calling `submit()` with a different method.\n\n    Args:\n        *args: Model arguments.\n        **kwargs: Model keyword arguments\n            (except for special `_method` keyword that is\n            used to call different class methods).\n\n    Returns:\n        ray.ObjectRef: Ray object reference as a string.\n    \"\"\"\n    # Submit the task to the actor pool, leveraging all replicas\n    method: str = kwargs.pop(\"_method\", self.spec.default_method)\n    # TODO (spillai): We should be able to determine if the output\n    # is an iterable or not from the signature, and set the default\n    stream: bool = kwargs.pop(\"_stream\", False)\n    remote_opts = {\"num_returns\": \"streaming\"} if stream else {}\n    if not self._actor_pool._idle_actors:\n        logger.warning(f\"Actor pool is full, this may result in dropped queue items [name={self.spec.name}]\")\n    future_ref = self._actor_pool.submit(\n        lambda a, v: getattr(a, method).options(**remote_opts).remote(**v), kwargs\n    )\n    logger.info(f\"Submitted task [name={self.spec.name}, method={method}, kwargs={kwargs}]\")\n    return future_ref\n</code></pre>"},{"location":"docs/api/managers.html#nos.managers.model.ModelHandle.cleanup","title":"cleanup","text":"<pre><code>cleanup() -&gt; None\n</code></pre> <p>Kill all the actor handles and garbage collect.</p> Source code in <code>nos/managers/model.py</code> <pre><code>def cleanup(self) -&gt; None:\n    \"\"\"Kill all the actor handles and garbage collect.\"\"\"\n    for actor_handle in self._actors:\n        ray.kill(actor_handle)\n    self._actors = []\n    gc.collect()\n</code></pre>"},{"location":"docs/api/managers.html#nos.managers.model.ModelHandle.get","title":"get","text":"<pre><code>get(future_ref: ObjectRef = None, timeout: int = None) -&gt; Any\n</code></pre> <p>Get the result future.</p> Source code in <code>nos/managers/model.py</code> <pre><code>def get(self, future_ref: ray.ObjectRef = None, timeout: int = None) -&gt; Any:\n    \"\"\"Get the result future.\"\"\"\n    return self._actor_pool.get(future_ref)\n</code></pre>"},{"location":"docs/api/managers.html#nos.managers.model.ModelHandle.async_get","title":"async_get  <code>async</code>","text":"<pre><code>async_get(future_ref: ObjectRef = None, timeout: int = None) -&gt; Any\n</code></pre> <p>Get the result future asynchronously.</p> Source code in <code>nos/managers/model.py</code> <pre><code>async def async_get(self, future_ref: ray.ObjectRef = None, timeout: int = None) -&gt; Any:\n    \"\"\"Get the result future asynchronously.\"\"\"\n    return await self._actor_pool.async_get(future_ref)\n</code></pre>"},{"location":"docs/api/managers.html#nos.managers.model.ModelHandlePartial","title":"nos.managers.model.ModelHandlePartial  <code>dataclass</code>","text":"<p>ModelHandle partial object with methods patched from model spec signature.</p> Each method will have two variants <ol> <li>A callable function that can be used to call the method     directly (e.g. <code>handle.process_images(images=images)</code>).</li> <li>A submit function that can be used to submit the method     to the actor pool (e.g. <code>handle.submit_process_images(images=images)</code>).</li> </ol> Source code in <code>nos/managers/model.py</code> <pre><code>@dataclass\nclass ModelHandlePartial:\n    \"\"\"\n    ModelHandle partial object with methods patched from model spec signature.\n\n    Each method will have two variants:\n        1. A callable function that can be used to call the method\n            directly (e.g. `handle.process_images(images=images)`).\n        2. A submit function that can be used to submit the method\n            to the actor pool (e.g. `handle.submit_process_images(images=images)`).\n    \"\"\"\n\n    handle: \"ModelHandle\"\n    \"\"\"Original model handle.\"\"\"\n    method: str\n    \"\"\"Method name.\"\"\"\n\n    def __call__(self, *args: Any, **kwargs: Any) -&gt; Any:\n        return self.handle.__call__(*args, **kwargs, _method=self.method)\n\n    def submit(self, *args: Any, **kwargs: Any) -&gt; str:\n        return self.handle.submit(*args, **kwargs, _method=self.method)\n</code></pre>"},{"location":"docs/api/managers.html#nos.managers.model.ModelHandlePartial.handle","title":"handle  <code>instance-attribute</code>","text":"<pre><code>handle: ModelHandle\n</code></pre> <p>Original model handle.</p>"},{"location":"docs/api/managers.html#nos.managers.model.ModelHandlePartial.method","title":"method  <code>instance-attribute</code>","text":"<pre><code>method: str\n</code></pre> <p>Method name.</p>"},{"location":"docs/api/managers.html#nos.managers.model.ModelManager","title":"nos.managers.model.ModelManager  <code>dataclass</code>","text":"<p>Model manager for serving models with ray actors.</p> Features <ul> <li>Concurrency: Support fixed number of concurrent models,   running simultaneously with FIFO model eviction policies.</li> <li>Parallelism: Support multiple replicas of the same model.</li> <li>Optimal memory management: Model memory consumption   are automatically inferred from the model specification   and used to optimally bin-pack models on the GPU.</li> <li>Automatic garbage collection: Models are automatically   garbage collected when they are evicted from the manager.   Scaling models with the model manager should not result in OOM.</li> </ul> Source code in <code>nos/managers/model.py</code> <pre><code>@dataclass(frozen=True)\nclass ModelManager:\n    \"\"\"Model manager for serving models with ray actors.\n\n    Features:\n      * Concurrency: Support fixed number of concurrent models,\n        running simultaneously with FIFO model eviction policies.\n      * Parallelism: Support multiple replicas of the same model.\n      * Optimal memory management: Model memory consumption\n        are automatically inferred from the model specification\n        and used to optimally bin-pack models on the GPU.\n      * Automatic garbage collection: Models are automatically\n        garbage collected when they are evicted from the manager.\n        Scaling models with the model manager should not result in OOM.\n\n    \"\"\"\n\n    class EvictionPolicy(str, Enum):\n        FIFO = \"FIFO\"\n        LRU = \"LRU\"\n\n    policy: EvictionPolicy = EvictionPolicy.FIFO\n    \"\"\"Eviction policy.\"\"\"\n\n    max_concurrent_models: int = NOS_MAX_CONCURRENT_MODELS\n    \"\"\"Maximum number of concurrent models.\"\"\"\n\n    handlers: Dict[str, ModelHandle] = field(default_factory=OrderedDict)\n    \"\"\"Model handles mapped (key=model-identifier, value=ModelHandle).\"\"\"\n\n    def __post_init__(self):\n        \"\"\"Initialize the model manager.\"\"\"\n        if self.policy not in (self.EvictionPolicy.FIFO,):\n            raise NotImplementedError(f\"Eviction policy not implemented: {self.policy}\")\n        if self.max_concurrent_models &gt; 8:\n            raise Exception(\n                f\"Large number of concurrent models requested, keep it &lt;= 8 [concurrency={self.max_concurrent_models}]\"\n            )\n\n    def __repr__(self) -&gt; str:\n        \"\"\"String representation of the model manager (memory consumption, models, in tabular format etc).\"\"\"\n        repr_str = f\"\\nModelManager(policy={self.policy}, models={len(self.handlers)})\"\n        for idx, (model_id, model_handle) in enumerate(self.handlers.items()):\n            repr_str += f\"\\n  {idx}: [id={model_id}, model={model_handle}]\"\n        return repr_str\n\n    def __len__(self) -&gt; int:\n        \"\"\"Get the number of models in the manager.\"\"\"\n        return len(self.handlers)\n\n    def __contains__(self, spec: ModelSpec) -&gt; bool:\n        \"\"\"Check if a model exists in the manager.\n\n        Args:\n            spec (ModelSpec): Model specification.\n        Returns:\n            bool: True if the model exists, else False.\n        \"\"\"\n        return spec.id in self.handlers\n\n    def load(self, spec: ModelSpec, deployment: ModelDeploymentSpec = ModelDeploymentSpec()) -&gt; ModelHandle:\n        \"\"\"Load a model handle from the manager using the model specification.\n\n        Create a new model handle if it does not exist,\n        else return an existing handle.\n\n        Args:\n            spec (ModelSpec): Model specification.\n            deployment (ModelDeploymentSpec): Model deployment specification.\n        Returns:\n            ModelHandle: Model handle.\n        \"\"\"\n        model_id: str = spec.id\n        if model_id not in self.handlers:\n            return self.add(spec, deployment)\n        else:\n            # Only scale the model if the number of replicas is specified,\n            # otherwise treat it as a get without modifying the number of replicas.\n            if deployment.num_replicas is not None and deployment.num_replicas != self.handlers[model_id].num_replicas:\n                self.handlers[model_id].scale(deployment.num_replicas)\n            return self.handlers[model_id]\n\n    def get(self, spec: ModelSpec) -&gt; ModelHandle:\n        \"\"\"Get a model handle from the manager using the model identifier.\n\n        Args:\n            spec (ModelSpec): Model specification.\n        Returns:\n            ModelHandle: Model handle.\n        \"\"\"\n        model_id: str = spec.id\n        if model_id not in self.handlers:\n            return self.add(spec, ModelDeploymentSpec(num_replicas=1))\n        else:\n            return self.handlers[model_id]\n\n    def add(self, spec: ModelSpec, deployment: ModelDeploymentSpec = ModelDeploymentSpec()) -&gt; ModelHandle:\n        \"\"\"Add a model to the manager.\n\n        Args:\n            spec (ModelSpec): Model specification.\n            deployment (ModelDeploymentSpec): Model deployment specification.\n        Raises:\n            ValueError: If the model already exists.\n        Returns:\n            ModelHandle: Model handle.\n        \"\"\"\n        # If the model already exists, raise an error\n        model_id = spec.id\n        if model_id in self.handlers:\n            raise ValueError(f\"Model already exists [model_id={model_id}]\")\n\n        # If the model handle is full, pop the oldest model\n        if len(self.handlers) &gt;= self.max_concurrent_models:\n            _handle: ModelHandle = self.evict()\n\n        # Create the serve deployment from the model handle\n        # Note: Currently one model per (model-name, task) is supported.\n        self.handlers[model_id] = ModelHandle(spec, deployment)\n        logger.debug(f\"Added model [{self.handlers[model_id]}]\")\n        logger.debug(self)\n        return self.handlers[model_id]\n\n    def evict(self) -&gt; ModelHandle:\n        \"\"\"Evict a model from the manager (FIFO, LRU etc).\n\n        Returns:\n            ModelHandle: Model handle.\n        \"\"\"\n        # Pop the oldest model\n        # TODO (spillai): Implement LRU policy\n        assert len(self.handlers) &gt; 0, \"No models to evict.\"\n        _, handle = self.handlers.popitem(last=False)\n        model_id = handle.spec.id\n        logger.debug(f\"Deleting model [model_id={model_id}]\")\n\n        # Explicitly cleanup the model handle (including all actors)\n        handle.cleanup()\n        logger.debug(f\"Deleted model [model_id={model_id}]\")\n        logger.debug(self)\n        assert model_id not in self.handlers, f\"Model should have been evicted [model_id={model_id}]\"\n        return handle\n</code></pre>"},{"location":"docs/api/managers.html#nos.managers.model.ModelManager.policy","title":"policy  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>policy: EvictionPolicy = FIFO\n</code></pre> <p>Eviction policy.</p>"},{"location":"docs/api/managers.html#nos.managers.model.ModelManager.max_concurrent_models","title":"max_concurrent_models  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_concurrent_models: int = NOS_MAX_CONCURRENT_MODELS\n</code></pre> <p>Maximum number of concurrent models.</p>"},{"location":"docs/api/managers.html#nos.managers.model.ModelManager.handlers","title":"handlers  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>handlers: Dict[str, ModelHandle] = field(default_factory=OrderedDict)\n</code></pre> <p>Model handles mapped (key=model-identifier, value=ModelHandle).</p>"},{"location":"docs/api/managers.html#nos.managers.model.ModelManager.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Initialize the model manager.</p> Source code in <code>nos/managers/model.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Initialize the model manager.\"\"\"\n    if self.policy not in (self.EvictionPolicy.FIFO,):\n        raise NotImplementedError(f\"Eviction policy not implemented: {self.policy}\")\n    if self.max_concurrent_models &gt; 8:\n        raise Exception(\n            f\"Large number of concurrent models requested, keep it &lt;= 8 [concurrency={self.max_concurrent_models}]\"\n        )\n</code></pre>"},{"location":"docs/api/managers.html#nos.managers.model.ModelManager.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> <p>String representation of the model manager (memory consumption, models, in tabular format etc).</p> Source code in <code>nos/managers/model.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"String representation of the model manager (memory consumption, models, in tabular format etc).\"\"\"\n    repr_str = f\"\\nModelManager(policy={self.policy}, models={len(self.handlers)})\"\n    for idx, (model_id, model_handle) in enumerate(self.handlers.items()):\n        repr_str += f\"\\n  {idx}: [id={model_id}, model={model_handle}]\"\n    return repr_str\n</code></pre>"},{"location":"docs/api/managers.html#nos.managers.model.ModelManager.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Get the number of models in the manager.</p> Source code in <code>nos/managers/model.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Get the number of models in the manager.\"\"\"\n    return len(self.handlers)\n</code></pre>"},{"location":"docs/api/managers.html#nos.managers.model.ModelManager.__contains__","title":"__contains__","text":"<pre><code>__contains__(spec: ModelSpec) -&gt; bool\n</code></pre> <p>Check if a model exists in the manager.</p> <p>Parameters:</p> <ul> <li> <code>spec</code>               (<code>ModelSpec</code>)           \u2013            <p>Model specification.</p> </li> </ul> <p>Returns:     bool: True if the model exists, else False.</p> Source code in <code>nos/managers/model.py</code> <pre><code>def __contains__(self, spec: ModelSpec) -&gt; bool:\n    \"\"\"Check if a model exists in the manager.\n\n    Args:\n        spec (ModelSpec): Model specification.\n    Returns:\n        bool: True if the model exists, else False.\n    \"\"\"\n    return spec.id in self.handlers\n</code></pre>"},{"location":"docs/api/managers.html#nos.managers.model.ModelManager.load","title":"load","text":"<pre><code>load(spec: ModelSpec, deployment: ModelDeploymentSpec = ModelDeploymentSpec()) -&gt; ModelHandle\n</code></pre> <p>Load a model handle from the manager using the model specification.</p> <p>Create a new model handle if it does not exist, else return an existing handle.</p> <p>Parameters:</p> <ul> <li> <code>spec</code>               (<code>ModelSpec</code>)           \u2013            <p>Model specification.</p> </li> <li> <code>deployment</code>               (<code>ModelDeploymentSpec</code>, default:                   <code>ModelDeploymentSpec()</code> )           \u2013            <p>Model deployment specification.</p> </li> </ul> <p>Returns:     ModelHandle: Model handle.</p> Source code in <code>nos/managers/model.py</code> <pre><code>def load(self, spec: ModelSpec, deployment: ModelDeploymentSpec = ModelDeploymentSpec()) -&gt; ModelHandle:\n    \"\"\"Load a model handle from the manager using the model specification.\n\n    Create a new model handle if it does not exist,\n    else return an existing handle.\n\n    Args:\n        spec (ModelSpec): Model specification.\n        deployment (ModelDeploymentSpec): Model deployment specification.\n    Returns:\n        ModelHandle: Model handle.\n    \"\"\"\n    model_id: str = spec.id\n    if model_id not in self.handlers:\n        return self.add(spec, deployment)\n    else:\n        # Only scale the model if the number of replicas is specified,\n        # otherwise treat it as a get without modifying the number of replicas.\n        if deployment.num_replicas is not None and deployment.num_replicas != self.handlers[model_id].num_replicas:\n            self.handlers[model_id].scale(deployment.num_replicas)\n        return self.handlers[model_id]\n</code></pre>"},{"location":"docs/api/managers.html#nos.managers.model.ModelManager.get","title":"get","text":"<pre><code>get(spec: ModelSpec) -&gt; ModelHandle\n</code></pre> <p>Get a model handle from the manager using the model identifier.</p> <p>Parameters:</p> <ul> <li> <code>spec</code>               (<code>ModelSpec</code>)           \u2013            <p>Model specification.</p> </li> </ul> <p>Returns:     ModelHandle: Model handle.</p> Source code in <code>nos/managers/model.py</code> <pre><code>def get(self, spec: ModelSpec) -&gt; ModelHandle:\n    \"\"\"Get a model handle from the manager using the model identifier.\n\n    Args:\n        spec (ModelSpec): Model specification.\n    Returns:\n        ModelHandle: Model handle.\n    \"\"\"\n    model_id: str = spec.id\n    if model_id not in self.handlers:\n        return self.add(spec, ModelDeploymentSpec(num_replicas=1))\n    else:\n        return self.handlers[model_id]\n</code></pre>"},{"location":"docs/api/managers.html#nos.managers.model.ModelManager.add","title":"add","text":"<pre><code>add(spec: ModelSpec, deployment: ModelDeploymentSpec = ModelDeploymentSpec()) -&gt; ModelHandle\n</code></pre> <p>Add a model to the manager.</p> <p>Parameters:</p> <ul> <li> <code>spec</code>               (<code>ModelSpec</code>)           \u2013            <p>Model specification.</p> </li> <li> <code>deployment</code>               (<code>ModelDeploymentSpec</code>, default:                   <code>ModelDeploymentSpec()</code> )           \u2013            <p>Model deployment specification.</p> </li> </ul> <p>Raises:     ValueError: If the model already exists. Returns:     ModelHandle: Model handle.</p> Source code in <code>nos/managers/model.py</code> <pre><code>def add(self, spec: ModelSpec, deployment: ModelDeploymentSpec = ModelDeploymentSpec()) -&gt; ModelHandle:\n    \"\"\"Add a model to the manager.\n\n    Args:\n        spec (ModelSpec): Model specification.\n        deployment (ModelDeploymentSpec): Model deployment specification.\n    Raises:\n        ValueError: If the model already exists.\n    Returns:\n        ModelHandle: Model handle.\n    \"\"\"\n    # If the model already exists, raise an error\n    model_id = spec.id\n    if model_id in self.handlers:\n        raise ValueError(f\"Model already exists [model_id={model_id}]\")\n\n    # If the model handle is full, pop the oldest model\n    if len(self.handlers) &gt;= self.max_concurrent_models:\n        _handle: ModelHandle = self.evict()\n\n    # Create the serve deployment from the model handle\n    # Note: Currently one model per (model-name, task) is supported.\n    self.handlers[model_id] = ModelHandle(spec, deployment)\n    logger.debug(f\"Added model [{self.handlers[model_id]}]\")\n    logger.debug(self)\n    return self.handlers[model_id]\n</code></pre>"},{"location":"docs/api/managers.html#nos.managers.model.ModelManager.evict","title":"evict","text":"<pre><code>evict() -&gt; ModelHandle\n</code></pre> <p>Evict a model from the manager (FIFO, LRU etc).</p> <p>Returns:</p> <ul> <li> <code>ModelHandle</code> (              <code>ModelHandle</code> )          \u2013            <p>Model handle.</p> </li> </ul> Source code in <code>nos/managers/model.py</code> <pre><code>def evict(self) -&gt; ModelHandle:\n    \"\"\"Evict a model from the manager (FIFO, LRU etc).\n\n    Returns:\n        ModelHandle: Model handle.\n    \"\"\"\n    # Pop the oldest model\n    # TODO (spillai): Implement LRU policy\n    assert len(self.handlers) &gt; 0, \"No models to evict.\"\n    _, handle = self.handlers.popitem(last=False)\n    model_id = handle.spec.id\n    logger.debug(f\"Deleting model [model_id={model_id}]\")\n\n    # Explicitly cleanup the model handle (including all actors)\n    handle.cleanup()\n    logger.debug(f\"Deleted model [model_id={model_id}]\")\n    logger.debug(self)\n    assert model_id not in self.handlers, f\"Model should have been evicted [model_id={model_id}]\"\n    return handle\n</code></pre>"},{"location":"docs/api/server.html","title":"nos.server","text":""},{"location":"docs/api/server.html#nosserver","title":"nos.server","text":""},{"location":"docs/api/server.html#docker-runtime","title":"Docker Runtime","text":"<p>The docker runtime provides the <code>docker-py</code> interface to run containerized inference workloads using Docker. It allows starting and stopping containers, getting container information, and running the containers programmatically with HW support for accelerators like GPUs, ASICs etc. </p>"},{"location":"docs/api/server.html#nos.server._docker.DeviceRequest","title":"nos.server._docker.DeviceRequest  <code>dataclass</code>","text":"<p>Device request mappings for docker-py.</p> <p>For the given key, we map to the corresponding <code>docker.types.DeviceRequest</code> or <code>docker.types.Device</code> object. This makes it easy to add new devices in the future.</p> Source code in <code>nos/server/_docker.py</code> <pre><code>@dataclass\nclass DeviceRequest:\n    \"\"\"Device request mappings for docker-py.\n\n    For the given key, we map to the corresponding\n    `docker.types.DeviceRequest` or `docker.types.Device` object.\n    This makes it easy to add new devices in the future.\n    \"\"\"\n\n    configs = {\n        \"gpu\": {\n            \"device_requests\": [\n                docker.types.DeviceRequest(\n                    device_ids=[\"all\"],\n                    capabilities=[[\"gpu\"]],\n                )\n            ],\n        },\n        \"inf2\": {\n            \"devices\": [\"/dev/neuron0:/dev/neuron0:rwm\"],\n        },\n    }\n\n    @classmethod\n    def get(cls, device: str) -&gt; Dict[str, Any]:\n        \"\"\"Get device request.\"\"\"\n        try:\n            return cls.configs[device]\n        except KeyError:\n            raise ValueError(f\"Invalid DeviceRequest: {device}\")\n</code></pre>"},{"location":"docs/api/server.html#nos.server._docker.DeviceRequest.get","title":"get  <code>classmethod</code>","text":"<pre><code>get(device: str) -&gt; Dict[str, Any]\n</code></pre> <p>Get device request.</p> Source code in <code>nos/server/_docker.py</code> <pre><code>@classmethod\ndef get(cls, device: str) -&gt; Dict[str, Any]:\n    \"\"\"Get device request.\"\"\"\n    try:\n        return cls.configs[device]\n    except KeyError:\n        raise ValueError(f\"Invalid DeviceRequest: {device}\")\n</code></pre>"},{"location":"docs/api/server.html#nos.server._docker.DockerRuntime","title":"nos.server._docker.DockerRuntime  <code>dataclass</code>","text":"<p>Docker runtime for running containerized inference workloads.</p> Source code in <code>nos/server/_docker.py</code> <pre><code>@dataclass\nclass DockerRuntime:\n    \"\"\"\n    Docker runtime for running containerized inference workloads.\n    \"\"\"\n\n    _instance: \"DockerRuntime\" = None\n    _client: docker.DockerClient = None\n\n    def __init__(self):\n        \"\"\"Initialize DockerExecutor.\"\"\"\n        self._client = docker.from_env()\n\n    @classmethod\n    def get(cls: \"DockerRuntime\") -&gt; \"DockerRuntime\":\n        \"\"\"Get DockerRuntime instance.\"\"\"\n        if cls._instance is None:\n            cls._instance = cls()\n        return cls._instance\n\n    @classmethod\n    def list(cls, **kwargs) -&gt; Iterable[docker.models.containers.Container]:\n        \"\"\"List docker containers.\"\"\"\n        return cls.get()._client.containers.list(**kwargs)\n\n    def start(\n        self,\n        image: str,\n        command: Optional[Union[str, List[str]]] = None,\n        name: str = None,\n        device: Optional[str] = None,\n        **kwargs: Any,\n    ) -&gt; docker.models.containers.Container:\n        \"\"\"Start docker container.\n\n        Args:\n            **kwargs: See https://docker-py.readthedocs.io/en/stable/containers.html#docker.models.containers.ContainerCollection.run\n            ports (Optional[Dict[int, int]], optional): Port mapping. Defaults to None.\n            environment (Optional[Dict[str, str]], optional): Environment variables. Defaults to None.\n            volumes (Optional[Dict[str, str]], optional): Volume mapping. Defaults to None.\n            shm_size (Optional[int], optional): Shared memory size. Defaults to None.\n            detach (bool, optional): Whether to run the container in detached mode. Defaults to True.\n            remove (bool, optional): Whether to remove the container when it exits. Defaults to True.\n            device (bool, optional): Device to request (i.e. gpu, inf2). Defaults to None (i.e. cpu).\n\n        Note (Non-standard arguments):\n            gpu (bool): Whether to start the container with GPU support.\n\n        \"\"\"\n        # Check if container is already running, raise error if it is\n        if name and self.get_container(name) is not None:\n            container = self.get_container(name)\n            if container.status == \"running\":\n                raise RuntimeError(f\"Container with same name already running (name={name}).\")\n            else:\n                logger.warning(f\"Container with same name already exists, removing it (name={name}).\")\n                self.stop(name)\n\n        # Validate kwargs before passing to `containers.run(...)`\n        if \"devices\" in kwargs:\n            raise ValueError(\"Use `device='inf2'` instead of `devices`.\")\n        if \"device_requests\" in kwargs:\n            raise ValueError(\"Use `device='gpu'` instead of `device_requests`.\")\n\n        # Handle device requests (gpu=True, or inf2=True)\n        if device is not None:\n            assert device in DeviceRequest.configs, f\"Invalid device: {device}, available: {DeviceRequest.configs}\"\n            device_kwargs = DeviceRequest.get(device)\n            logger.debug(f\"Adding device [device={device}, {device_kwargs}]\")\n            kwargs.update(device_kwargs)\n\n        # Try starting the container, if it fails, remove it and try again\n        logger.debug(f\"Starting container: {name}\")\n        logger.debug(f\"\\timage: {image}\")\n        logger.debug(f\"\\tcommand: {command}\")\n        logger.debug(f\"\\tname: {name}\")\n        for k, v in kwargs.items():\n            logger.debug(f\"\\t{k}: {v}\")\n\n        # Start container (pass through kwargs)\n        try:\n            container = self._client.containers.run(\n                image,\n                command=command,\n                name=name,\n                **kwargs,\n            )\n            logger.debug(f\"Started container [name={name}, image={container.image}, id={container.id[:12]}]\")\n            logger.debug(f\"Get logs using `docker logs -f {container.id[:12]}`\")\n        except (docker.errors.APIError, docker.errors.DockerException) as exc:\n            self.stop(name)\n            raise ServerException(f\"Failed to start container [image={image}]\", exc=exc)\n        return container\n\n    def stop(self, name: str, timeout: int = 30) -&gt; docker.models.containers.Container:\n        \"\"\"Stop docker container.\"\"\"\n        try:\n            container = self.get_container(name)\n            if container is None:\n                logger.debug(f\"Container not running: {name}, exiting early.\")\n                return\n            logger.debug(f\"Removing container: [name={name}, image={container.image}, id={container.id[:12]}]\")\n            container.remove(force=True)\n            logger.debug(f\"Removed container: [name={name}, image={container.image}, id={container.id[:12]}]\")\n        except (docker.errors.APIError, docker.errors.DockerException) as exc:\n            raise ServerException(f\"Failed to stop container [name={name}]\", exc=exc)\n        return container\n\n    def get_container_id(self, name: str) -&gt; Optional[str]:\n        \"\"\"Get the runtime container ID.\"\"\"\n        container = self.get_container(name)\n        return container.id if container else None\n\n    def get_container(self, id_or_name: str) -&gt; docker.models.containers.Container:\n        \"\"\"Get container by id or name.\"\"\"\n        try:\n            return self._client.containers.get(id_or_name)\n        except docker.errors.NotFound:\n            return None\n\n    def get_container_status(self, id_or_name: str) -&gt; Optional[str]:\n        \"\"\"Get container status by id or name.\"\"\"\n        container = self.get_container(id_or_name)\n        return container.status if container else None\n\n    def get_container_logs(self, name: str, **kwargs) -&gt; Iterable[str]:\n        \"\"\"Get container logs.\"\"\"\n        try:\n            container = self.get_container(name)\n            if container is None:\n                return iter([])\n\n            for line in container.logs(stream=True):\n                yield line.decode(\"utf-8\")\n        except (docker.errors.APIError, docker.errors.DockerException) as exc:\n            logger.error(f\"Failed to get container logs: {exc}\")\n            raise ServerException(\"Failed to get container logs [name={name}]\", exc=exc)\n</code></pre>"},{"location":"docs/api/server.html#nos.server._docker.DockerRuntime.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize DockerExecutor.</p> Source code in <code>nos/server/_docker.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize DockerExecutor.\"\"\"\n    self._client = docker.from_env()\n</code></pre>"},{"location":"docs/api/server.html#nos.server._docker.DockerRuntime.get","title":"get  <code>classmethod</code>","text":"<pre><code>get() -&gt; DockerRuntime\n</code></pre> <p>Get DockerRuntime instance.</p> Source code in <code>nos/server/_docker.py</code> <pre><code>@classmethod\ndef get(cls: \"DockerRuntime\") -&gt; \"DockerRuntime\":\n    \"\"\"Get DockerRuntime instance.\"\"\"\n    if cls._instance is None:\n        cls._instance = cls()\n    return cls._instance\n</code></pre>"},{"location":"docs/api/server.html#nos.server._docker.DockerRuntime.list","title":"list  <code>classmethod</code>","text":"<pre><code>list(**kwargs) -&gt; Iterable[Container]\n</code></pre> <p>List docker containers.</p> Source code in <code>nos/server/_docker.py</code> <pre><code>@classmethod\ndef list(cls, **kwargs) -&gt; Iterable[docker.models.containers.Container]:\n    \"\"\"List docker containers.\"\"\"\n    return cls.get()._client.containers.list(**kwargs)\n</code></pre>"},{"location":"docs/api/server.html#nos.server._docker.DockerRuntime.start","title":"start","text":"<pre><code>start(image: str, command: Optional[Union[str, List[str]]] = None, name: str = None, device: Optional[str] = None, **kwargs: Any) -&gt; Container\n</code></pre> <p>Start docker container.</p> <p>Parameters:</p> <ul> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>See https://docker-py.readthedocs.io/en/stable/containers.html#docker.models.containers.ContainerCollection.run</p> </li> <li> <code>ports</code>               (<code>Optional[Dict[int, int]]</code>)           \u2013            <p>Port mapping. Defaults to None.</p> </li> <li> <code>environment</code>               (<code>Optional[Dict[str, str]]</code>)           \u2013            <p>Environment variables. Defaults to None.</p> </li> <li> <code>volumes</code>               (<code>Optional[Dict[str, str]]</code>)           \u2013            <p>Volume mapping. Defaults to None.</p> </li> <li> <code>shm_size</code>               (<code>Optional[int]</code>)           \u2013            <p>Shared memory size. Defaults to None.</p> </li> <li> <code>detach</code>               (<code>bool</code>)           \u2013            <p>Whether to run the container in detached mode. Defaults to True.</p> </li> <li> <code>remove</code>               (<code>bool</code>)           \u2013            <p>Whether to remove the container when it exits. Defaults to True.</p> </li> <li> <code>device</code>               (<code>bool</code>, default:                   <code>None</code> )           \u2013            <p>Device to request (i.e. gpu, inf2). Defaults to None (i.e. cpu).</p> </li> </ul> <p>Note (Non-standard arguments):     gpu (bool): Whether to start the container with GPU support.</p> Source code in <code>nos/server/_docker.py</code> <pre><code>def start(\n    self,\n    image: str,\n    command: Optional[Union[str, List[str]]] = None,\n    name: str = None,\n    device: Optional[str] = None,\n    **kwargs: Any,\n) -&gt; docker.models.containers.Container:\n    \"\"\"Start docker container.\n\n    Args:\n        **kwargs: See https://docker-py.readthedocs.io/en/stable/containers.html#docker.models.containers.ContainerCollection.run\n        ports (Optional[Dict[int, int]], optional): Port mapping. Defaults to None.\n        environment (Optional[Dict[str, str]], optional): Environment variables. Defaults to None.\n        volumes (Optional[Dict[str, str]], optional): Volume mapping. Defaults to None.\n        shm_size (Optional[int], optional): Shared memory size. Defaults to None.\n        detach (bool, optional): Whether to run the container in detached mode. Defaults to True.\n        remove (bool, optional): Whether to remove the container when it exits. Defaults to True.\n        device (bool, optional): Device to request (i.e. gpu, inf2). Defaults to None (i.e. cpu).\n\n    Note (Non-standard arguments):\n        gpu (bool): Whether to start the container with GPU support.\n\n    \"\"\"\n    # Check if container is already running, raise error if it is\n    if name and self.get_container(name) is not None:\n        container = self.get_container(name)\n        if container.status == \"running\":\n            raise RuntimeError(f\"Container with same name already running (name={name}).\")\n        else:\n            logger.warning(f\"Container with same name already exists, removing it (name={name}).\")\n            self.stop(name)\n\n    # Validate kwargs before passing to `containers.run(...)`\n    if \"devices\" in kwargs:\n        raise ValueError(\"Use `device='inf2'` instead of `devices`.\")\n    if \"device_requests\" in kwargs:\n        raise ValueError(\"Use `device='gpu'` instead of `device_requests`.\")\n\n    # Handle device requests (gpu=True, or inf2=True)\n    if device is not None:\n        assert device in DeviceRequest.configs, f\"Invalid device: {device}, available: {DeviceRequest.configs}\"\n        device_kwargs = DeviceRequest.get(device)\n        logger.debug(f\"Adding device [device={device}, {device_kwargs}]\")\n        kwargs.update(device_kwargs)\n\n    # Try starting the container, if it fails, remove it and try again\n    logger.debug(f\"Starting container: {name}\")\n    logger.debug(f\"\\timage: {image}\")\n    logger.debug(f\"\\tcommand: {command}\")\n    logger.debug(f\"\\tname: {name}\")\n    for k, v in kwargs.items():\n        logger.debug(f\"\\t{k}: {v}\")\n\n    # Start container (pass through kwargs)\n    try:\n        container = self._client.containers.run(\n            image,\n            command=command,\n            name=name,\n            **kwargs,\n        )\n        logger.debug(f\"Started container [name={name}, image={container.image}, id={container.id[:12]}]\")\n        logger.debug(f\"Get logs using `docker logs -f {container.id[:12]}`\")\n    except (docker.errors.APIError, docker.errors.DockerException) as exc:\n        self.stop(name)\n        raise ServerException(f\"Failed to start container [image={image}]\", exc=exc)\n    return container\n</code></pre>"},{"location":"docs/api/server.html#nos.server._docker.DockerRuntime.stop","title":"stop","text":"<pre><code>stop(name: str, timeout: int = 30) -&gt; Container\n</code></pre> <p>Stop docker container.</p> Source code in <code>nos/server/_docker.py</code> <pre><code>def stop(self, name: str, timeout: int = 30) -&gt; docker.models.containers.Container:\n    \"\"\"Stop docker container.\"\"\"\n    try:\n        container = self.get_container(name)\n        if container is None:\n            logger.debug(f\"Container not running: {name}, exiting early.\")\n            return\n        logger.debug(f\"Removing container: [name={name}, image={container.image}, id={container.id[:12]}]\")\n        container.remove(force=True)\n        logger.debug(f\"Removed container: [name={name}, image={container.image}, id={container.id[:12]}]\")\n    except (docker.errors.APIError, docker.errors.DockerException) as exc:\n        raise ServerException(f\"Failed to stop container [name={name}]\", exc=exc)\n    return container\n</code></pre>"},{"location":"docs/api/server.html#nos.server._docker.DockerRuntime.get_container_id","title":"get_container_id","text":"<pre><code>get_container_id(name: str) -&gt; Optional[str]\n</code></pre> <p>Get the runtime container ID.</p> Source code in <code>nos/server/_docker.py</code> <pre><code>def get_container_id(self, name: str) -&gt; Optional[str]:\n    \"\"\"Get the runtime container ID.\"\"\"\n    container = self.get_container(name)\n    return container.id if container else None\n</code></pre>"},{"location":"docs/api/server.html#nos.server._docker.DockerRuntime.get_container","title":"get_container","text":"<pre><code>get_container(id_or_name: str) -&gt; Container\n</code></pre> <p>Get container by id or name.</p> Source code in <code>nos/server/_docker.py</code> <pre><code>def get_container(self, id_or_name: str) -&gt; docker.models.containers.Container:\n    \"\"\"Get container by id or name.\"\"\"\n    try:\n        return self._client.containers.get(id_or_name)\n    except docker.errors.NotFound:\n        return None\n</code></pre>"},{"location":"docs/api/server.html#nos.server._docker.DockerRuntime.get_container_status","title":"get_container_status","text":"<pre><code>get_container_status(id_or_name: str) -&gt; Optional[str]\n</code></pre> <p>Get container status by id or name.</p> Source code in <code>nos/server/_docker.py</code> <pre><code>def get_container_status(self, id_or_name: str) -&gt; Optional[str]:\n    \"\"\"Get container status by id or name.\"\"\"\n    container = self.get_container(id_or_name)\n    return container.status if container else None\n</code></pre>"},{"location":"docs/api/server.html#nos.server._docker.DockerRuntime.get_container_logs","title":"get_container_logs","text":"<pre><code>get_container_logs(name: str, **kwargs) -&gt; Iterable[str]\n</code></pre> <p>Get container logs.</p> Source code in <code>nos/server/_docker.py</code> <pre><code>def get_container_logs(self, name: str, **kwargs) -&gt; Iterable[str]:\n    \"\"\"Get container logs.\"\"\"\n    try:\n        container = self.get_container(name)\n        if container is None:\n            return iter([])\n\n        for line in container.logs(stream=True):\n            yield line.decode(\"utf-8\")\n    except (docker.errors.APIError, docker.errors.DockerException) as exc:\n        logger.error(f\"Failed to get container logs: {exc}\")\n        raise ServerException(\"Failed to get container logs [name={name}]\", exc=exc)\n</code></pre>"},{"location":"docs/api/server.html#inferenceserviceruntime","title":"InferenceServiceRuntime","text":""},{"location":"docs/api/server.html#nos.server._runtime.InferenceServiceRuntime","title":"nos.server._runtime.InferenceServiceRuntime","text":"<p>Inference service runtime.</p> <p>This class is responsible for handling the lifecycle of the inference service docker runtime.</p> <p>Attributes:</p> <ul> <li> <code>configs</code>               (<code>InferenceServiceConfig</code>)           \u2013            <p>Inference service configuration.</p> </li> </ul> Source code in <code>nos/server/_runtime.py</code> <pre><code>class InferenceServiceRuntime:\n    \"\"\"Inference service runtime.\n\n    This class is responsible for handling the lifecycle of the\n    inference service docker runtime.\n\n    Attributes:\n        configs (InferenceServiceConfig): Inference service configuration.\n    \"\"\"\n\n    configs = {\n        \"cpu\": InferenceServiceRuntimeConfig(\n            image=NOS_DOCKER_IMAGE_CPU,\n            name=f\"{NOS_INFERENCE_SERVICE_CONTAINER_NAME}-cpu\",\n            kwargs={\n                \"nano_cpus\": int(6e9),\n                \"mem_limit\": \"6g\",\n                \"log_config\": {\"type\": LogConfig.types.JSON, \"config\": {\"max-size\": \"100m\", \"max-file\": \"10\"}},\n            },\n        ),\n        \"gpu\": InferenceServiceRuntimeConfig(\n            image=NOS_DOCKER_IMAGE_GPU,\n            name=f\"{NOS_INFERENCE_SERVICE_CONTAINER_NAME}-gpu\",\n            device=\"gpu\",\n            kwargs={\n                \"nano_cpus\": int(8e9),\n                \"mem_limit\": \"12g\",\n                \"log_config\": {\"type\": LogConfig.types.JSON, \"config\": {\"max-size\": \"100m\", \"max-file\": \"10\"}},\n            },\n        ),\n        \"trt\": InferenceServiceRuntimeConfig(\n            image=\"autonomi/nos:latest-trt\",\n            name=f\"{NOS_INFERENCE_SERVICE_CONTAINER_NAME}-trt\",\n            device=\"gpu\",\n            kwargs={\n                \"nano_cpus\": int(8e9),\n                \"mem_limit\": \"12g\",\n                \"log_config\": {\"type\": LogConfig.types.JSON, \"config\": {\"max-size\": \"100m\", \"max-file\": \"10\"}},\n            },\n        ),\n        \"inf2\": InferenceServiceRuntimeConfig(\n            image=\"autonomi/nos:latest-inf2\",\n            name=f\"{NOS_INFERENCE_SERVICE_CONTAINER_NAME}-inf2\",\n            device=\"inf2\",\n            environment=_default_environment({\"NEURON_RT_VISIBLE_CORES\": 2}),\n            kwargs={\n                \"nano_cpus\": int(8e9),\n                \"log_config\": {\"type\": LogConfig.types.JSON, \"config\": {\"max-size\": \"100m\", \"max-file\": \"10\"}},\n            },\n        ),\n    }\n\n    def __init__(self, runtime: str = \"cpu\", name: str = None):\n        \"\"\"Initialize the inference runtime.\n\n        Args:\n            runtime (str, optional): Inference runtime. Defaults to \"cpu\".\n            name (str, optional): Inference runtime name. Defaults to \"nos-inference-service\".\n        \"\"\"\n        if runtime not in self.configs:\n            raise ValueError(f\"Invalid inference runtime: {runtime}, available: {list(self.configs.keys())}\")\n        self.cfg = copy.deepcopy(self.configs[runtime])\n        if name is not None:\n            self.cfg.name = name\n\n        self._runtime = DockerRuntime.get()\n\n    def __repr__(self) -&gt; str:\n        return f\"InferenceServiceRuntime(image={self.cfg.image}, name={self.cfg.name}, device={self.cfg.device})\"\n\n    @staticmethod\n    def detect() -&gt; str:\n        \"\"\"Auto-detect inference runtime.\"\"\"\n        from nos.common.system import has_gpu, is_aws_inf2\n\n        if is_aws_inf2():\n            return \"inf2\"\n        elif has_gpu():\n            return \"gpu\"\n        else:\n            return \"cpu\"\n\n    @staticmethod\n    def devices() -&gt; List[str]:\n        \"\"\"Auto-detect devices to use.\"\"\"\n        from nos.common.system import has_gpu, is_aws_inf2\n\n        if is_aws_inf2():\n            return [str(p) for p in Path(\"/dev/\").glob(\"neuron*\")]\n        elif has_gpu():\n            return []\n        else:\n            return []\n\n    @staticmethod\n    def list(**kwargs) -&gt; List[docker.models.containers.Container]:\n        \"\"\"List running docker containers.\"\"\"\n        containers = DockerRuntime.get().list(**kwargs)\n        return [\n            container for container in containers if container.name.startswith(NOS_INFERENCE_SERVICE_CONTAINER_NAME)\n        ]\n\n    @classmethod\n    def supported_runtimes(cls) -&gt; List[str]:\n        \"\"\"Get supported runtimes.\"\"\"\n        return list(cls.configs.keys())\n\n    def start(self, **kwargs: Any) -&gt; docker.models.containers.Container:\n        \"\"\"Start the inference runtime.\n\n        Args:\n            **kwargs: Additional keyword-arguments to pass to `DockerRuntime.start`.\n        \"\"\"\n        logger.debug(f\"Starting inference runtime with image: {self.cfg.image}\")\n\n        # Override dict values\n        for k in (\"ports\", \"volumes\", \"environment\"):\n            if k in kwargs:\n                self.cfg.__dict__[k].update(kwargs.pop(k))\n                logger.debug(f\"Updating runtime configuration [key={k}, value={self.cfg.__dict__[k]}]\")\n\n        # Override config with supplied kwargs\n        for k in list(kwargs.keys()):\n            value = kwargs[k]\n            if hasattr(self.cfg, k):\n                setattr(self.cfg, k, value)\n                logger.debug(f\"Overriding inference runtime config: {k}={value}\")\n            else:\n                self.cfg.kwargs[k] = value\n\n        # Start inference runtime\n        container = self._runtime.start(\n            image=self.cfg.image,\n            name=self.cfg.name,\n            command=self.cfg.command,\n            ports=self.cfg.ports,\n            environment=self.cfg.environment,\n            volumes=self.cfg.volumes,\n            detach=self.cfg.detach,\n            device=self.cfg.device,\n            ipc_mode=self.cfg.ipc_mode,\n            **self.cfg.kwargs,\n        )\n        logger.debug(f\"Started inference runtime: {self}\")\n        return container\n\n    def stop(self, timeout: int = 30) -&gt; docker.models.containers.Container:\n        return self._runtime.stop(self.cfg.name, timeout=timeout)\n\n    def get_container(self) -&gt; docker.models.containers.Container:\n        return self._runtime.get_container(self.cfg.name)\n\n    def get_container_name(self) -&gt; Optional[str]:\n        return self._runtime.get_container(self.cfg.name).name\n\n    def get_container_id(self) -&gt; Optional[str]:\n        return self._runtime.get_container_id(self.cfg.name)\n\n    def get_container_status(self) -&gt; Optional[str]:\n        return self._runtime.get_container_status(self.cfg.name)\n\n    def get_container_logs(self, **kwargs) -&gt; Iterable[str]:\n        return self._runtime.get_container_logs(self.cfg.name, **kwargs)\n</code></pre>"},{"location":"docs/api/server.html#nos.server._runtime.InferenceServiceRuntime.__init__","title":"__init__","text":"<pre><code>__init__(runtime: str = 'cpu', name: str = None)\n</code></pre> <p>Initialize the inference runtime.</p> <p>Parameters:</p> <ul> <li> <code>runtime</code>               (<code>str</code>, default:                   <code>'cpu'</code> )           \u2013            <p>Inference runtime. Defaults to \"cpu\".</p> </li> <li> <code>name</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Inference runtime name. Defaults to \"nos-inference-service\".</p> </li> </ul> Source code in <code>nos/server/_runtime.py</code> <pre><code>def __init__(self, runtime: str = \"cpu\", name: str = None):\n    \"\"\"Initialize the inference runtime.\n\n    Args:\n        runtime (str, optional): Inference runtime. Defaults to \"cpu\".\n        name (str, optional): Inference runtime name. Defaults to \"nos-inference-service\".\n    \"\"\"\n    if runtime not in self.configs:\n        raise ValueError(f\"Invalid inference runtime: {runtime}, available: {list(self.configs.keys())}\")\n    self.cfg = copy.deepcopy(self.configs[runtime])\n    if name is not None:\n        self.cfg.name = name\n\n    self._runtime = DockerRuntime.get()\n</code></pre>"},{"location":"docs/api/server.html#nos.server._runtime.InferenceServiceRuntime.detect","title":"detect  <code>staticmethod</code>","text":"<pre><code>detect() -&gt; str\n</code></pre> <p>Auto-detect inference runtime.</p> Source code in <code>nos/server/_runtime.py</code> <pre><code>@staticmethod\ndef detect() -&gt; str:\n    \"\"\"Auto-detect inference runtime.\"\"\"\n    from nos.common.system import has_gpu, is_aws_inf2\n\n    if is_aws_inf2():\n        return \"inf2\"\n    elif has_gpu():\n        return \"gpu\"\n    else:\n        return \"cpu\"\n</code></pre>"},{"location":"docs/api/server.html#nos.server._runtime.InferenceServiceRuntime.devices","title":"devices  <code>staticmethod</code>","text":"<pre><code>devices() -&gt; List[str]\n</code></pre> <p>Auto-detect devices to use.</p> Source code in <code>nos/server/_runtime.py</code> <pre><code>@staticmethod\ndef devices() -&gt; List[str]:\n    \"\"\"Auto-detect devices to use.\"\"\"\n    from nos.common.system import has_gpu, is_aws_inf2\n\n    if is_aws_inf2():\n        return [str(p) for p in Path(\"/dev/\").glob(\"neuron*\")]\n    elif has_gpu():\n        return []\n    else:\n        return []\n</code></pre>"},{"location":"docs/api/server.html#nos.server._runtime.InferenceServiceRuntime.list","title":"list  <code>staticmethod</code>","text":"<pre><code>list(**kwargs) -&gt; List[Container]\n</code></pre> <p>List running docker containers.</p> Source code in <code>nos/server/_runtime.py</code> <pre><code>@staticmethod\ndef list(**kwargs) -&gt; List[docker.models.containers.Container]:\n    \"\"\"List running docker containers.\"\"\"\n    containers = DockerRuntime.get().list(**kwargs)\n    return [\n        container for container in containers if container.name.startswith(NOS_INFERENCE_SERVICE_CONTAINER_NAME)\n    ]\n</code></pre>"},{"location":"docs/api/server.html#nos.server._runtime.InferenceServiceRuntime.supported_runtimes","title":"supported_runtimes  <code>classmethod</code>","text":"<pre><code>supported_runtimes() -&gt; List[str]\n</code></pre> <p>Get supported runtimes.</p> Source code in <code>nos/server/_runtime.py</code> <pre><code>@classmethod\ndef supported_runtimes(cls) -&gt; List[str]:\n    \"\"\"Get supported runtimes.\"\"\"\n    return list(cls.configs.keys())\n</code></pre>"},{"location":"docs/api/server.html#nos.server._runtime.InferenceServiceRuntime.start","title":"start","text":"<pre><code>start(**kwargs: Any) -&gt; Container\n</code></pre> <p>Start the inference runtime.</p> <p>Parameters:</p> <ul> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword-arguments to pass to <code>DockerRuntime.start</code>.</p> </li> </ul> Source code in <code>nos/server/_runtime.py</code> <pre><code>def start(self, **kwargs: Any) -&gt; docker.models.containers.Container:\n    \"\"\"Start the inference runtime.\n\n    Args:\n        **kwargs: Additional keyword-arguments to pass to `DockerRuntime.start`.\n    \"\"\"\n    logger.debug(f\"Starting inference runtime with image: {self.cfg.image}\")\n\n    # Override dict values\n    for k in (\"ports\", \"volumes\", \"environment\"):\n        if k in kwargs:\n            self.cfg.__dict__[k].update(kwargs.pop(k))\n            logger.debug(f\"Updating runtime configuration [key={k}, value={self.cfg.__dict__[k]}]\")\n\n    # Override config with supplied kwargs\n    for k in list(kwargs.keys()):\n        value = kwargs[k]\n        if hasattr(self.cfg, k):\n            setattr(self.cfg, k, value)\n            logger.debug(f\"Overriding inference runtime config: {k}={value}\")\n        else:\n            self.cfg.kwargs[k] = value\n\n    # Start inference runtime\n    container = self._runtime.start(\n        image=self.cfg.image,\n        name=self.cfg.name,\n        command=self.cfg.command,\n        ports=self.cfg.ports,\n        environment=self.cfg.environment,\n        volumes=self.cfg.volumes,\n        detach=self.cfg.detach,\n        device=self.cfg.device,\n        ipc_mode=self.cfg.ipc_mode,\n        **self.cfg.kwargs,\n    )\n    logger.debug(f\"Started inference runtime: {self}\")\n    return container\n</code></pre>"},{"location":"docs/api/server.html#inferenceservice","title":"InferenceService","text":"<p>The <code>InferenceService</code> along with the <code>InferenceServiceImpl</code> gRPC service implementation provides a fully wrapped inference service via gRPC/HTTP2. The <code>InferenceServiceImpl</code> wraps the relevant API services such as <code>ListModels()</code>, <code>GetModelInfo()</code> and crucially <code>Run()</code> and executes the inference request via the <code>InferenceService</code> class. The <code>InferenceService</code> class manages models via the <code>ModelManager</code>, and sets up the necessary execution backend via <code>RayExecutor</code>. In addition to this, it is also responsible for managing shared memory regions (if requested) for high-performance inference running locally in a single machine.</p>"},{"location":"docs/api/server.html#nos.server._service.ModelHandle","title":"nos.server._service.ModelHandle  <code>dataclass</code>","text":"<p>Model handles for distributed model execution.</p> Usage <pre><code># Initialize a model handle\n&gt;&gt; model = ModelHandle(spec, num_replicas=1)\n\n# Call the task immediately\n&gt;&gt; response = model(**model_inputs)\n\n# Call a method on the model handle\n&gt;&gt; response = model.process_images(**model_inputs)\n\n# Submit a task to the model handle,\n# this will add results to the queue\n&gt;&gt; model.submit(**model_inputs)\n# Fetch the next result from the queue\n&gt;&gt; response = model.get()\n\n# Submit a task to a specific model handle method\n&gt;&gt; model.submit(**model_inputs, _method=\"process_images\")\n\n# Submit a task to the model handle,\n# this will add results to the queue\n&gt;&gt; model_handle.submit(**model_inputs)\n# Fetch the next result from the queue\n&gt;&gt; response = model_handle.get()\n\n# Cleanup model resources\n&gt;&gt; model_handle.cleanup()\n</code></pre> Source code in <code>nos/managers/model.py</code> <pre><code>@dataclass\nclass ModelHandle:\n    \"\"\"Model handles for distributed model execution.\n\n    Usage:\n        ```python\n        # Initialize a model handle\n        &gt;&gt; model = ModelHandle(spec, num_replicas=1)\n\n        # Call the task immediately\n        &gt;&gt; response = model(**model_inputs)\n\n        # Call a method on the model handle\n        &gt;&gt; response = model.process_images(**model_inputs)\n\n        # Submit a task to the model handle,\n        # this will add results to the queue\n        &gt;&gt; model.submit(**model_inputs)\n        # Fetch the next result from the queue\n        &gt;&gt; response = model.get()\n\n        # Submit a task to a specific model handle method\n        &gt;&gt; model.submit(**model_inputs, _method=\"process_images\")\n\n        # Submit a task to the model handle,\n        # this will add results to the queue\n        &gt;&gt; model_handle.submit(**model_inputs)\n        # Fetch the next result from the queue\n        &gt;&gt; response = model_handle.get()\n\n        # Cleanup model resources\n        &gt;&gt; model_handle.cleanup()\n        ```\n    \"\"\"\n\n    spec: ModelSpec\n    \"\"\"Model specification.\"\"\"\n    deployment: ModelDeploymentSpec = field(default_factory=ModelDeploymentSpec)\n    \"\"\"Number of replicas.\"\"\"\n    _actors: List[Union[ray.remote, ray.actor.ActorHandle]] = field(init=False, default=None)\n    \"\"\"Ray actor handle.\"\"\"\n    _actor_pool: ActorPool = field(init=False, default=None)\n    \"\"\"Ray actor pool.\"\"\"\n    _actor_options: Dict[str, Any] = field(init=False, default=None)\n    \"\"\"Ray actor options.\"\"\"\n\n    def __post_init__(self):\n        \"\"\"Initialize the actor handles.\"\"\"\n        self._actor_options = self._get_actor_options(self.spec, self.deployment)\n        self._actors = [self._get_actor() for _ in range(self.deployment.num_replicas)]\n        self._actor_pool = ActorPool(self._actors)\n\n        # Patch the model handle with methods from the model spec signature\n        for method in self.spec.signature:\n            # Note (spillai): We do not need to patch the __call__ method\n            # since it is already re-directed in the model handle.\n            if hasattr(self, method):\n                logger.debug(f\"Model handle ({self}) already has method ({method}), skipping ....\")\n                continue\n\n            # Methods:\n            #   &gt;&gt; handle.process_images: ModelHandlePartial\n            #   &gt;&gt; handle.process_images(images=...) =&gt; handle.__call__(images=..., _method=\"process_images\")\n            #   &gt;&gt; handle.process_images.submit(images=...) =&gt; handle.submit(images=..., _method=\"process_images\")\n            setattr(self, method, ModelHandlePartial(self, method))\n\n    def __repr__(self) -&gt; str:\n        assert len(self._actors) == self.num_replicas\n        opts_str = \", \".join([f\"{k}={v}\" for k, v in self._actor_options.items()])\n        return f\"ModelHandle(name={self.spec.name}, replicas={len(self._actors)}, opts=({opts_str}))\"\n\n    @property\n    def num_replicas(self) -&gt; int:\n        \"\"\"Get the number of replicas.\"\"\"\n        return self.deployment.num_replicas\n\n    @classmethod\n    def _get_actor_options(cls, spec: ModelSpec, deployment: ModelDeploymentSpec) -&gt; Dict[str, Any]:\n        \"\"\"Get actor options from model specification.\"\"\"\n        # TOFIX (spillai): When considering CPU-only models with num_cpus specified,\n        # OMP_NUM_THREADS will be set to the number of CPUs requested. Otherwise,\n        # if num_cpus is not specified, OMP_NUM_THREADS will default to 1.\n        # Instead, for now, we manually set the environment variable in `InferenceServiceRuntime`\n        # to the number of CPUs threads available.\n\n        # If deployment resources are not specified, get the model resources from the catalog\n        if deployment.resources is None:\n            try:\n                catalog = ModelSpecMetadataCatalog.get()\n                resources: ModelResources = catalog._resources_catalog[f\"{spec.id}/{spec.default_method}\"]\n            except Exception:\n                resources = ModelResources()\n                logger.debug(f\"Failed to get model resources [model={spec.id}, method={spec.default_method}]\")\n\n        # Otherwise, use the deployment resources provided\n        else:\n            resources = deployment.resources\n\n        # For GPU models, we need to set the number of fractional GPUs to use\n        if (resources.device == \"auto\" or resources.device == \"gpu\") and torch.cuda.is_available():\n            try:\n                # TODO (spillai): This needs to be resolved differently for\n                # multi-node clusters.\n                # Determine the current device id by checking the number of GPUs used.\n                total, available = ray.cluster_resources(), ray.available_resources()\n                gpus_used = total[\"GPU\"] - available[\"GPU\"]\n                device_id = int(gpus_used)\n\n                if isinstance(resources.device_memory, str) and resources.device_memory == \"auto\":\n                    gpu_frac = 1.0 / NOS_MAX_CONCURRENT_MODELS\n                    actor_opts = {\"num_gpus\": gpu_frac}\n                elif isinstance(resources.device_memory, int):\n                    # Fractional GPU memory needed within the current device\n                    device_memory = torch.cuda.get_device_properties(device_id).total_memory\n                    gpu_frac = float(resources.device_memory) / device_memory\n                    gpu_frac = round(gpu_frac * 10) / 10.0\n\n                    # Fractional GPU used for the current device\n                    gpu_frac_used = gpus_used - int(gpus_used)\n                    gpu_frac_avail = (1 - gpu_frac_used) * device_memory\n                    logger.debug(\n                        f\"\"\"actor_opts [model={spec.id}, \"\"\"\n                        f\"\"\"mem={humanize.naturalsize(resources.device_memory, binary=True)}, device={device_id}, device_mem={humanize.naturalsize(device_memory, binary=True)}, \"\"\"\n                        f\"\"\"gpu_frac={gpu_frac}, gpu_frac_avail={gpu_frac_avail}, \"\"\"\n                        f\"\"\"gpu_frac_used={gpu_frac_used}]\"\"\"\n                    )\n                    if gpu_frac &gt; gpu_frac_avail:\n                        logger.debug(\n                            f\"Insufficient GPU memory for model [model={spec.id}, \"\n                            f\"method={spec.default_method}, gpu_frac={gpu_frac}, \"\n                            f\"gpu_frac_avail={gpu_frac_avail}, gpu_frac_used={gpu_frac_used}]\"\n                        )\n                        if device_id == torch.cuda.device_count() - 1:\n                            # TOFIX (spillai): evict models to make space for the current model\n                            logger.debug(\"All GPUs are fully utilized, this may result in undesirable behavior.\")\n                    actor_opts = {\"num_gpus\": gpu_frac}\n                else:\n                    raise ValueError(f\"Invalid device memory: {resources.device_memory}\")\n            except Exception as exc:\n                logger.debug(f\"Failed to get GPU memory [e={exc}].\")\n                actor_opts = {\"num_gpus\": 1.0 / NOS_MAX_CONCURRENT_MODELS}\n\n        elif resources.device == \"cpu\":\n            actor_opts = {\"num_cpus\": resources.cpus, \"memory\": resources.memory}\n\n        else:\n            actor_opts = {\"num_cpus\": resources.cpus, \"memory\": resources.memory}\n\n        if spec.runtime_env is not None:\n            logger.debug(\"Using custom runtime environment, this may take a while to build.\")\n            actor_opts[\"runtime_env\"] = RuntimeEnv(**spec.runtime_env.model_dump())\n        logger.debug(f\"Actor options [id={spec.id}, opts={actor_opts}]\")\n\n        return actor_opts\n\n    def _get_actor(self) -&gt; Union[ray.remote, ray.actor.ActorHandle]:\n        \"\"\"Get an actor handle from model specification.\n\n        Returns:\n            Union[ray.remote, ray.actor.ActorHandle]: Ray actor handle.\n        \"\"\"\n        # TODO (spillai): Use the auto-tuned model spec to instantiate an\n        # actor the desired memory requirements. Fractional GPU amounts\n        # will need to be calculated from the target HW and model spec\n        # (i.e. 0.5 on A100 vs. T4 are different).\n        # NOTE (spillai): Using default signature here is OK, since\n        # all the signatures for a model spec have the same `func_or_cls`.\n        model_cls = self.spec.default_signature.func_or_cls\n\n        # Get the actor options from the model spec\n        actor_options = self._actor_options\n        actor_cls = ray.remote(**actor_options)(model_cls)\n\n        # Check if the model class has the required method\n        logger.debug(\n            f\"Creating actor [actor={actor_cls}, opts={actor_options}, cls={model_cls}, init_args={self.spec.default_signature.init_args}, init_kwargs={self.spec.default_signature.init_kwargs}]\"\n        )\n        actor = actor_cls.remote(*self.spec.default_signature.init_args, **self.spec.default_signature.init_kwargs)\n\n        # Note: Only check if default signature method is implemented\n        # even though other methods may be implemented and used.\n        if not hasattr(actor, self.spec.default_method):\n            raise NotImplementedError(f\"Model class {model_cls} does not have {self.spec.default_method} implemented.\")\n        logger.debug(f\"Creating actor [actor={actor}, opts={actor_options}, cls={model_cls}]\")\n\n        # Add some memory logs to this actor\n        if NOS_MEMRAY_ENABLED:\n            # Replace all non-alphanumeric characters with underscores\n            actor_name = re.sub(r\"\\W+\", \"_\", str(actor))\n            log_name = Path(NOS_RAY_LOGS_DIR) / f\"{actor_name}_mem_profile.bin\"\n            if log_name.exists():\n                log_name.unlink()\n            try:\n                memray.Tracker(log_name).__enter__()\n            except Exception:\n                logger.error(\"Failed to iniitialize memray tracker.\")\n        return actor\n\n    def __call__(self, *args: Any, **kwargs: Any) -&gt; Any:\n        \"\"\"Call the task immediately.\n\n        Args:\n            *args: Model arguments.\n            **kwargs: Model keyword arguments\n                (except for special `_method` keyword that is\n                used to call different class methods).\n        Returns:\n            Model response.\n        \"\"\"\n        assert len(self._actors) &gt;= 1, \"Model should have atleast one replica.\"\n        if self.num_replicas &gt; 1:\n            logger.warning(\"Model has &gt;1 replicas, use `.submit()` instead to fully utilize them.\")\n\n        method: str = kwargs.pop(\"_method\", self.spec.default_method)\n        # TODO (spillai): We should be able to determine if the output\n        # is an iterable or not from the signature, and set the default\n        stream: bool = kwargs.pop(\"_stream\", False)\n        actor_method_func = getattr(self._actors[0], method)\n        if not stream:\n            response_ref: ray.ObjectRef = actor_method_func.remote(**kwargs)\n            return ray.get(response_ref)\n        else:\n            response_refs: Iterable[ray.ObjectRef] = actor_method_func.options(num_returns=\"streaming\").remote(\n                **kwargs\n            )\n            return _StreamingModelHandleResponse(response_refs)\n\n    def scale(self, num_replicas: Union[int, str] = 1) -&gt; \"ModelHandle\":\n        \"\"\"Scale the model handle to a new number of replicas.\n\n        Args:\n            num_replicas (int or str): Number of replicas, or set to \"auto\" to\n                automatically scale the model to the number of GPUs available.\n        \"\"\"\n        if isinstance(num_replicas, str) and num_replicas == \"auto\":\n            raise NotImplementedError(\"Automatic scaling not implemented.\")\n        if not isinstance(num_replicas, int):\n            raise ValueError(f\"Invalid replicas: {num_replicas}\")\n\n        # Check if there are any pending futures\n        if self._actor_pool.has_next():\n            logger.warning(f\"Pending futures detected, this may result in dropped queue items [name={self.spec.name}]\")\n        logger.debug(f\"Waiting for pending futures to complete before scaling [name={self.spec.name}].\")\n        logger.debug(f\"Scaling model [name={self.spec.name}].\")\n\n        if num_replicas == len(self._actors):\n            logger.debug(f\"Model already scaled appropriately [name={self.spec.name}, replicas={num_replicas}].\")\n            return self\n        elif num_replicas &gt; len(self._actors):\n            self._actors += [self._get_actor() for _ in range(num_replicas - len(self._actors))]\n            logger.debug(f\"Scaling up model [name={self.spec.name}, replicas={num_replicas}].\")\n        else:\n            actors_to_remove = self._actors[num_replicas:]\n            for actor in actors_to_remove:\n                ray.kill(actor)\n            self._actors = self._actors[:num_replicas]\n\n            logger.debug(f\"Scaling down model [name={self.spec.name}, replicas={num_replicas}].\")\n\n        # Update repicas and queue size\n        self.deployment.num_replicas = num_replicas\n\n        # Re-create the actor pool\n        logger.debug(f\"Removing actor pool [replicas={len(self._actors)}].\")\n        del self._actor_pool\n        self._actor_pool = None\n\n        # Re-create the actor pool\n        logger.debug(f\"Re-creating actor pool [name={self.spec.name}, replicas={num_replicas}].\")\n        self._actor_pool = ActorPool(self._actors)\n        assert len(self._actors) == num_replicas, \"Model scaling failed.\"\n        gc.collect()\n        return self\n\n    def submit(self, *args: Any, **kwargs: Any) -&gt; ray.ObjectRef:\n        \"\"\"Submit a task to the actor pool.\n\n        Note (spillai): Caveats for `.submit()` with custom methods:\n            ModelHandles have a single result queue that add\n            results asynchronously on task completion. Calling `submit()`\n            with different methods interchangably will result in\n            the results queue being populated with results from\n            different methods. In other words, it is advised to\n            use `submit()` with the same method for a given model\n            and then use `get()` to fetch all the results, before\n            calling `submit()` with a different method.\n\n        Args:\n            *args: Model arguments.\n            **kwargs: Model keyword arguments\n                (except for special `_method` keyword that is\n                used to call different class methods).\n\n        Returns:\n            ray.ObjectRef: Ray object reference as a string.\n        \"\"\"\n        # Submit the task to the actor pool, leveraging all replicas\n        method: str = kwargs.pop(\"_method\", self.spec.default_method)\n        # TODO (spillai): We should be able to determine if the output\n        # is an iterable or not from the signature, and set the default\n        stream: bool = kwargs.pop(\"_stream\", False)\n        remote_opts = {\"num_returns\": \"streaming\"} if stream else {}\n        if not self._actor_pool._idle_actors:\n            logger.warning(f\"Actor pool is full, this may result in dropped queue items [name={self.spec.name}]\")\n        future_ref = self._actor_pool.submit(\n            lambda a, v: getattr(a, method).options(**remote_opts).remote(**v), kwargs\n        )\n        logger.info(f\"Submitted task [name={self.spec.name}, method={method}, kwargs={kwargs}]\")\n        return future_ref\n\n    def cleanup(self) -&gt; None:\n        \"\"\"Kill all the actor handles and garbage collect.\"\"\"\n        for actor_handle in self._actors:\n            ray.kill(actor_handle)\n        self._actors = []\n        gc.collect()\n\n    def get(self, future_ref: ray.ObjectRef = None, timeout: int = None) -&gt; Any:\n        \"\"\"Get the result future.\"\"\"\n        return self._actor_pool.get(future_ref)\n\n    async def async_get(self, future_ref: ray.ObjectRef = None, timeout: int = None) -&gt; Any:\n        \"\"\"Get the result future asynchronously.\"\"\"\n        return await self._actor_pool.async_get(future_ref)\n</code></pre>"},{"location":"docs/api/server.html#nos.server._service.ModelHandle.spec","title":"spec  <code>instance-attribute</code>","text":"<pre><code>spec: ModelSpec\n</code></pre> <p>Model specification.</p>"},{"location":"docs/api/server.html#nos.server._service.ModelHandle.deployment","title":"deployment  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>deployment: ModelDeploymentSpec = field(default_factory=ModelDeploymentSpec)\n</code></pre> <p>Number of replicas.</p>"},{"location":"docs/api/server.html#nos.server._service.ModelHandle.num_replicas","title":"num_replicas  <code>property</code>","text":"<pre><code>num_replicas: int\n</code></pre> <p>Get the number of replicas.</p>"},{"location":"docs/api/server.html#nos.server._service.ModelHandle.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> <p>Initialize the actor handles.</p> Source code in <code>nos/managers/model.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Initialize the actor handles.\"\"\"\n    self._actor_options = self._get_actor_options(self.spec, self.deployment)\n    self._actors = [self._get_actor() for _ in range(self.deployment.num_replicas)]\n    self._actor_pool = ActorPool(self._actors)\n\n    # Patch the model handle with methods from the model spec signature\n    for method in self.spec.signature:\n        # Note (spillai): We do not need to patch the __call__ method\n        # since it is already re-directed in the model handle.\n        if hasattr(self, method):\n            logger.debug(f\"Model handle ({self}) already has method ({method}), skipping ....\")\n            continue\n\n        # Methods:\n        #   &gt;&gt; handle.process_images: ModelHandlePartial\n        #   &gt;&gt; handle.process_images(images=...) =&gt; handle.__call__(images=..., _method=\"process_images\")\n        #   &gt;&gt; handle.process_images.submit(images=...) =&gt; handle.submit(images=..., _method=\"process_images\")\n        setattr(self, method, ModelHandlePartial(self, method))\n</code></pre>"},{"location":"docs/api/server.html#nos.server._service.ModelHandle.__call__","title":"__call__","text":"<pre><code>__call__(*args: Any, **kwargs: Any) -&gt; Any\n</code></pre> <p>Call the task immediately.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Model arguments.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Model keyword arguments (except for special <code>_method</code> keyword that is used to call different class methods).</p> </li> </ul> <p>Returns:     Model response.</p> Source code in <code>nos/managers/model.py</code> <pre><code>def __call__(self, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"Call the task immediately.\n\n    Args:\n        *args: Model arguments.\n        **kwargs: Model keyword arguments\n            (except for special `_method` keyword that is\n            used to call different class methods).\n    Returns:\n        Model response.\n    \"\"\"\n    assert len(self._actors) &gt;= 1, \"Model should have atleast one replica.\"\n    if self.num_replicas &gt; 1:\n        logger.warning(\"Model has &gt;1 replicas, use `.submit()` instead to fully utilize them.\")\n\n    method: str = kwargs.pop(\"_method\", self.spec.default_method)\n    # TODO (spillai): We should be able to determine if the output\n    # is an iterable or not from the signature, and set the default\n    stream: bool = kwargs.pop(\"_stream\", False)\n    actor_method_func = getattr(self._actors[0], method)\n    if not stream:\n        response_ref: ray.ObjectRef = actor_method_func.remote(**kwargs)\n        return ray.get(response_ref)\n    else:\n        response_refs: Iterable[ray.ObjectRef] = actor_method_func.options(num_returns=\"streaming\").remote(\n            **kwargs\n        )\n        return _StreamingModelHandleResponse(response_refs)\n</code></pre>"},{"location":"docs/api/server.html#nos.server._service.ModelHandle.scale","title":"scale","text":"<pre><code>scale(num_replicas: Union[int, str] = 1) -&gt; ModelHandle\n</code></pre> <p>Scale the model handle to a new number of replicas.</p> <p>Parameters:</p> <ul> <li> <code>num_replicas</code>               (<code>int or str</code>, default:                   <code>1</code> )           \u2013            <p>Number of replicas, or set to \"auto\" to automatically scale the model to the number of GPUs available.</p> </li> </ul> Source code in <code>nos/managers/model.py</code> <pre><code>def scale(self, num_replicas: Union[int, str] = 1) -&gt; \"ModelHandle\":\n    \"\"\"Scale the model handle to a new number of replicas.\n\n    Args:\n        num_replicas (int or str): Number of replicas, or set to \"auto\" to\n            automatically scale the model to the number of GPUs available.\n    \"\"\"\n    if isinstance(num_replicas, str) and num_replicas == \"auto\":\n        raise NotImplementedError(\"Automatic scaling not implemented.\")\n    if not isinstance(num_replicas, int):\n        raise ValueError(f\"Invalid replicas: {num_replicas}\")\n\n    # Check if there are any pending futures\n    if self._actor_pool.has_next():\n        logger.warning(f\"Pending futures detected, this may result in dropped queue items [name={self.spec.name}]\")\n    logger.debug(f\"Waiting for pending futures to complete before scaling [name={self.spec.name}].\")\n    logger.debug(f\"Scaling model [name={self.spec.name}].\")\n\n    if num_replicas == len(self._actors):\n        logger.debug(f\"Model already scaled appropriately [name={self.spec.name}, replicas={num_replicas}].\")\n        return self\n    elif num_replicas &gt; len(self._actors):\n        self._actors += [self._get_actor() for _ in range(num_replicas - len(self._actors))]\n        logger.debug(f\"Scaling up model [name={self.spec.name}, replicas={num_replicas}].\")\n    else:\n        actors_to_remove = self._actors[num_replicas:]\n        for actor in actors_to_remove:\n            ray.kill(actor)\n        self._actors = self._actors[:num_replicas]\n\n        logger.debug(f\"Scaling down model [name={self.spec.name}, replicas={num_replicas}].\")\n\n    # Update repicas and queue size\n    self.deployment.num_replicas = num_replicas\n\n    # Re-create the actor pool\n    logger.debug(f\"Removing actor pool [replicas={len(self._actors)}].\")\n    del self._actor_pool\n    self._actor_pool = None\n\n    # Re-create the actor pool\n    logger.debug(f\"Re-creating actor pool [name={self.spec.name}, replicas={num_replicas}].\")\n    self._actor_pool = ActorPool(self._actors)\n    assert len(self._actors) == num_replicas, \"Model scaling failed.\"\n    gc.collect()\n    return self\n</code></pre>"},{"location":"docs/api/server.html#nos.server._service.ModelHandle.submit","title":"submit","text":"<pre><code>submit(*args: Any, **kwargs: Any) -&gt; ObjectRef\n</code></pre> <p>Submit a task to the actor pool.</p> <p>Note (spillai): Caveats for <code>.submit()</code> with custom methods:     ModelHandles have a single result queue that add     results asynchronously on task completion. Calling <code>submit()</code>     with different methods interchangably will result in     the results queue being populated with results from     different methods. In other words, it is advised to     use <code>submit()</code> with the same method for a given model     and then use <code>get()</code> to fetch all the results, before     calling <code>submit()</code> with a different method.</p> <p>Parameters:</p> <ul> <li> <code>*args</code>               (<code>Any</code>, default:                   <code>()</code> )           \u2013            <p>Model arguments.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Model keyword arguments (except for special <code>_method</code> keyword that is used to call different class methods).</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ObjectRef</code>           \u2013            <p>ray.ObjectRef: Ray object reference as a string.</p> </li> </ul> Source code in <code>nos/managers/model.py</code> <pre><code>def submit(self, *args: Any, **kwargs: Any) -&gt; ray.ObjectRef:\n    \"\"\"Submit a task to the actor pool.\n\n    Note (spillai): Caveats for `.submit()` with custom methods:\n        ModelHandles have a single result queue that add\n        results asynchronously on task completion. Calling `submit()`\n        with different methods interchangably will result in\n        the results queue being populated with results from\n        different methods. In other words, it is advised to\n        use `submit()` with the same method for a given model\n        and then use `get()` to fetch all the results, before\n        calling `submit()` with a different method.\n\n    Args:\n        *args: Model arguments.\n        **kwargs: Model keyword arguments\n            (except for special `_method` keyword that is\n            used to call different class methods).\n\n    Returns:\n        ray.ObjectRef: Ray object reference as a string.\n    \"\"\"\n    # Submit the task to the actor pool, leveraging all replicas\n    method: str = kwargs.pop(\"_method\", self.spec.default_method)\n    # TODO (spillai): We should be able to determine if the output\n    # is an iterable or not from the signature, and set the default\n    stream: bool = kwargs.pop(\"_stream\", False)\n    remote_opts = {\"num_returns\": \"streaming\"} if stream else {}\n    if not self._actor_pool._idle_actors:\n        logger.warning(f\"Actor pool is full, this may result in dropped queue items [name={self.spec.name}]\")\n    future_ref = self._actor_pool.submit(\n        lambda a, v: getattr(a, method).options(**remote_opts).remote(**v), kwargs\n    )\n    logger.info(f\"Submitted task [name={self.spec.name}, method={method}, kwargs={kwargs}]\")\n    return future_ref\n</code></pre>"},{"location":"docs/api/server.html#nos.server._service.ModelHandle.cleanup","title":"cleanup","text":"<pre><code>cleanup() -&gt; None\n</code></pre> <p>Kill all the actor handles and garbage collect.</p> Source code in <code>nos/managers/model.py</code> <pre><code>def cleanup(self) -&gt; None:\n    \"\"\"Kill all the actor handles and garbage collect.\"\"\"\n    for actor_handle in self._actors:\n        ray.kill(actor_handle)\n    self._actors = []\n    gc.collect()\n</code></pre>"},{"location":"docs/api/server.html#nos.server._service.ModelHandle.get","title":"get","text":"<pre><code>get(future_ref: ObjectRef = None, timeout: int = None) -&gt; Any\n</code></pre> <p>Get the result future.</p> Source code in <code>nos/managers/model.py</code> <pre><code>def get(self, future_ref: ray.ObjectRef = None, timeout: int = None) -&gt; Any:\n    \"\"\"Get the result future.\"\"\"\n    return self._actor_pool.get(future_ref)\n</code></pre>"},{"location":"docs/api/server.html#nos.server._service.ModelHandle.async_get","title":"async_get  <code>async</code>","text":"<pre><code>async_get(future_ref: ObjectRef = None, timeout: int = None) -&gt; Any\n</code></pre> <p>Get the result future asynchronously.</p> Source code in <code>nos/managers/model.py</code> <pre><code>async def async_get(self, future_ref: ray.ObjectRef = None, timeout: int = None) -&gt; Any:\n    \"\"\"Get the result future asynchronously.\"\"\"\n    return await self._actor_pool.async_get(future_ref)\n</code></pre>"},{"location":"docs/api/server.html#nos.server._service.InferenceServiceImpl","title":"nos.server._service.InferenceServiceImpl","text":"<p>Experimental gRPC-based inference service.</p> <p>This service is used to serve models over gRPC.</p> <p>Refer to the bring-your-own-schema section: https://docs.ray.io/en/master/serve/direct-ingress.html?highlight=grpc#bring-your-own-schema</p> Source code in <code>nos/server/_service.py</code> <pre><code>class InferenceServiceImpl(nos_service_pb2_grpc.InferenceServiceServicer, InferenceService):\n    \"\"\"\n    Experimental gRPC-based inference service.\n\n    This service is used to serve models over gRPC.\n\n    Refer to the bring-your-own-schema section:\n    https://docs.ray.io/en/master/serve/direct-ingress.html?highlight=grpc#bring-your-own-schema\n    \"\"\"\n\n    def __init__(self, catalog_filename: str = None):\n        super().__init__()\n        self._tmp_files = {}\n\n        if catalog_filename is None:\n            return\n\n        if not Path(catalog_filename).exists():\n            raise ValueError(f\"Model catalog not found [catalog={catalog_filename}]\")\n\n        # Register models from the catalog\n        services: List[ModelServiceSpec] = hub.register_from_yaml(catalog_filename)\n        for svc in services:\n            logger.debug(f\"Servicing model [svc={svc}, replicas={svc.deployment.num_replicas}]\")\n            self.load_model_spec(svc.model, svc.deployment)\n            logger.debug(f\"Deployed model [svc={svc}]. \\n{self.model_manager}\")\n\n    async def Ping(self, _: empty_pb2.Empty, context: grpc.ServicerContext) -&gt; nos_service_pb2.PingResponse:\n        \"\"\"Health check.\"\"\"\n        return nos_service_pb2.PingResponse(status=\"ok\")\n\n    def GetServiceInfo(self, _: empty_pb2.Empty, context: grpc.ServicerContext) -&gt; nos_service_pb2.ServiceInfoResponse:\n        \"\"\"Get information on the service.\"\"\"\n        from nos.common.system import has_gpu, is_inside_docker\n\n        if is_inside_docker():\n            runtime = \"gpu\" if has_gpu() else \"cpu\"\n        else:\n            runtime = \"local\"\n        return nos_service_pb2.ServiceInfoResponse(version=__version__, runtime=runtime)\n\n    def ListModels(self, _: empty_pb2.Empty, context: grpc.ServicerContext) -&gt; nos_service_pb2.GenericResponse:\n        \"\"\"List all models.\"\"\"\n        models = list(hub.list())\n        logger.debug(f\"ListModels() [models={len(models)}]\")\n        return nos_service_pb2.GenericResponse(response_bytes=dumps(models))\n\n    def GetModelCatalog(self, _: empty_pb2.Empty, context: grpc.ServicerContext) -&gt; nos_service_pb2.GenericResponse:\n        \"\"\"Get the model catalog.\"\"\"\n        catalog = ModelSpecMetadataCatalog.get()\n        logger.debug(f\"GetModelCatalog() [catalog={catalog._metadata_catalog}]\")\n        return nos_service_pb2.GenericResponse(response_bytes=dumps(catalog))\n\n    def GetModelInfo(\n        self, request: wrappers_pb2.StringValue, context: grpc.ServicerContext\n    ) -&gt; nos_service_pb2.GenericResponse:\n        \"\"\"Get model information.\"\"\"\n        try:\n            model_id = request.value\n            spec: ModelSpec = hub.load_spec(model_id)\n        except KeyError as e:\n            logger.error(f\"Failed to load spec [request={request}, e={e}]\")\n            context.abort(grpc.StatusCode.NOT_FOUND, str(e))\n        return spec._to_proto()\n\n    def LoadModel(self, request: nos_service_pb2.GenericRequest, context: grpc.ServicerContext) -&gt; empty_pb2.Empty:\n        \"\"\"Load / scale the model to the specified number of replicas.\"\"\"\n        request: Dict[str, Any] = loads(request.request_bytes)\n        logger.debug(f\"ScaleModel() [request={request}]\")\n        try:\n            model_id = request[\"id\"]\n            num_replicas = request.get(\"num_replicas\", 1)\n            self.load_model(model_id, num_replicas=num_replicas)\n            return empty_pb2.Empty()\n        except Exception as e:\n            err_msg = f\"Failed to scale model [model_id={model_id}, num_replicas={num_replicas}, e={e}]\"\n            logger.error(err_msg)\n            context.abort(grpc.StatusCode.INTERNAL, err_msg)\n\n    def RegisterSystemSharedMemory(\n        self, request: nos_service_pb2.GenericRequest, context: grpc.ServicerContext\n    ) -&gt; nos_service_pb2.GenericResponse:\n        \"\"\"Register system shared memory under a specific namespace `&lt;client_id&gt;/&lt;object_id&gt;`.\"\"\"\n        if not NOS_SHM_ENABLED:\n            context.abort(grpc.StatusCode.UNIMPLEMENTED, \"Shared memory not enabled.\")\n\n        metadata = dict(context.invocation_metadata())\n        client_id = metadata.get(\"client_id\", None)\n        object_id = metadata.get(\"object_id\", None)\n        namespace = f\"{client_id}/{object_id}\"\n        logger.debug(f\"Registering shm [client_id={client_id}, object_id={object_id}]\")\n        try:\n            # Create a shared memory segment for the inputs\n            # Note: The returned keys for shared memory segments are identical to the\n            # keys in the input dictionary (i.e. &lt;key&gt;), and are not prefixed with the\n            # namespace `&lt;client_id&gt;/&lt;object_id&gt;`.\n            shm_map = self.shm_manager.create(loads(request.request_bytes), namespace=namespace)\n            # Here, dumps() is used to serialize the shared memory numy objects via __getstate__().\n            # The serialized data is then sent back to the client, which can then deserialized\n            # and set via __setstate__() on the client-side, so that the client can access the shared\n            # memory segments.\n            logger.debug(f\"Registered shm [client_id={client_id}, object_id={object_id}, shm_map={shm_map}]\")\n            return nos_service_pb2.GenericResponse(response_bytes=dumps(shm_map))\n        except Exception as e:\n            logger.error(f\"Failed to register system shared memory: {e}\")\n            context.abort(grpc.StatusCode.INTERNAL, str(e))\n\n    def UnregisterSystemSharedMemory(\n        self, request: nos_service_pb2.GenericRequest, context: grpc.ServicerContext\n    ) -&gt; nos_service_pb2.GenericResponse:\n        \"\"\"Unregister system shared memory for specific namespace `&lt;client_id&gt;/&lt;object_id&gt;`.\"\"\"\n        if not NOS_SHM_ENABLED:\n            context.abort(context, grpc.StatusCode.UNIMPLEMENTED, \"Shared memory not enabled.\")\n\n        metadata = dict(context.invocation_metadata())\n        client_id = metadata.get(\"client_id\", None)\n        object_id = metadata.get(\"object_id\", None)\n        namespace = f\"{client_id}/{object_id}\"\n        # TODO (spillai): Currently, we can ignore the `request` provided\n        # by the client, since all the shared memory segments under the namespace are deleted.\n        logger.debug(f\"Unregistering shm [client_id={client_id}, object_id={object_id}]\")\n        try:\n            self.shm_manager.cleanup(namespace=namespace)\n        except Exception as e:\n            logger.error(f\"Failed to unregister shm [e{e}]\")\n            context.abort(grpc.StatusCode.INTERNAL, str(e))\n        return nos_service_pb2.GenericResponse()\n\n    def UploadFile(self, request_iterator: Any, context: grpc.ServicerContext) -&gt; nos_service_pb2.GenericResponse:\n        \"\"\"Upload a file.\"\"\"\n        for _chunk_idx, chunk_request in enumerate(request_iterator):\n            chunk = loads(chunk_request.request_bytes)\n            chunk_bytes = chunk[\"chunk_bytes\"]\n            path = Path(chunk[\"filename\"])\n            if str(path) not in self._tmp_files:\n                tmp_file = NamedTemporaryFile(delete=False, dir=\"/tmp\", suffix=path.suffix)\n                self._tmp_files[str(path)] = tmp_file\n                logger.debug(\n                    f\"Streaming upload [path={tmp_file.name}, size={Path(tmp_file.name).stat().st_size / (1024 * 1024):.2f} MB]\"\n                )\n            else:\n                tmp_file = self._tmp_files[str(path)]\n            with open(tmp_file.name, \"ab\") as f:\n                f.write(chunk_bytes)\n        return nos_service_pb2.GenericResponse(response_bytes=dumps({\"filename\": tmp_file.name}))\n\n    def DeleteFile(self, request: nos_service_pb2.GenericRequest, context: grpc.ServicerContext) -&gt; empty_pb2.Empty:\n        \"\"\"Delete a file by its file-identifier.\"\"\"\n        request = loads(request.request_bytes)\n\n        filename = str(request[\"filename\"])\n        try:\n            tmp_file = self._tmp_files[str(filename)]\n            path = Path(tmp_file.name)\n            assert path.exists(), f\"File handle {filename} not found\"\n        except Exception as e:\n            err_msg = f\"Failed to delete file [filename={filename}, e={e}]\"\n            logger.error(err_msg)\n            context.abort(grpc.StatusCode.NOT_FOUND, err_msg)\n\n        logger.debug(f\"Deleting file [path={path}]\")\n        path.unlink()\n        del self._tmp_files[str(filename)]\n        return empty_pb2.Empty()\n\n    async def Run(\n        self, request: nos_service_pb2.GenericRequest, context: grpc.ServicerContext\n    ) -&gt; nos_service_pb2.GenericResponse:\n        \"\"\"Main model prediction interface.\"\"\"\n        request: Dict[str, Any] = loads(request.request_bytes)\n        try:\n            st = time.perf_counter()\n            logger.info(f\"Executing request [model={request['id']}, method={request['method']}]\")\n            response = await self.execute_model(request[\"id\"], method=request[\"method\"], inputs=request[\"inputs\"])\n            logger.info(\n                f\"Executed request [model={request['id']}, method={request['method']}, elapsed={(time.perf_counter() - st) * 1e3:.1f}ms]\"\n            )\n            return nos_service_pb2.GenericResponse(response_bytes=dumps(response))\n        except (grpc.RpcError, Exception) as e:\n            msg = f\"Failed to execute request [model={request['id']}, method={request['method']}]\"\n            msg += f\"{traceback.format_exc()}\"\n            logger.error(f\"{msg}, e={e}\")\n            context.abort(grpc.StatusCode.INTERNAL, \"Internal Server Error\")\n\n    async def Stream(\n        self, request: nos_service_pb2.GenericRequest, context: grpc.ServicerContext\n    ) -&gt; Iterator[nos_service_pb2.GenericResponse]:\n        \"\"\"Main streaming model prediction interface.\"\"\"\n        request: Dict[str, Any] = loads(request.request_bytes)\n        try:\n            logger.info(f\"Executing request [model={request['id']}, method={request['method']}]\")\n            response_stream = await self.execute_model(\n                request[\"id\"], method=request[\"method\"], inputs=request[\"inputs\"], stream=True\n            )\n            for response in response_stream:\n                yield nos_service_pb2.GenericResponse(response_bytes=dumps(response))\n            logger.info(f\"Executed request [model={request['id']}, method={request['method']}]\")\n        except (grpc.RpcError, Exception) as e:\n            msg = f\"Failed to execute request [model={request['id']}, method={request['method']}]\"\n            msg += f\"{traceback.format_exc()}\"\n            logger.error(f\"{msg}, e={e}\")\n            context.abort(grpc.StatusCode.INTERNAL, \"Internal Server Error\")\n</code></pre>"},{"location":"docs/api/server.html#nos.server._service.InferenceServiceImpl.Ping","title":"Ping  <code>async</code>","text":"<pre><code>Ping(_: Empty, context: ServicerContext) -&gt; PingResponse\n</code></pre> <p>Health check.</p> Source code in <code>nos/server/_service.py</code> <pre><code>async def Ping(self, _: empty_pb2.Empty, context: grpc.ServicerContext) -&gt; nos_service_pb2.PingResponse:\n    \"\"\"Health check.\"\"\"\n    return nos_service_pb2.PingResponse(status=\"ok\")\n</code></pre>"},{"location":"docs/api/server.html#nos.server._service.InferenceServiceImpl.GetServiceInfo","title":"GetServiceInfo","text":"<pre><code>GetServiceInfo(_: Empty, context: ServicerContext) -&gt; ServiceInfoResponse\n</code></pre> <p>Get information on the service.</p> Source code in <code>nos/server/_service.py</code> <pre><code>def GetServiceInfo(self, _: empty_pb2.Empty, context: grpc.ServicerContext) -&gt; nos_service_pb2.ServiceInfoResponse:\n    \"\"\"Get information on the service.\"\"\"\n    from nos.common.system import has_gpu, is_inside_docker\n\n    if is_inside_docker():\n        runtime = \"gpu\" if has_gpu() else \"cpu\"\n    else:\n        runtime = \"local\"\n    return nos_service_pb2.ServiceInfoResponse(version=__version__, runtime=runtime)\n</code></pre>"},{"location":"docs/api/server.html#nos.server._service.InferenceServiceImpl.ListModels","title":"ListModels","text":"<pre><code>ListModels(_: Empty, context: ServicerContext) -&gt; GenericResponse\n</code></pre> <p>List all models.</p> Source code in <code>nos/server/_service.py</code> <pre><code>def ListModels(self, _: empty_pb2.Empty, context: grpc.ServicerContext) -&gt; nos_service_pb2.GenericResponse:\n    \"\"\"List all models.\"\"\"\n    models = list(hub.list())\n    logger.debug(f\"ListModels() [models={len(models)}]\")\n    return nos_service_pb2.GenericResponse(response_bytes=dumps(models))\n</code></pre>"},{"location":"docs/api/server.html#nos.server._service.InferenceServiceImpl.GetModelCatalog","title":"GetModelCatalog","text":"<pre><code>GetModelCatalog(_: Empty, context: ServicerContext) -&gt; GenericResponse\n</code></pre> <p>Get the model catalog.</p> Source code in <code>nos/server/_service.py</code> <pre><code>def GetModelCatalog(self, _: empty_pb2.Empty, context: grpc.ServicerContext) -&gt; nos_service_pb2.GenericResponse:\n    \"\"\"Get the model catalog.\"\"\"\n    catalog = ModelSpecMetadataCatalog.get()\n    logger.debug(f\"GetModelCatalog() [catalog={catalog._metadata_catalog}]\")\n    return nos_service_pb2.GenericResponse(response_bytes=dumps(catalog))\n</code></pre>"},{"location":"docs/api/server.html#nos.server._service.InferenceServiceImpl.GetModelInfo","title":"GetModelInfo","text":"<pre><code>GetModelInfo(request: StringValue, context: ServicerContext) -&gt; GenericResponse\n</code></pre> <p>Get model information.</p> Source code in <code>nos/server/_service.py</code> <pre><code>def GetModelInfo(\n    self, request: wrappers_pb2.StringValue, context: grpc.ServicerContext\n) -&gt; nos_service_pb2.GenericResponse:\n    \"\"\"Get model information.\"\"\"\n    try:\n        model_id = request.value\n        spec: ModelSpec = hub.load_spec(model_id)\n    except KeyError as e:\n        logger.error(f\"Failed to load spec [request={request}, e={e}]\")\n        context.abort(grpc.StatusCode.NOT_FOUND, str(e))\n    return spec._to_proto()\n</code></pre>"},{"location":"docs/api/server.html#nos.server._service.InferenceServiceImpl.LoadModel","title":"LoadModel","text":"<pre><code>LoadModel(request: GenericRequest, context: ServicerContext) -&gt; Empty\n</code></pre> <p>Load / scale the model to the specified number of replicas.</p> Source code in <code>nos/server/_service.py</code> <pre><code>def LoadModel(self, request: nos_service_pb2.GenericRequest, context: grpc.ServicerContext) -&gt; empty_pb2.Empty:\n    \"\"\"Load / scale the model to the specified number of replicas.\"\"\"\n    request: Dict[str, Any] = loads(request.request_bytes)\n    logger.debug(f\"ScaleModel() [request={request}]\")\n    try:\n        model_id = request[\"id\"]\n        num_replicas = request.get(\"num_replicas\", 1)\n        self.load_model(model_id, num_replicas=num_replicas)\n        return empty_pb2.Empty()\n    except Exception as e:\n        err_msg = f\"Failed to scale model [model_id={model_id}, num_replicas={num_replicas}, e={e}]\"\n        logger.error(err_msg)\n        context.abort(grpc.StatusCode.INTERNAL, err_msg)\n</code></pre>"},{"location":"docs/api/server.html#nos.server._service.InferenceServiceImpl.RegisterSystemSharedMemory","title":"RegisterSystemSharedMemory","text":"<pre><code>RegisterSystemSharedMemory(request: GenericRequest, context: ServicerContext) -&gt; GenericResponse\n</code></pre> <p>Register system shared memory under a specific namespace <code>&lt;client_id&gt;/&lt;object_id&gt;</code>.</p> Source code in <code>nos/server/_service.py</code> <pre><code>def RegisterSystemSharedMemory(\n    self, request: nos_service_pb2.GenericRequest, context: grpc.ServicerContext\n) -&gt; nos_service_pb2.GenericResponse:\n    \"\"\"Register system shared memory under a specific namespace `&lt;client_id&gt;/&lt;object_id&gt;`.\"\"\"\n    if not NOS_SHM_ENABLED:\n        context.abort(grpc.StatusCode.UNIMPLEMENTED, \"Shared memory not enabled.\")\n\n    metadata = dict(context.invocation_metadata())\n    client_id = metadata.get(\"client_id\", None)\n    object_id = metadata.get(\"object_id\", None)\n    namespace = f\"{client_id}/{object_id}\"\n    logger.debug(f\"Registering shm [client_id={client_id}, object_id={object_id}]\")\n    try:\n        # Create a shared memory segment for the inputs\n        # Note: The returned keys for shared memory segments are identical to the\n        # keys in the input dictionary (i.e. &lt;key&gt;), and are not prefixed with the\n        # namespace `&lt;client_id&gt;/&lt;object_id&gt;`.\n        shm_map = self.shm_manager.create(loads(request.request_bytes), namespace=namespace)\n        # Here, dumps() is used to serialize the shared memory numy objects via __getstate__().\n        # The serialized data is then sent back to the client, which can then deserialized\n        # and set via __setstate__() on the client-side, so that the client can access the shared\n        # memory segments.\n        logger.debug(f\"Registered shm [client_id={client_id}, object_id={object_id}, shm_map={shm_map}]\")\n        return nos_service_pb2.GenericResponse(response_bytes=dumps(shm_map))\n    except Exception as e:\n        logger.error(f\"Failed to register system shared memory: {e}\")\n        context.abort(grpc.StatusCode.INTERNAL, str(e))\n</code></pre>"},{"location":"docs/api/server.html#nos.server._service.InferenceServiceImpl.UnregisterSystemSharedMemory","title":"UnregisterSystemSharedMemory","text":"<pre><code>UnregisterSystemSharedMemory(request: GenericRequest, context: ServicerContext) -&gt; GenericResponse\n</code></pre> <p>Unregister system shared memory for specific namespace <code>&lt;client_id&gt;/&lt;object_id&gt;</code>.</p> Source code in <code>nos/server/_service.py</code> <pre><code>def UnregisterSystemSharedMemory(\n    self, request: nos_service_pb2.GenericRequest, context: grpc.ServicerContext\n) -&gt; nos_service_pb2.GenericResponse:\n    \"\"\"Unregister system shared memory for specific namespace `&lt;client_id&gt;/&lt;object_id&gt;`.\"\"\"\n    if not NOS_SHM_ENABLED:\n        context.abort(context, grpc.StatusCode.UNIMPLEMENTED, \"Shared memory not enabled.\")\n\n    metadata = dict(context.invocation_metadata())\n    client_id = metadata.get(\"client_id\", None)\n    object_id = metadata.get(\"object_id\", None)\n    namespace = f\"{client_id}/{object_id}\"\n    # TODO (spillai): Currently, we can ignore the `request` provided\n    # by the client, since all the shared memory segments under the namespace are deleted.\n    logger.debug(f\"Unregistering shm [client_id={client_id}, object_id={object_id}]\")\n    try:\n        self.shm_manager.cleanup(namespace=namespace)\n    except Exception as e:\n        logger.error(f\"Failed to unregister shm [e{e}]\")\n        context.abort(grpc.StatusCode.INTERNAL, str(e))\n    return nos_service_pb2.GenericResponse()\n</code></pre>"},{"location":"docs/api/server.html#nos.server._service.InferenceServiceImpl.UploadFile","title":"UploadFile","text":"<pre><code>UploadFile(request_iterator: Any, context: ServicerContext) -&gt; GenericResponse\n</code></pre> <p>Upload a file.</p> Source code in <code>nos/server/_service.py</code> <pre><code>def UploadFile(self, request_iterator: Any, context: grpc.ServicerContext) -&gt; nos_service_pb2.GenericResponse:\n    \"\"\"Upload a file.\"\"\"\n    for _chunk_idx, chunk_request in enumerate(request_iterator):\n        chunk = loads(chunk_request.request_bytes)\n        chunk_bytes = chunk[\"chunk_bytes\"]\n        path = Path(chunk[\"filename\"])\n        if str(path) not in self._tmp_files:\n            tmp_file = NamedTemporaryFile(delete=False, dir=\"/tmp\", suffix=path.suffix)\n            self._tmp_files[str(path)] = tmp_file\n            logger.debug(\n                f\"Streaming upload [path={tmp_file.name}, size={Path(tmp_file.name).stat().st_size / (1024 * 1024):.2f} MB]\"\n            )\n        else:\n            tmp_file = self._tmp_files[str(path)]\n        with open(tmp_file.name, \"ab\") as f:\n            f.write(chunk_bytes)\n    return nos_service_pb2.GenericResponse(response_bytes=dumps({\"filename\": tmp_file.name}))\n</code></pre>"},{"location":"docs/api/server.html#nos.server._service.InferenceServiceImpl.DeleteFile","title":"DeleteFile","text":"<pre><code>DeleteFile(request: GenericRequest, context: ServicerContext) -&gt; Empty\n</code></pre> <p>Delete a file by its file-identifier.</p> Source code in <code>nos/server/_service.py</code> <pre><code>def DeleteFile(self, request: nos_service_pb2.GenericRequest, context: grpc.ServicerContext) -&gt; empty_pb2.Empty:\n    \"\"\"Delete a file by its file-identifier.\"\"\"\n    request = loads(request.request_bytes)\n\n    filename = str(request[\"filename\"])\n    try:\n        tmp_file = self._tmp_files[str(filename)]\n        path = Path(tmp_file.name)\n        assert path.exists(), f\"File handle {filename} not found\"\n    except Exception as e:\n        err_msg = f\"Failed to delete file [filename={filename}, e={e}]\"\n        logger.error(err_msg)\n        context.abort(grpc.StatusCode.NOT_FOUND, err_msg)\n\n    logger.debug(f\"Deleting file [path={path}]\")\n    path.unlink()\n    del self._tmp_files[str(filename)]\n    return empty_pb2.Empty()\n</code></pre>"},{"location":"docs/api/server.html#nos.server._service.InferenceServiceImpl.Run","title":"Run  <code>async</code>","text":"<pre><code>Run(request: GenericRequest, context: ServicerContext) -&gt; GenericResponse\n</code></pre> <p>Main model prediction interface.</p> Source code in <code>nos/server/_service.py</code> <pre><code>async def Run(\n    self, request: nos_service_pb2.GenericRequest, context: grpc.ServicerContext\n) -&gt; nos_service_pb2.GenericResponse:\n    \"\"\"Main model prediction interface.\"\"\"\n    request: Dict[str, Any] = loads(request.request_bytes)\n    try:\n        st = time.perf_counter()\n        logger.info(f\"Executing request [model={request['id']}, method={request['method']}]\")\n        response = await self.execute_model(request[\"id\"], method=request[\"method\"], inputs=request[\"inputs\"])\n        logger.info(\n            f\"Executed request [model={request['id']}, method={request['method']}, elapsed={(time.perf_counter() - st) * 1e3:.1f}ms]\"\n        )\n        return nos_service_pb2.GenericResponse(response_bytes=dumps(response))\n    except (grpc.RpcError, Exception) as e:\n        msg = f\"Failed to execute request [model={request['id']}, method={request['method']}]\"\n        msg += f\"{traceback.format_exc()}\"\n        logger.error(f\"{msg}, e={e}\")\n        context.abort(grpc.StatusCode.INTERNAL, \"Internal Server Error\")\n</code></pre>"},{"location":"docs/api/server.html#nos.server._service.InferenceServiceImpl.Stream","title":"Stream  <code>async</code>","text":"<pre><code>Stream(request: GenericRequest, context: ServicerContext) -&gt; Iterator[GenericResponse]\n</code></pre> <p>Main streaming model prediction interface.</p> Source code in <code>nos/server/_service.py</code> <pre><code>async def Stream(\n    self, request: nos_service_pb2.GenericRequest, context: grpc.ServicerContext\n) -&gt; Iterator[nos_service_pb2.GenericResponse]:\n    \"\"\"Main streaming model prediction interface.\"\"\"\n    request: Dict[str, Any] = loads(request.request_bytes)\n    try:\n        logger.info(f\"Executing request [model={request['id']}, method={request['method']}]\")\n        response_stream = await self.execute_model(\n            request[\"id\"], method=request[\"method\"], inputs=request[\"inputs\"], stream=True\n        )\n        for response in response_stream:\n            yield nos_service_pb2.GenericResponse(response_bytes=dumps(response))\n        logger.info(f\"Executed request [model={request['id']}, method={request['method']}]\")\n    except (grpc.RpcError, Exception) as e:\n        msg = f\"Failed to execute request [model={request['id']}, method={request['method']}]\"\n        msg += f\"{traceback.format_exc()}\"\n        logger.error(f\"{msg}, e={e}\")\n        context.abort(grpc.StatusCode.INTERNAL, \"Internal Server Error\")\n</code></pre>"},{"location":"docs/api/server.html#nos.server._service.async_serve","title":"nos.server._service.async_serve","text":"<pre><code>async_serve(address: str = DEFAULT_GRPC_ADDRESS, max_workers: int = GRPC_MAX_WORKER_THREADS, wait_for_termination: bool = True, catalog: str = None)\n</code></pre> <p>Start the gRPC server.</p> Source code in <code>nos/server/_service.py</code> <pre><code>def async_serve(\n    address: str = DEFAULT_GRPC_ADDRESS,\n    max_workers: int = GRPC_MAX_WORKER_THREADS,\n    wait_for_termination: bool = True,\n    catalog: str = None,\n):\n    \"\"\"Start the gRPC server.\"\"\"\n    import asyncio\n\n    loop = asyncio.new_event_loop()\n    task = loop.create_task(async_serve_impl(address, wait_for_termination, catalog))\n    loop.run_until_complete(task)\n    return task.result()\n</code></pre>"},{"location":"docs/api/common/exceptions.html","title":"nos.common.exceptions","text":""},{"location":"docs/api/common/exceptions.html#nos.common.exceptions","title":"nos.common.exceptions","text":"<p>Custom exceptions for the nos client.</p>"},{"location":"docs/api/common/exceptions.html#nos.common.exceptions.ClientException","title":"ClientException  <code>dataclass</code>","text":"<p>Base exception for the nos client.</p> Source code in <code>nos/common/exceptions.py</code> <pre><code>@dataclass(frozen=True)\nclass ClientException(Exception):\n    \"\"\"Base exception for the nos client.\"\"\"\n\n    message: str\n    \"\"\"Exception message.\"\"\"\n    exc: Exception = None\n    \"\"\"Exception object.\"\"\"\n\n    def __str__(self) -&gt; str:\n        return f\"{self.message}\"\n</code></pre>"},{"location":"docs/api/common/exceptions.html#nos.common.exceptions.ClientException.message","title":"message  <code>instance-attribute</code>","text":"<pre><code>message: str\n</code></pre> <p>Exception message.</p>"},{"location":"docs/api/common/exceptions.html#nos.common.exceptions.ClientException.exc","title":"exc  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>exc: Exception = None\n</code></pre> <p>Exception object.</p>"},{"location":"docs/api/common/exceptions.html#nos.common.exceptions.ServerReadyException","title":"ServerReadyException  <code>dataclass</code>","text":"<p>Exception raised when the server is not ready.</p> Source code in <code>nos/common/exceptions.py</code> <pre><code>class ServerReadyException(ClientException):\n    \"\"\"Exception raised when the server is not ready.\"\"\"\n\n    def __str__(self) -&gt; str:\n        return f\"Server not ready. {self.message}\"\n</code></pre>"},{"location":"docs/api/common/exceptions.html#nos.common.exceptions.InputValidationException","title":"InputValidationException  <code>dataclass</code>","text":"<p>Exception raised when input validation fails.</p> Source code in <code>nos/common/exceptions.py</code> <pre><code>class InputValidationException(ClientException):\n    \"\"\"Exception raised when input validation fails.\"\"\"\n\n    def __str__(self) -&gt; str:\n        return f\"Input validation failed. {self.message}\"\n</code></pre>"},{"location":"docs/api/common/exceptions.html#nos.common.exceptions.InferenceException","title":"InferenceException  <code>dataclass</code>","text":"<p>Exception raised when inference fails.</p> Source code in <code>nos/common/exceptions.py</code> <pre><code>class InferenceException(ClientException):\n    \"\"\"Exception raised when inference fails.\"\"\"\n\n    def __str__(self) -&gt; str:\n        return f\"Inference failed. {self.message}\"\n</code></pre>"},{"location":"docs/api/common/metaclass.html","title":"nos.common.metaclass","text":""},{"location":"docs/api/common/metaclass.html#nos.common.metaclass","title":"nos.common.metaclass","text":"<p>Common metaclasses for re-use.</p>"},{"location":"docs/api/common/metaclass.html#nos.common.metaclass.SingletonMetaclass","title":"SingletonMetaclass","text":"<p>Singleton metaclass for instantation of a single instance of a class.</p> For example <pre><code>class Foo(metaclass=SingletonMetaclass):\n    ...\n</code></pre> Source code in <code>nos/common/metaclass.py</code> <pre><code>class SingletonMetaclass(type):\n    \"\"\"Singleton metaclass for instantation of a single instance of a class.\n\n    For example:\n        ```python\n        class Foo(metaclass=SingletonMetaclass):\n            ...\n        ```\n    \"\"\"\n\n    _instance = None\n\n    def __call__(cls, *args, **kwargs):\n        \"\"\"Call the class constructor.\"\"\"\n        if cls._instance is None:\n            cls._instance = super().__call__(*args, **kwargs)\n        return cls._instance\n</code></pre>"},{"location":"docs/api/common/metaclass.html#nos.common.metaclass.SingletonMetaclass.__call__","title":"__call__","text":"<pre><code>__call__(*args, **kwargs)\n</code></pre> <p>Call the class constructor.</p> Source code in <code>nos/common/metaclass.py</code> <pre><code>def __call__(cls, *args, **kwargs):\n    \"\"\"Call the class constructor.\"\"\"\n    if cls._instance is None:\n        cls._instance = super().__call__(*args, **kwargs)\n    return cls._instance\n</code></pre>"},{"location":"docs/api/common/shm.html","title":"nos.common.shm","text":""},{"location":"docs/api/common/shm.html#nos.common.shm","title":"nos.common.shm","text":""},{"location":"docs/api/common/shm.html#nos.common.shm.SharedMemoryNumpyObject","title":"SharedMemoryNumpyObject  <code>dataclass</code>","text":"<p>Shared memory object wrapping numpy array.</p> <p>Shared memory objects are updated with user-permissions (0666) under /dev/shm/nos_psm_ and are automatically cleaned up when the object is garbage collected. Source code in <code>nos/common/shm.py</code> <pre><code>@dataclass\nclass SharedMemoryNumpyObject:\n    \"\"\"Shared memory object wrapping numpy array.\n\n    Shared memory objects are updated with user-permissions (0666) under\n    /dev/shm/nos_psm_&lt;random_hex_string&gt; and are automatically cleaned up\n    when the object is garbage collected.\n    \"\"\"\n\n    nbytes: int\n    \"\"\"numpy array nbytes\"\"\"\n    shape: Tuple[int, ...]\n    \"\"\"numpy array shape\"\"\"\n    dtype: np.dtype\n    \"\"\"numpy array dtype\"\"\"\n    mode: str = field(init=False, default=\"r\")\n    \"\"\"Shared memory  mode\"\"\"\n    _shm: SharedMemory = field(init=False, default=None)\n    \"\"\"Shared memory object\"\"\"\n    _shm_arr: np.ndarray = field(init=False, default=None)\n\n    def __post_init__(self) -&gt; None:\n        # Set user-level permissions on the shared memory object\n        self._shm = SharedMemory(name=f\"nos_psm_{secrets.token_hex(8)}\", create=True, size=self.nbytes)\n        # TOFIX (spillai): This is a hack to get around the fact that the shared memory\n        # object is created with the default permissions (0600) and the user running\n        # the inference service is not the same as the user running the client.\n        self._shm._fd = os.dup(self._shm._fd)\n        os.chown(self._shm._fd, 1000, 1000)\n        os.chmod(self._shm._fd, 0o666)\n        self.mode = \"w\"\n        # Create a numpy array view of the shared memory object\n        self._shm_arr = np.ndarray(shape=self.shape, dtype=self.dtype, buffer=self._shm.buf)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the shared memory object representation.\"\"\"\n        return f\"ShmObject(name={self.name}, shape={self.shape}, dtype={self.dtype})\"\n\n    def __getstate__(self) -&gt; Dict[str, Any]:\n        \"\"\"Return the shared memory object state.\n\n        This method is called when the object is pickled (dumps).\n        \"\"\"\n        return {\"name\": self.name, \"shape\": self.shape, \"dtype\": str(self.dtype)}\n\n    def __setstate__(self, state: Dict[str, Any]) -&gt; None:\n        \"\"\"Set the shared memory object state.\n        This method is called when the object is unpickled (loads).\n\n        Args:\n            state (Dict[str, Any]): Shared memory object state.\n        \"\"\"\n        self._shm = SharedMemory(name=state[\"name\"], create=False)\n        self.shape = state[\"shape\"]\n        assert isinstance(state[\"dtype\"], str)\n        self.dtype = np.dtype(state[\"dtype\"])\n        self.mode = \"r\"\n        # Create a numpy array view of the shared memory object\n        self._shm_arr = np.ndarray(shape=self.shape, dtype=self.dtype, buffer=self._shm.buf)\n\n    def cleanup(self) -&gt; None:\n        \"\"\"Close and unlink the shared memory object (server-side / writer).\"\"\"\n        self._shm.close()\n        self._shm.unlink()\n        self._shm_arr = None\n\n    def close(self) -&gt; None:\n        \"\"\"Close the shared memory object (client-side / reader).\"\"\"\n        # Note (spillai): We need to explicitly call `unregister()` here\n        # to avoid the resource tracker from raising a UserWarning about leaked\n        # resources. This is because the shared memory implementation in Python\n        # assumes that all clients of a segment are child processes from a single\n        # parent, and that they inherit the same resource_tracker.\n        self._shm.close()\n        self._shm_arr = None\n        resource_tracker.unregister(self._shm._name, \"shared_memory\")\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Return the shared memory name.\"\"\"\n        return self._shm.name\n\n    def copy_from(self, arr: np.ndarray) -&gt; None:\n        \"\"\"Copy data from the numpy array to shared memory object.\"\"\"\n        assert arr.shape == self.shape, f\"Array shape {arr.shape} does not match shared memory shape {self.shape}\"\n        assert arr.dtype == self.dtype, f\"Array dtype {arr.dtype} does not match shared memory dtype {self.dtype}\"\n        self._shm_arr[:] = arr[:]\n\n    def get(self) -&gt; np.ndarray:\n        \"\"\"Get the numpy array from the shared memory object .\"\"\"\n        return self._shm_arr.copy()\n</code></pre>"},{"location":"docs/api/common/shm.html#nos.common.shm.SharedMemoryNumpyObject.nbytes","title":"nbytes  <code>instance-attribute</code>","text":"<pre><code>nbytes: int\n</code></pre> <p>numpy array nbytes</p>"},{"location":"docs/api/common/shm.html#nos.common.shm.SharedMemoryNumpyObject.shape","title":"shape  <code>instance-attribute</code>","text":"<pre><code>shape: Tuple[int, ...]\n</code></pre> <p>numpy array shape</p>"},{"location":"docs/api/common/shm.html#nos.common.shm.SharedMemoryNumpyObject.dtype","title":"dtype  <code>instance-attribute</code>","text":"<pre><code>dtype: dtype\n</code></pre> <p>numpy array dtype</p>"},{"location":"docs/api/common/shm.html#nos.common.shm.SharedMemoryNumpyObject.mode","title":"mode  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mode: str = field(init=False, default='r')\n</code></pre> <p>Shared memory  mode</p>"},{"location":"docs/api/common/shm.html#nos.common.shm.SharedMemoryNumpyObject.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Return the shared memory name.</p>"},{"location":"docs/api/common/shm.html#nos.common.shm.SharedMemoryNumpyObject.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> <p>Return the shared memory object representation.</p> Source code in <code>nos/common/shm.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the shared memory object representation.\"\"\"\n    return f\"ShmObject(name={self.name}, shape={self.shape}, dtype={self.dtype})\"\n</code></pre>"},{"location":"docs/api/common/shm.html#nos.common.shm.SharedMemoryNumpyObject.__getstate__","title":"__getstate__","text":"<pre><code>__getstate__() -&gt; Dict[str, Any]\n</code></pre> <p>Return the shared memory object state.</p> <p>This method is called when the object is pickled (dumps).</p> Source code in <code>nos/common/shm.py</code> <pre><code>def __getstate__(self) -&gt; Dict[str, Any]:\n    \"\"\"Return the shared memory object state.\n\n    This method is called when the object is pickled (dumps).\n    \"\"\"\n    return {\"name\": self.name, \"shape\": self.shape, \"dtype\": str(self.dtype)}\n</code></pre>"},{"location":"docs/api/common/shm.html#nos.common.shm.SharedMemoryNumpyObject.__setstate__","title":"__setstate__","text":"<pre><code>__setstate__(state: Dict[str, Any]) -&gt; None\n</code></pre> <p>Set the shared memory object state. This method is called when the object is unpickled (loads).</p> <p>Parameters:</p> <ul> <li> <code>state</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>Shared memory object state.</p> </li> </ul> Source code in <code>nos/common/shm.py</code> <pre><code>def __setstate__(self, state: Dict[str, Any]) -&gt; None:\n    \"\"\"Set the shared memory object state.\n    This method is called when the object is unpickled (loads).\n\n    Args:\n        state (Dict[str, Any]): Shared memory object state.\n    \"\"\"\n    self._shm = SharedMemory(name=state[\"name\"], create=False)\n    self.shape = state[\"shape\"]\n    assert isinstance(state[\"dtype\"], str)\n    self.dtype = np.dtype(state[\"dtype\"])\n    self.mode = \"r\"\n    # Create a numpy array view of the shared memory object\n    self._shm_arr = np.ndarray(shape=self.shape, dtype=self.dtype, buffer=self._shm.buf)\n</code></pre>"},{"location":"docs/api/common/shm.html#nos.common.shm.SharedMemoryNumpyObject.cleanup","title":"cleanup","text":"<pre><code>cleanup() -&gt; None\n</code></pre> <p>Close and unlink the shared memory object (server-side / writer).</p> Source code in <code>nos/common/shm.py</code> <pre><code>def cleanup(self) -&gt; None:\n    \"\"\"Close and unlink the shared memory object (server-side / writer).\"\"\"\n    self._shm.close()\n    self._shm.unlink()\n    self._shm_arr = None\n</code></pre>"},{"location":"docs/api/common/shm.html#nos.common.shm.SharedMemoryNumpyObject.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the shared memory object (client-side / reader).</p> Source code in <code>nos/common/shm.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close the shared memory object (client-side / reader).\"\"\"\n    # Note (spillai): We need to explicitly call `unregister()` here\n    # to avoid the resource tracker from raising a UserWarning about leaked\n    # resources. This is because the shared memory implementation in Python\n    # assumes that all clients of a segment are child processes from a single\n    # parent, and that they inherit the same resource_tracker.\n    self._shm.close()\n    self._shm_arr = None\n    resource_tracker.unregister(self._shm._name, \"shared_memory\")\n</code></pre>"},{"location":"docs/api/common/shm.html#nos.common.shm.SharedMemoryNumpyObject.copy_from","title":"copy_from","text":"<pre><code>copy_from(arr: ndarray) -&gt; None\n</code></pre> <p>Copy data from the numpy array to shared memory object.</p> Source code in <code>nos/common/shm.py</code> <pre><code>def copy_from(self, arr: np.ndarray) -&gt; None:\n    \"\"\"Copy data from the numpy array to shared memory object.\"\"\"\n    assert arr.shape == self.shape, f\"Array shape {arr.shape} does not match shared memory shape {self.shape}\"\n    assert arr.dtype == self.dtype, f\"Array dtype {arr.dtype} does not match shared memory dtype {self.dtype}\"\n    self._shm_arr[:] = arr[:]\n</code></pre>"},{"location":"docs/api/common/shm.html#nos.common.shm.SharedMemoryNumpyObject.get","title":"get","text":"<pre><code>get() -&gt; ndarray\n</code></pre> <p>Get the numpy array from the shared memory object .</p> Source code in <code>nos/common/shm.py</code> <pre><code>def get(self) -&gt; np.ndarray:\n    \"\"\"Get the numpy array from the shared memory object .\"\"\"\n    return self._shm_arr.copy()\n</code></pre>"},{"location":"docs/api/common/shm.html#nos.common.shm.SharedMemoryDataDict","title":"SharedMemoryDataDict","text":"<p>Shared-memory data wrapper.</p> Source code in <code>nos/common/shm.py</code> <pre><code>class SharedMemoryDataDict:\n    \"\"\"Shared-memory data wrapper.\"\"\"\n\n    @staticmethod\n    def decode(data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Decode the data dictionary with shared-memory references.\n\n        Note: First unpickle the data bytes, then replace the shared-memory references\n        with numpy arrays. SharedMemoryNumpyObject have a custom __getstate__ method\n        that returns the shared-memory name, shape and dtype. The __setstate__ method\n        is called when the object is unpickled, and it creates a new SharedMemoryNumpyObject\n        instance with the given name, shape and dtype.\n        \"\"\"\n        st = time.perf_counter()\n        data = {k: loads(v) for k, v in data.items()}\n        if NOS_SHM_ENABLED:\n            logger.debug(f\"Loaded shm dict [elapsed={(time.perf_counter() - st) * 1e3:.1f}ms]\")\n            st = time.perf_counter()\n            shm_keys = set()\n            for k, v in data.items():\n                if isinstance(v, SharedMemoryNumpyObject):\n                    data[k] = v.get()\n                    shm_keys.add(k)\n            logger.debug(f\"Decoded shm data [keys={shm_keys}, elapsed={(time.perf_counter() - st) * 1e3:.1f}ms]\")\n        return data\n\n    @staticmethod\n    def encode(data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Encode the data dictionary with shared-memory references.\"\"\"\n        return dumps(data)\n</code></pre>"},{"location":"docs/api/common/shm.html#nos.common.shm.SharedMemoryDataDict.decode","title":"decode  <code>staticmethod</code>","text":"<pre><code>decode(data: Dict[str, Any]) -&gt; Dict[str, Any]\n</code></pre> <p>Decode the data dictionary with shared-memory references.</p> <p>Note: First unpickle the data bytes, then replace the shared-memory references with numpy arrays. SharedMemoryNumpyObject have a custom getstate method that returns the shared-memory name, shape and dtype. The setstate method is called when the object is unpickled, and it creates a new SharedMemoryNumpyObject instance with the given name, shape and dtype.</p> Source code in <code>nos/common/shm.py</code> <pre><code>@staticmethod\ndef decode(data: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Decode the data dictionary with shared-memory references.\n\n    Note: First unpickle the data bytes, then replace the shared-memory references\n    with numpy arrays. SharedMemoryNumpyObject have a custom __getstate__ method\n    that returns the shared-memory name, shape and dtype. The __setstate__ method\n    is called when the object is unpickled, and it creates a new SharedMemoryNumpyObject\n    instance with the given name, shape and dtype.\n    \"\"\"\n    st = time.perf_counter()\n    data = {k: loads(v) for k, v in data.items()}\n    if NOS_SHM_ENABLED:\n        logger.debug(f\"Loaded shm dict [elapsed={(time.perf_counter() - st) * 1e3:.1f}ms]\")\n        st = time.perf_counter()\n        shm_keys = set()\n        for k, v in data.items():\n            if isinstance(v, SharedMemoryNumpyObject):\n                data[k] = v.get()\n                shm_keys.add(k)\n        logger.debug(f\"Decoded shm data [keys={shm_keys}, elapsed={(time.perf_counter() - st) * 1e3:.1f}ms]\")\n    return data\n</code></pre>"},{"location":"docs/api/common/shm.html#nos.common.shm.SharedMemoryDataDict.encode","title":"encode  <code>staticmethod</code>","text":"<pre><code>encode(data: Dict[str, Any]) -&gt; Dict[str, Any]\n</code></pre> <p>Encode the data dictionary with shared-memory references.</p> Source code in <code>nos/common/shm.py</code> <pre><code>@staticmethod\ndef encode(data: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Encode the data dictionary with shared-memory references.\"\"\"\n    return dumps(data)\n</code></pre>"},{"location":"docs/api/common/shm.html#nos.common.shm.SharedMemoryTransportManager","title":"SharedMemoryTransportManager  <code>dataclass</code>","text":"<p>Shared memory transport manager.</p> Source code in <code>nos/common/shm.py</code> <pre><code>@dataclass\nclass SharedMemoryTransportManager:\n    \"\"\"Shared memory transport manager.\"\"\"\n\n    _shm_manager: SharedMemoryManager = field(init=False, default=None)\n    \"\"\"Shared memory manager.\"\"\"\n    _objects_map: Dict[str, Any] = field(default_factory=dict)\n    \"\"\"Shared memory objects map.\"\"\"\n    _shm_counter: int = field(init=False, default=0)\n    \"\"\"Shared memory counter.\"\"\"\n    _max_rate: float = field(init=False, default=10)\n    \"\"\"Maximum shared memory allocation rate.\"\"\"\n    _last_polled: float = field(init=False, default_factory=lambda: time.time())\n    \"\"\"Last time the shared memory allocation rate was polled.\"\"\"\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Initialize the shared memory transport manager.\"\"\"\n        logger.debug(\"Initializing shared memory transport manager\")\n\n    def __del__(self) -&gt; None:\n        \"\"\"Cleanup the shared memory transport manager.\"\"\"\n        logger.debug(\"Cleaning up shared memory transport manager\")\n        self.cleanup()\n\n    def create(self, data: Dict[str, Any], namespace: Optional[str] = None) -&gt; Dict[str, Any]:\n        \"\"\"Create a shared memory segment for the data dictionary.\n\n        Note: The keys for shared memory segments are prefixed with the\n        namespace `&lt;client_id&gt;/&lt;object_id&gt;/&lt;key&gt;`, while the `objects_map`\n        returned does not have the namespace prefixed (i.e. &lt;key&gt;)\n\n        Args:\n            data (Dict[str, Any]): Inputs to the model (\"images\", \"texts\", \"prompts\" etc) as\n                a dictionary of numpy arrays.\n            namespace (str, optional): Unique namespace for the shared memory segment. Defaults to \"\".\n        Returns:\n            Dict[str, Any]: Shared memory segment for the data dictionary.\n        \"\"\"\n        namespace = namespace or \"\"\n\n        # Update the number of shm allocations, and rate-limit\n        self._shm_counter += 1\n        if self._shm_counter % 10 == 0:\n            rate = self._shm_counter / (time.time() - self._last_polled)\n            if rate &gt; self._max_rate:\n                logger.warning(\n                    f\"Shared memory allocation rate is high, check for variable input shapes with every request \"\n                    f\"[allocation calls={self._shm_counter}, rate={rate:.1f} calls/s]\"\n                )\n            self._last_polled = time.time()\n            self._shm_counter = 0\n\n        # Create shared memory segments for numpy arrays (or lists of numpy arrays)\n        objects_map: Dict[str, Any] = {}\n        for key, value in data.items():\n            full_key = f\"{namespace}/{key}\"\n            assert full_key not in self._objects_map, f\"Shared memory segment {full_key} already exists.\"\n\n            if isinstance(value, TensorSpec):\n                objects_map[key] = SharedMemoryNumpyObject(\n                    value.nbytes,\n                    value.shape,\n                    np.dtype(value.dtype),\n                )\n                logger.debug(\n                    f\"Created shm segment [key={full_key}, size={value.nbytes / 1024 / 1024:.2f} MB, shape={value.shape}, dtype={value.dtype}, len=1]\"\n                )\n            else:\n                logger.debug(\"Ignoring non-tensor input\")\n\n        self._objects_map.update({f\"{namespace}/{key}\": value for key, value in objects_map.items()})\n        return objects_map\n\n    def cleanup(self, namespace: Optional[str] = None) -&gt; None:\n        \"\"\"Cleanup the shared memory segments.\"\"\"\n        for key in list(self._objects_map.keys()):\n            logger.debug(f\"Cleaning up shm segment [key={key}]\")\n            if namespace is None or key.startswith(namespace):\n                # Note (spillai): We need to explicitly call `cleanup()` here\n                # as the shared memory segments in order to clean up the shared\n                # memory segments immediately after being unregistered.\n                self._objects_map[key].cleanup()\n                del self._objects_map[key]\n                logger.debug(f\"Removed shm segment [key={key}]\")\n\n    @staticmethod\n    def copy(shm_map: Dict[str, Any], data: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Copy the data dict values to the shared memory segments for transport.\n\n        Args:\n            shm_map (Dict[str, SharedMemoryNumpyObject]): Shared memory segments for the data\n                dict values (destination).\n            data (Dict[str, Any]): Inputs to the model (\"images\", \"texts\", \"prompts\" etc) as\n                numpy arrays or lists of numpy arrays (src).\n        Returns:\n            Dict[str, Any]: Shared memory segments for the data dict values.\n        \"\"\"\n        assert len(shm_map) &gt; 0, \"Shared memory segments should not be empty.\"\n        assert len(data) &gt; 0, \"Data dict should not be empty.\"\n\n        # Copy the data dict values to the shared memory segments i.e. memcpy(dest, src).\n        st = time.perf_counter()\n        for key in shm_map.keys():\n            assert key in data, f\"Key {key} not found in data dict.\"\n            if isinstance(data[key], list):\n                assert len(data[key]) == len(\n                    shm_map[key]\n                ), f\"Shared memory already initialized with length={len(shm_map[key])}, provided input with length={len(data[key])}.\"\n                # Move data from the data dict value to the shared memory segment.\n                for item, shm in zip(data[key], shm_map[key], strict=False):\n                    shm.copy_from(item)\n            elif isinstance(data[key], np.ndarray):\n                # Move data from the data dict value to the shared memory segment.\n                shm_map[key].copy_from(data[key])\n            else:\n                raise ValueError(f\"Unsupported type [type={type(data[key])}]\")\n\n            # Overwrite the data dict value with the shared memory segments for transport.\n            data[key] = shm_map[key]\n        logger.debug(f\"Copied inputs to shm [keys={shm_map.keys()}, elapsed={(time.perf_counter() - st) * 1e3:.1f}ms]\")\n        return data\n</code></pre>"},{"location":"docs/api/common/shm.html#nos.common.shm.SharedMemoryTransportManager.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Initialize the shared memory transport manager.</p> Source code in <code>nos/common/shm.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Initialize the shared memory transport manager.\"\"\"\n    logger.debug(\"Initializing shared memory transport manager\")\n</code></pre>"},{"location":"docs/api/common/shm.html#nos.common.shm.SharedMemoryTransportManager.__del__","title":"__del__","text":"<pre><code>__del__() -&gt; None\n</code></pre> <p>Cleanup the shared memory transport manager.</p> Source code in <code>nos/common/shm.py</code> <pre><code>def __del__(self) -&gt; None:\n    \"\"\"Cleanup the shared memory transport manager.\"\"\"\n    logger.debug(\"Cleaning up shared memory transport manager\")\n    self.cleanup()\n</code></pre>"},{"location":"docs/api/common/shm.html#nos.common.shm.SharedMemoryTransportManager.create","title":"create","text":"<pre><code>create(data: Dict[str, Any], namespace: Optional[str] = None) -&gt; Dict[str, Any]\n</code></pre> <p>Create a shared memory segment for the data dictionary.</p> <p>Note: The keys for shared memory segments are prefixed with the namespace <code>&lt;client_id&gt;/&lt;object_id&gt;/&lt;key&gt;</code>, while the <code>objects_map</code> returned does not have the namespace prefixed (i.e. ) <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>Inputs to the model (\"images\", \"texts\", \"prompts\" etc) as a dictionary of numpy arrays.</p> </li> <li> <code>namespace</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Unique namespace for the shared memory segment. Defaults to \"\".</p> </li> </ul> <p>Returns:     Dict[str, Any]: Shared memory segment for the data dictionary.</p> Source code in <code>nos/common/shm.py</code> <pre><code>def create(self, data: Dict[str, Any], namespace: Optional[str] = None) -&gt; Dict[str, Any]:\n    \"\"\"Create a shared memory segment for the data dictionary.\n\n    Note: The keys for shared memory segments are prefixed with the\n    namespace `&lt;client_id&gt;/&lt;object_id&gt;/&lt;key&gt;`, while the `objects_map`\n    returned does not have the namespace prefixed (i.e. &lt;key&gt;)\n\n    Args:\n        data (Dict[str, Any]): Inputs to the model (\"images\", \"texts\", \"prompts\" etc) as\n            a dictionary of numpy arrays.\n        namespace (str, optional): Unique namespace for the shared memory segment. Defaults to \"\".\n    Returns:\n        Dict[str, Any]: Shared memory segment for the data dictionary.\n    \"\"\"\n    namespace = namespace or \"\"\n\n    # Update the number of shm allocations, and rate-limit\n    self._shm_counter += 1\n    if self._shm_counter % 10 == 0:\n        rate = self._shm_counter / (time.time() - self._last_polled)\n        if rate &gt; self._max_rate:\n            logger.warning(\n                f\"Shared memory allocation rate is high, check for variable input shapes with every request \"\n                f\"[allocation calls={self._shm_counter}, rate={rate:.1f} calls/s]\"\n            )\n        self._last_polled = time.time()\n        self._shm_counter = 0\n\n    # Create shared memory segments for numpy arrays (or lists of numpy arrays)\n    objects_map: Dict[str, Any] = {}\n    for key, value in data.items():\n        full_key = f\"{namespace}/{key}\"\n        assert full_key not in self._objects_map, f\"Shared memory segment {full_key} already exists.\"\n\n        if isinstance(value, TensorSpec):\n            objects_map[key] = SharedMemoryNumpyObject(\n                value.nbytes,\n                value.shape,\n                np.dtype(value.dtype),\n            )\n            logger.debug(\n                f\"Created shm segment [key={full_key}, size={value.nbytes / 1024 / 1024:.2f} MB, shape={value.shape}, dtype={value.dtype}, len=1]\"\n            )\n        else:\n            logger.debug(\"Ignoring non-tensor input\")\n\n    self._objects_map.update({f\"{namespace}/{key}\": value for key, value in objects_map.items()})\n    return objects_map\n</code></pre>"},{"location":"docs/api/common/shm.html#nos.common.shm.SharedMemoryTransportManager.cleanup","title":"cleanup","text":"<pre><code>cleanup(namespace: Optional[str] = None) -&gt; None\n</code></pre> <p>Cleanup the shared memory segments.</p> Source code in <code>nos/common/shm.py</code> <pre><code>def cleanup(self, namespace: Optional[str] = None) -&gt; None:\n    \"\"\"Cleanup the shared memory segments.\"\"\"\n    for key in list(self._objects_map.keys()):\n        logger.debug(f\"Cleaning up shm segment [key={key}]\")\n        if namespace is None or key.startswith(namespace):\n            # Note (spillai): We need to explicitly call `cleanup()` here\n            # as the shared memory segments in order to clean up the shared\n            # memory segments immediately after being unregistered.\n            self._objects_map[key].cleanup()\n            del self._objects_map[key]\n            logger.debug(f\"Removed shm segment [key={key}]\")\n</code></pre>"},{"location":"docs/api/common/shm.html#nos.common.shm.SharedMemoryTransportManager.copy","title":"copy  <code>staticmethod</code>","text":"<pre><code>copy(shm_map: Dict[str, Any], data: Dict[str, Any]) -&gt; Dict[str, Any]\n</code></pre> <p>Copy the data dict values to the shared memory segments for transport.</p> <p>Parameters:</p> <ul> <li> <code>shm_map</code>               (<code>Dict[str, SharedMemoryNumpyObject]</code>)           \u2013            <p>Shared memory segments for the data dict values (destination).</p> </li> <li> <code>data</code>               (<code>Dict[str, Any]</code>)           \u2013            <p>Inputs to the model (\"images\", \"texts\", \"prompts\" etc) as numpy arrays or lists of numpy arrays (src).</p> </li> </ul> <p>Returns:     Dict[str, Any]: Shared memory segments for the data dict values.</p> Source code in <code>nos/common/shm.py</code> <pre><code>@staticmethod\ndef copy(shm_map: Dict[str, Any], data: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Copy the data dict values to the shared memory segments for transport.\n\n    Args:\n        shm_map (Dict[str, SharedMemoryNumpyObject]): Shared memory segments for the data\n            dict values (destination).\n        data (Dict[str, Any]): Inputs to the model (\"images\", \"texts\", \"prompts\" etc) as\n            numpy arrays or lists of numpy arrays (src).\n    Returns:\n        Dict[str, Any]: Shared memory segments for the data dict values.\n    \"\"\"\n    assert len(shm_map) &gt; 0, \"Shared memory segments should not be empty.\"\n    assert len(data) &gt; 0, \"Data dict should not be empty.\"\n\n    # Copy the data dict values to the shared memory segments i.e. memcpy(dest, src).\n    st = time.perf_counter()\n    for key in shm_map.keys():\n        assert key in data, f\"Key {key} not found in data dict.\"\n        if isinstance(data[key], list):\n            assert len(data[key]) == len(\n                shm_map[key]\n            ), f\"Shared memory already initialized with length={len(shm_map[key])}, provided input with length={len(data[key])}.\"\n            # Move data from the data dict value to the shared memory segment.\n            for item, shm in zip(data[key], shm_map[key], strict=False):\n                shm.copy_from(item)\n        elif isinstance(data[key], np.ndarray):\n            # Move data from the data dict value to the shared memory segment.\n            shm_map[key].copy_from(data[key])\n        else:\n            raise ValueError(f\"Unsupported type [type={type(data[key])}]\")\n\n        # Overwrite the data dict value with the shared memory segments for transport.\n        data[key] = shm_map[key]\n    logger.debug(f\"Copied inputs to shm [keys={shm_map.keys()}, elapsed={(time.perf_counter() - st) * 1e3:.1f}ms]\")\n    return data\n</code></pre>"},{"location":"docs/api/common/spec.html","title":"nos.common.spec","text":""},{"location":"docs/api/common/spec.html#nos.common.spec.FunctionSignature","title":"nos.common.spec.FunctionSignature","text":"<p>Function signature that fully describes the remote-model to be executed including <code>inputs</code>, <code>outputs</code>, <code>func_or_cls</code> to be executed, initialization <code>args</code>/<code>kwargs</code>.</p> Source code in <code>nos/common/spec.py</code> <pre><code>class FunctionSignature(BaseModel):\n    \"\"\"Function signature that fully describes the remote-model to be executed\n    including `inputs`, `outputs`, `func_or_cls` to be executed,\n    initialization `args`/`kwargs`.\"\"\"\n\n    func_or_cls: Callable\n    \"\"\"Class instance.\"\"\"\n    method: str\n    \"\"\"Class method name. (e.g. forward, __call__ etc)\"\"\"\n\n    init_args: Tuple[Any, ...] = Field(default_factory=tuple)\n    \"\"\"Arguments to initialize the model instance.\"\"\"\n    init_kwargs: Dict[str, Any] = Field(default_factory=dict)\n    \"\"\"Keyword arguments to initialize the model instance.\"\"\"\n\n    parameters: Dict[str, Any] = Field(init=False, default_factory=dict)\n    \"\"\"Input function signature (as returned by inspect.signature).\"\"\"\n    return_annotation: Any = Field(init=False, default=None)\n    \"\"\"Output / return function signature (as returned by inspect.signature).\"\"\"\n\n    input_annotations: Dict[str, Any] = Field(default_factory=dict)\n    \"\"\"Mapping of input keyword arguments to dtypes.\"\"\"\n    output_annotations: Union[Any, Dict[str, Any], None] = Field(default=None)\n    \"\"\"Mapping of output names to dtypes.\"\"\"\n\n    def __init__(\n        self,\n        func_or_cls: Callable,\n        method: str,\n        init_args: Tuple[Any, ...] = (),\n        init_kwargs: Dict[str, Any] = {},  # noqa: B006\n        parameters: Dict[str, Any] = {},  # noqa: B006\n        return_annotation: Any = None,\n        input_annotations: Dict[str, Any] = {},  # noqa: B006\n        output_annotations: Union[Any, Dict[str, Any], None] = None,\n    ):\n        super().__init__(\n            func_or_cls=func_or_cls,\n            method=method,\n            init_args=init_args,\n            init_kwargs=init_kwargs,\n            parameters=parameters,\n            return_annotation=return_annotation,\n            input_annotations=input_annotations,\n            output_annotations=output_annotations,\n        )\n        if not callable(self.func_or_cls):\n            raise ValueError(f\"Invalid function/class provided, func_or_cls={self.func_or_cls}.\")\n\n        if not self.method or not hasattr(self.func_or_cls, self.method):\n            raise ValueError(f\"Invalid method name provided, method={self.method}.\")\n\n        # Get the function signature\n        sig: Dict[str, inspect.Parameter] = inspect.signature(getattr(self.func_or_cls, self.method))\n\n        # Get the input/output annotations\n        self.parameters = sig.parameters\n        self.return_annotation = sig.return_annotation\n        logger.debug(f\"Function signature [method={self.method}, sig={sig}].\")\n\n    @staticmethod\n    def validate(inputs: Dict[str, Any], sig: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Validate the input dict against the defined signature (input or output).\"\"\"\n        # TOFIX (spillai): This needs to be able to validate using args/kwargs instead\n        if not set(inputs.keys()).issubset(set(sig.keys())):  # noqa: W503\n            raise InputValidationException(\n                f\"Invalid inputs, provided={set(inputs.keys())}, expected={set(sig.keys())}.\"\n            )\n        # TODO (spillai): Validate input types and shapes.\n        return inputs\n\n    @field_validator(\"init_args\", mode=\"before\")\n    @classmethod\n    def _validate_init_args(cls, init_args: Union[Tuple[Any, ...], Any]) -&gt; Tuple[Any, ...]:\n        \"\"\"Validate the initialization arguments.\"\"\"\n        # TODO (spillai): Check the function signature of the func_or_cls class and validate\n        # the init_args against the signature.\n        return init_args\n\n    @field_validator(\"init_kwargs\", mode=\"before\")\n    @classmethod\n    def _validate_init_kwargs(cls, init_kwargs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Validate the initialization keyword arguments.\"\"\"\n        # TODO (spillai): Check the function signature of the func_or_cls class and validate\n        # the init_kwargs against the signature.\n        return init_kwargs\n\n    def _encode_inputs(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Encode inputs based on defined signature.\"\"\"\n        inputs = FunctionSignature.validate(inputs, self.parameters)\n        return {k: dumps(v) for k, v in inputs.items()}\n\n    def _decode_inputs(self, inputs: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Decode inputs based on defined signature.\"\"\"\n        inputs = FunctionSignature.validate(inputs, self.parameters)\n        return {k: loads(v) for k, v in inputs.items()}\n</code></pre>"},{"location":"docs/api/common/spec.html#nos.common.spec.FunctionSignature.func_or_cls","title":"func_or_cls  <code>instance-attribute</code>","text":"<pre><code>func_or_cls: Callable\n</code></pre> <p>Class instance.</p>"},{"location":"docs/api/common/spec.html#nos.common.spec.FunctionSignature.method","title":"method  <code>instance-attribute</code>","text":"<pre><code>method: str\n</code></pre> <p>Class method name. (e.g. forward, call etc)</p>"},{"location":"docs/api/common/spec.html#nos.common.spec.FunctionSignature.init_args","title":"init_args  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>init_args: Tuple[Any, ...] = Field(default_factory=tuple)\n</code></pre> <p>Arguments to initialize the model instance.</p>"},{"location":"docs/api/common/spec.html#nos.common.spec.FunctionSignature.init_kwargs","title":"init_kwargs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>init_kwargs: Dict[str, Any] = Field(default_factory=dict)\n</code></pre> <p>Keyword arguments to initialize the model instance.</p>"},{"location":"docs/api/common/spec.html#nos.common.spec.FunctionSignature.input_annotations","title":"input_annotations  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>input_annotations: Dict[str, Any] = Field(default_factory=dict)\n</code></pre> <p>Mapping of input keyword arguments to dtypes.</p>"},{"location":"docs/api/common/spec.html#nos.common.spec.FunctionSignature.output_annotations","title":"output_annotations  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_annotations: Union[Any, Dict[str, Any], None] = Field(default=None)\n</code></pre> <p>Mapping of output names to dtypes.</p>"},{"location":"docs/api/common/spec.html#nos.common.spec.FunctionSignature.parameters","title":"parameters  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>parameters: Dict[str, Any] = parameters\n</code></pre> <p>Input function signature (as returned by inspect.signature).</p>"},{"location":"docs/api/common/spec.html#nos.common.spec.FunctionSignature.return_annotation","title":"return_annotation  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>return_annotation: Any = return_annotation\n</code></pre> <p>Output / return function signature (as returned by inspect.signature).</p>"},{"location":"docs/api/common/spec.html#nos.common.spec.FunctionSignature.validate","title":"validate  <code>staticmethod</code>","text":"<pre><code>validate(inputs: Dict[str, Any], sig: Dict[str, Any]) -&gt; Dict[str, Any]\n</code></pre> <p>Validate the input dict against the defined signature (input or output).</p> Source code in <code>nos/common/spec.py</code> <pre><code>@staticmethod\ndef validate(inputs: Dict[str, Any], sig: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Validate the input dict against the defined signature (input or output).\"\"\"\n    # TOFIX (spillai): This needs to be able to validate using args/kwargs instead\n    if not set(inputs.keys()).issubset(set(sig.keys())):  # noqa: W503\n        raise InputValidationException(\n            f\"Invalid inputs, provided={set(inputs.keys())}, expected={set(sig.keys())}.\"\n        )\n    # TODO (spillai): Validate input types and shapes.\n    return inputs\n</code></pre>"},{"location":"docs/api/common/spec.html#nos.common.spec.ModelSpec","title":"nos.common.spec.ModelSpec","text":"<p>Model specification for the registry.</p> <p>ModelSpec captures all the relevant information for the instantiation, runtime and execution of a model.</p> Source code in <code>nos/common/spec.py</code> <pre><code>class ModelSpec(BaseModel):\n    \"\"\"Model specification for the registry.\n\n    ModelSpec captures all the relevant information for\n    the instantiation, runtime and execution of a model.\n    \"\"\"\n\n    id: str\n    \"\"\"Model identifier.\"\"\"\n    signature: Dict[str, FunctionSignature] = Field(default_factory=dict)\n    \"\"\"Model function signatures to export (method -&gt; FunctionSignature).\"\"\"\n    runtime_env: Union[RuntimeEnv, None] = Field(default=None)\n    \"\"\"Runtime environment with custom packages.\"\"\"\n\n    def __init__(\n        self, id: str, signature: Dict[str, FunctionSignature] = {}, runtime_env: RuntimeEnv = None  # noqa: B006\n    ):\n        super().__init__(id=id, signature=signature, runtime_env=runtime_env)\n\n    def __repr__(self):\n        return f\"\"\"ModelSpec(id={self.id}, methods=({', '.join(list(self.signature.keys()))}), tasks=({', '.join([str(self.task(m)) for m in self.signature])}))\"\"\"\n\n    @field_validator(\"id\", mode=\"before\")\n    def _validate_id(cls, id: str) -&gt; str:\n        \"\"\"Validate the model identifier.\"\"\"\n        regex = re.compile(r\"^[a-zA-Z0-9\\/._-]+$\")  # allow alphanumerics, `/`, `.`, `_`, and `-`\n        if not regex.match(id):\n            raise ValueError(\n                f\"Invalid model id, id={id} can only contain alphanumerics characters, `/`, `.`, `_`, and `-`\"\n            )\n        return id\n\n    @field_validator(\"signature\", mode=\"before\")\n    def _validate_signature(\n        cls, signature: Union[FunctionSignature, Dict[str, FunctionSignature]], **kwargs: Dict[str, Any]\n    ) -&gt; Dict[str, FunctionSignature]:\n        \"\"\"Validate the model signature / signatures.\n\n        Checks that the model class `cls` has the function name attribute\n        as defined in the signature `function_name`.\n\n        Args:\n            signature (Union[FunctionSignature, Dict[str, FunctionSignature]]): Model signature.\n            **kwargs: Keyword arguments.\n        Returns:\n            Dict[str, FunctionSignature]: Model signature.\n        \"\"\"\n        if isinstance(signature, (list, tuple)):\n            raise TypeError(f\"Invalid signature provided, signature={signature}.\")\n        if isinstance(signature, FunctionSignature):\n            signature = {signature.method: signature}\n        for method, sig in signature.items():\n            if method != sig.method:\n                raise ValueError(f\"Invalid method name provided, method={method}, sig.method={sig.method}.\")\n            if sig and sig.func_or_cls:\n                model_cls = sig.func_or_cls\n                if sig.method and not hasattr(model_cls, sig.method):\n                    raise ValueError(f\"Model class {model_cls} does not have function {sig.method}.\")\n        return signature\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Return the model name (for backwards compatibility).\"\"\"\n        return self.id\n\n    def task(self, method: str = None) -&gt; TaskType:\n        \"\"\"Return the task type for a given method (or defaults to default method).\"\"\"\n        if method is None:\n            method = self.default_method\n        try:\n            md = self.metadata(method)\n            return md.task\n        except Exception:\n            logger.debug(f\"Model metadata not found, id={self.id}.\")\n            return None\n\n    def metadata(self, method: str = None) -&gt; ModelSpecMetadata:\n        \"\"\"Return the model spec metadata for a given method (or defaults to default method).\"\"\"\n        if method is None:\n            method = self.default_method\n        catalog = ModelSpecMetadataCatalog.get()\n        try:\n            metadata: ModelSpecMetadata = catalog._metadata_catalog[f\"{self.id}/{method}\"]\n        except KeyError:\n            logger.debug(f\"Model metadata not found in catalog, id={self.id}, method={method}.\")\n            return ModelSpecMetadata(id=self.id, method=method)\n        return metadata\n\n    @cached_property\n    def default_method(self) -&gt; str:\n        \"\"\"Return the default method name.\"\"\"\n        assert len(self.signature) &gt; 0, f\"No default signature found, signature={self.signature}.\"\n        return list(self.signature.keys())[0]\n\n    @cached_property\n    def default_signature(self) -&gt; FunctionSignature:\n        \"\"\"Return the default function signature.\n\n        Returns:\n            FunctionSignature: Default function signature.\n        \"\"\"\n        # Note (spillai): For now, we assume that the first\n        # signature is the default signature. In the `.from_cls()`\n        # method, we add the __call__ method as the first method\n        # for this exact reason.\n        return self.signature[self.default_method]\n\n    def set_default_method(self, method: str) -&gt; None:\n        \"\"\"Set the default method name.\"\"\"\n        if method not in self.signature:\n            raise ValueError(f\"Invalid method name provided, method={method}.\")\n\n        # Update the default method in the signature\n        signature = {}\n        signature[method] = self.signature.pop(method)\n        signature.update(self.signature)\n        self.signature = signature\n        # Clear the cached properties to force re-computation\n        self.__dict__.pop(\"default_method\", None)\n        self.__dict__.pop(\"default_signature\", None)\n\n    def __call__(self, *init_args, **init_kwargs) -&gt; Any:\n        \"\"\"Create a model instance.\n\n        This method allows us to create a model instance directly\n        from the model spec. Let's consider the example below\n\n            ```\n            class CustomModel:\n                ...\n\n            CustomModel = ModelSpec.from_cls(CustomModel)\n            model = CustomModel(*init_args, **init_kwargs)\n            ```\n\n        Args:\n            *init_args: Positional arguments.\n            **init_kwargs: Keyword arguments.\n        Returns:\n            Any: Model instance.\n        \"\"\"\n        sig: FunctionSignature = self.default_signature\n        return sig.func_or_cls(*init_args, **init_kwargs)\n\n    @classmethod\n    def from_yaml(cls, filename: str) -&gt; \"ModelSpec\":\n        raise NotImplementedError()\n\n    @classmethod\n    def from_cls(\n        cls,\n        func_or_cls: Callable,\n        init_args: Tuple[Any, ...] = (),\n        init_kwargs: Dict[str, Any] = {},  # noqa: B006\n        method: str = \"__call__\",\n        runtime_env: RuntimeEnv = None,\n        model_id: str = None,\n        **kwargs: Any,\n    ) -&gt; \"ModelSpec\":\n        \"\"\"Wrap custom models/classes into a nos-compatible model spec.\n\n        Args:\n            func_or_cls (Callable): Model function or class. For now, only classes are supported.\n            init_args (Tuple[Any, ...]): Initialization arguments.\n            init_kwargs (Dict[str, Any]): Initialization keyword arguments.\n            method (str): Method name to be executed.\n            runtime_env (RuntimeEnv): Runtime environment specification.\n            model_id (str): Optional model identifier.\n            **kwargs: Additional keyword arguments.\n                These include `init_args` and `init_kwargs` to initialize the model instance.\n\n        Returns:\n            ModelSpec: The resulting model specification that fully describes the model execution.\n        \"\"\"\n        # Check if the cls is not a function\n        if not callable(func_or_cls) or not inspect.isclass(func_or_cls):\n            raise ValueError(f\"Invalid class `{func_or_cls}` provided, needs to be a class object.\")\n\n        # Check if the cls has the method_name\n        if not hasattr(func_or_cls, method):\n            raise ValueError(f\"Invalid method name `{method}` provided.\")\n\n        # TODO (spillai): Provide additional RayRuntimeEnvConfig as `config`\n        # config = dict(setup_timeout_seconds=10 * 60, eager_install=True)\n        if runtime_env:\n            logger.debug(f\"Using custom runtime_env [env={runtime_env}]\")\n\n        # Inspect all the public methods of the class\n        # and expose them as model methods\n        ignore_methods = [\"__init__\", method]\n        all_methods = [m for m, _ in inspect.getmembers(func_or_cls, predicate=inspect.isfunction)]\n        methods = [m for m in all_methods if m not in ignore_methods]\n\n        # Note (spillai): See .default_signature property for why we add\n        # the __call__ method as the first method.\n        if method in all_methods:\n            methods.insert(0, method)  # first method is the default method\n        logger.debug(f\"Registering methods [methods={methods}].\")\n\n        # Add function signature for each method\n        model_id: str = func_or_cls.__name__ if model_id is None else model_id\n        signature: Dict[str, FunctionSignature] = {}\n        metadata: Dict[str, ModelSpecMetadata] = {}\n        for method in methods:\n            # Add the function signature\n            sig = FunctionSignature(\n                func_or_cls=func_or_cls,\n                method=method,\n                init_args=init_args,\n                init_kwargs=init_kwargs,\n            )\n            signature[method] = sig\n            metadata[method] = ModelSpecMetadata(model_id, method, task=None)\n            logger.debug(f\"Added function signature [method={method}, signature={sig}].\")\n\n        # Build the model spec from the function signature\n        spec = cls(\n            model_id,\n            signature=signature,\n            runtime_env=runtime_env,\n        )\n        return spec\n\n    def _to_proto(self) -&gt; nos_service_pb2.GenericResponse:\n        \"\"\"Convert the model spec to proto.\"\"\"\n        spec: ModelSpec = loads(dumps(self, protocol=-1))\n        # Note (spillai): We only serialize the input/output\n        # signatures and method of the spec. Notably, the\n        # `func_or_cls` attribute is not serialized to avoid\n        # the dependency on torch and other server-side dependencies.\n        for method in spec.signature:\n            spec.signature[method].func_or_cls = None\n            spec.signature[method].init_args = ()\n            spec.signature[method].init_kwargs = {}\n        return nos_service_pb2.GenericResponse(\n            response_bytes=dumps(spec),\n        )\n\n    @staticmethod\n    def _from_proto(minfo: nos_service_pb2.GenericResponse) -&gt; \"ModelSpec\":\n        \"\"\"Convert the generic response back to the spec.\"\"\"\n        return loads(minfo.response_bytes)\n</code></pre>"},{"location":"docs/api/common/spec.html#nos.common.spec.ModelSpec.id","title":"id  <code>instance-attribute</code>","text":"<pre><code>id: str\n</code></pre> <p>Model identifier.</p>"},{"location":"docs/api/common/spec.html#nos.common.spec.ModelSpec.signature","title":"signature  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>signature: Dict[str, FunctionSignature] = Field(default_factory=dict)\n</code></pre> <p>Model function signatures to export (method -&gt; FunctionSignature).</p>"},{"location":"docs/api/common/spec.html#nos.common.spec.ModelSpec.runtime_env","title":"runtime_env  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>runtime_env: Union[RuntimeEnv, None] = Field(default=None)\n</code></pre> <p>Runtime environment with custom packages.</p>"},{"location":"docs/api/common/spec.html#nos.common.spec.ModelSpec.name","title":"name  <code>property</code>","text":"<pre><code>name: str\n</code></pre> <p>Return the model name (for backwards compatibility).</p>"},{"location":"docs/api/common/spec.html#nos.common.spec.ModelSpec.default_method","title":"default_method  <code>cached</code> <code>property</code>","text":"<pre><code>default_method: str\n</code></pre> <p>Return the default method name.</p>"},{"location":"docs/api/common/spec.html#nos.common.spec.ModelSpec.default_signature","title":"default_signature  <code>cached</code> <code>property</code>","text":"<pre><code>default_signature: FunctionSignature\n</code></pre> <p>Return the default function signature.</p> <p>Returns:</p> <ul> <li> <code>FunctionSignature</code> (              <code>FunctionSignature</code> )          \u2013            <p>Default function signature.</p> </li> </ul>"},{"location":"docs/api/common/spec.html#nos.common.spec.ModelSpec.task","title":"task","text":"<pre><code>task(method: str = None) -&gt; TaskType\n</code></pre> <p>Return the task type for a given method (or defaults to default method).</p> Source code in <code>nos/common/spec.py</code> <pre><code>def task(self, method: str = None) -&gt; TaskType:\n    \"\"\"Return the task type for a given method (or defaults to default method).\"\"\"\n    if method is None:\n        method = self.default_method\n    try:\n        md = self.metadata(method)\n        return md.task\n    except Exception:\n        logger.debug(f\"Model metadata not found, id={self.id}.\")\n        return None\n</code></pre>"},{"location":"docs/api/common/spec.html#nos.common.spec.ModelSpec.metadata","title":"metadata","text":"<pre><code>metadata(method: str = None) -&gt; ModelSpecMetadata\n</code></pre> <p>Return the model spec metadata for a given method (or defaults to default method).</p> Source code in <code>nos/common/spec.py</code> <pre><code>def metadata(self, method: str = None) -&gt; ModelSpecMetadata:\n    \"\"\"Return the model spec metadata for a given method (or defaults to default method).\"\"\"\n    if method is None:\n        method = self.default_method\n    catalog = ModelSpecMetadataCatalog.get()\n    try:\n        metadata: ModelSpecMetadata = catalog._metadata_catalog[f\"{self.id}/{method}\"]\n    except KeyError:\n        logger.debug(f\"Model metadata not found in catalog, id={self.id}, method={method}.\")\n        return ModelSpecMetadata(id=self.id, method=method)\n    return metadata\n</code></pre>"},{"location":"docs/api/common/spec.html#nos.common.spec.ModelSpec.set_default_method","title":"set_default_method","text":"<pre><code>set_default_method(method: str) -&gt; None\n</code></pre> <p>Set the default method name.</p> Source code in <code>nos/common/spec.py</code> <pre><code>def set_default_method(self, method: str) -&gt; None:\n    \"\"\"Set the default method name.\"\"\"\n    if method not in self.signature:\n        raise ValueError(f\"Invalid method name provided, method={method}.\")\n\n    # Update the default method in the signature\n    signature = {}\n    signature[method] = self.signature.pop(method)\n    signature.update(self.signature)\n    self.signature = signature\n    # Clear the cached properties to force re-computation\n    self.__dict__.pop(\"default_method\", None)\n    self.__dict__.pop(\"default_signature\", None)\n</code></pre>"},{"location":"docs/api/common/spec.html#nos.common.spec.ModelSpec.__call__","title":"__call__","text":"<pre><code>__call__(*init_args, **init_kwargs) -&gt; Any\n</code></pre> <p>Create a model instance.</p> <p>This method allows us to create a model instance directly from the model spec. Let's consider the example below</p> <pre><code>```\nclass CustomModel:\n    ...\n\nCustomModel = ModelSpec.from_cls(CustomModel)\nmodel = CustomModel(*init_args, **init_kwargs)\n```\n</code></pre> <p>Parameters:</p> <ul> <li> <code>*init_args</code>           \u2013            <p>Positional arguments.</p> </li> <li> <code>**init_kwargs</code>           \u2013            <p>Keyword arguments.</p> </li> </ul> <p>Returns:     Any: Model instance.</p> Source code in <code>nos/common/spec.py</code> <pre><code>def __call__(self, *init_args, **init_kwargs) -&gt; Any:\n    \"\"\"Create a model instance.\n\n    This method allows us to create a model instance directly\n    from the model spec. Let's consider the example below\n\n        ```\n        class CustomModel:\n            ...\n\n        CustomModel = ModelSpec.from_cls(CustomModel)\n        model = CustomModel(*init_args, **init_kwargs)\n        ```\n\n    Args:\n        *init_args: Positional arguments.\n        **init_kwargs: Keyword arguments.\n    Returns:\n        Any: Model instance.\n    \"\"\"\n    sig: FunctionSignature = self.default_signature\n    return sig.func_or_cls(*init_args, **init_kwargs)\n</code></pre>"},{"location":"docs/api/common/spec.html#nos.common.spec.ModelSpec.from_cls","title":"from_cls  <code>classmethod</code>","text":"<pre><code>from_cls(func_or_cls: Callable, init_args: Tuple[Any, ...] = (), init_kwargs: Dict[str, Any] = {}, method: str = '__call__', runtime_env: RuntimeEnv = None, model_id: str = None, **kwargs: Any) -&gt; ModelSpec\n</code></pre> <p>Wrap custom models/classes into a nos-compatible model spec.</p> <p>Parameters:</p> <ul> <li> <code>func_or_cls</code>               (<code>Callable</code>)           \u2013            <p>Model function or class. For now, only classes are supported.</p> </li> <li> <code>init_args</code>               (<code>Tuple[Any, ...]</code>, default:                   <code>()</code> )           \u2013            <p>Initialization arguments.</p> </li> <li> <code>init_kwargs</code>               (<code>Dict[str, Any]</code>, default:                   <code>{}</code> )           \u2013            <p>Initialization keyword arguments.</p> </li> <li> <code>method</code>               (<code>str</code>, default:                   <code>'__call__'</code> )           \u2013            <p>Method name to be executed.</p> </li> <li> <code>runtime_env</code>               (<code>RuntimeEnv</code>, default:                   <code>None</code> )           \u2013            <p>Runtime environment specification.</p> </li> <li> <code>model_id</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>Optional model identifier.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments. These include <code>init_args</code> and <code>init_kwargs</code> to initialize the model instance.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ModelSpec</code> (              <code>ModelSpec</code> )          \u2013            <p>The resulting model specification that fully describes the model execution.</p> </li> </ul> Source code in <code>nos/common/spec.py</code> <pre><code>@classmethod\ndef from_cls(\n    cls,\n    func_or_cls: Callable,\n    init_args: Tuple[Any, ...] = (),\n    init_kwargs: Dict[str, Any] = {},  # noqa: B006\n    method: str = \"__call__\",\n    runtime_env: RuntimeEnv = None,\n    model_id: str = None,\n    **kwargs: Any,\n) -&gt; \"ModelSpec\":\n    \"\"\"Wrap custom models/classes into a nos-compatible model spec.\n\n    Args:\n        func_or_cls (Callable): Model function or class. For now, only classes are supported.\n        init_args (Tuple[Any, ...]): Initialization arguments.\n        init_kwargs (Dict[str, Any]): Initialization keyword arguments.\n        method (str): Method name to be executed.\n        runtime_env (RuntimeEnv): Runtime environment specification.\n        model_id (str): Optional model identifier.\n        **kwargs: Additional keyword arguments.\n            These include `init_args` and `init_kwargs` to initialize the model instance.\n\n    Returns:\n        ModelSpec: The resulting model specification that fully describes the model execution.\n    \"\"\"\n    # Check if the cls is not a function\n    if not callable(func_or_cls) or not inspect.isclass(func_or_cls):\n        raise ValueError(f\"Invalid class `{func_or_cls}` provided, needs to be a class object.\")\n\n    # Check if the cls has the method_name\n    if not hasattr(func_or_cls, method):\n        raise ValueError(f\"Invalid method name `{method}` provided.\")\n\n    # TODO (spillai): Provide additional RayRuntimeEnvConfig as `config`\n    # config = dict(setup_timeout_seconds=10 * 60, eager_install=True)\n    if runtime_env:\n        logger.debug(f\"Using custom runtime_env [env={runtime_env}]\")\n\n    # Inspect all the public methods of the class\n    # and expose them as model methods\n    ignore_methods = [\"__init__\", method]\n    all_methods = [m for m, _ in inspect.getmembers(func_or_cls, predicate=inspect.isfunction)]\n    methods = [m for m in all_methods if m not in ignore_methods]\n\n    # Note (spillai): See .default_signature property for why we add\n    # the __call__ method as the first method.\n    if method in all_methods:\n        methods.insert(0, method)  # first method is the default method\n    logger.debug(f\"Registering methods [methods={methods}].\")\n\n    # Add function signature for each method\n    model_id: str = func_or_cls.__name__ if model_id is None else model_id\n    signature: Dict[str, FunctionSignature] = {}\n    metadata: Dict[str, ModelSpecMetadata] = {}\n    for method in methods:\n        # Add the function signature\n        sig = FunctionSignature(\n            func_or_cls=func_or_cls,\n            method=method,\n            init_args=init_args,\n            init_kwargs=init_kwargs,\n        )\n        signature[method] = sig\n        metadata[method] = ModelSpecMetadata(model_id, method, task=None)\n        logger.debug(f\"Added function signature [method={method}, signature={sig}].\")\n\n    # Build the model spec from the function signature\n    spec = cls(\n        model_id,\n        signature=signature,\n        runtime_env=runtime_env,\n    )\n    return spec\n</code></pre>"},{"location":"docs/api/common/system.html","title":"nos.common.system","text":""},{"location":"docs/api/common/system.html#nos.common.system","title":"nos.common.system","text":""},{"location":"docs/api/common/system.html#nos.common.system.cpu_info","title":"cpu_info  <code>cached</code>","text":"<pre><code>cpu_info() -&gt; Dict[str, Any]\n</code></pre> <p>Get cached CPU information.</p> Source code in <code>nos/common/system.py</code> <pre><code>@lru_cache(maxsize=1)\ndef cpu_info() -&gt; Dict[str, Any]:\n    \"\"\"Get cached CPU information.\"\"\"\n    return get_cpu_info()\n</code></pre>"},{"location":"docs/api/common/system.html#nos.common.system.sh","title":"sh","text":"<pre><code>sh(command: str) -&gt; None\n</code></pre> <p>Execute shell command, returning stdout.</p> Source code in <code>nos/common/system.py</code> <pre><code>def sh(command: str) -&gt; None:\n    \"\"\"Execute shell command, returning stdout.\"\"\"\n    try:\n        output = subprocess.run(command, shell=True, check=True, capture_output=True, text=True)\n        return output.stdout.strip()\n    except subprocess.CalledProcessError:\n        logger.debug(f\"Failed to execute command: {command}\")\n        return None\n</code></pre>"},{"location":"docs/api/common/system.html#nos.common.system.get_nvidia_smi","title":"get_nvidia_smi","text":"<pre><code>get_nvidia_smi(df: bool = False) -&gt; Optional[Union[str, DataFrame]]\n</code></pre> <p>Get nvidia-smi details, if installed.</p> <p>Parameters:</p> <ul> <li> <code>df</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Return as pandas DataFrame.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optional[Union[str, DataFrame]]</code>           \u2013            <p>nvidia-smi details as string or DataFrame.</p> </li> </ul> Source code in <code>nos/common/system.py</code> <pre><code>def get_nvidia_smi(df: bool = False) -&gt; Optional[Union[str, pd.DataFrame]]:\n    \"\"\"Get nvidia-smi details, if installed.\n\n    Args:\n        df: Return as pandas DataFrame.\n\n    Returns:\n        nvidia-smi details as string or DataFrame.\n    \"\"\"\n    output = sh(\n        \"nvidia-smi --query-gpu=name,driver_version,pcie.link.gen.max,pcie.link.gen.current,memory.total --format=csv\"\n    )\n    if output is None or not df:\n        return output\n    return pd.read_csv(StringIO(output), sep=\", \", engine=\"python\")\n</code></pre>"},{"location":"docs/api/common/system.html#nos.common.system.has_gpu","title":"has_gpu","text":"<pre><code>has_gpu() -&gt; bool\n</code></pre> <p>Check if GPU is available.</p> Source code in <code>nos/common/system.py</code> <pre><code>def has_gpu() -&gt; bool:\n    \"\"\"Check if GPU is available.\"\"\"\n    return get_nvidia_smi() is not None\n</code></pre>"},{"location":"docs/api/common/system.html#nos.common.system.has_docker","title":"has_docker","text":"<pre><code>has_docker() -&gt; bool\n</code></pre> <p>Check if Docker is available.</p> Source code in <code>nos/common/system.py</code> <pre><code>def has_docker() -&gt; bool:\n    \"\"\"Check if Docker is available.\"\"\"\n    return sh(\"docker --version\") is not None\n</code></pre>"},{"location":"docs/api/common/system.html#nos.common.system.docker_compose_command","title":"docker_compose_command","text":"<pre><code>docker_compose_command() -&gt; Optional[str]\n</code></pre> <p>Return the docker-compose command.</p> Source code in <code>nos/common/system.py</code> <pre><code>def docker_compose_command() -&gt; Optional[str]:\n    \"\"\"Return the docker-compose command.\"\"\"\n    if sh(\"docker-compose --version\") is not None:\n        return \"docker-compose\"\n    elif sh(\"docker compose version\") is not None:\n        return \"docker compose\"\n    else:\n        return None\n</code></pre>"},{"location":"docs/api/common/system.html#nos.common.system.has_docker_compose","title":"has_docker_compose","text":"<pre><code>has_docker_compose() -&gt; bool\n</code></pre> <p>Check if Docker Compose is available.</p> Source code in <code>nos/common/system.py</code> <pre><code>def has_docker_compose() -&gt; bool:\n    \"\"\"Check if Docker Compose is available.\"\"\"\n    return docker_compose_command() is not None\n</code></pre>"},{"location":"docs/api/common/system.html#nos.common.system.is_inside_docker","title":"is_inside_docker","text":"<pre><code>is_inside_docker() -&gt; bool\n</code></pre> <p>Check if within Docker.</p> Source code in <code>nos/common/system.py</code> <pre><code>def is_inside_docker() -&gt; bool:\n    \"\"\"Check if within Docker.\"\"\"\n    cgroup = \"/proc/self/cgroup\"\n    return os.path.isfile(\"/.dockerenv\") or os.path.isfile(cgroup) and any(\"docker\" in line for line in open(cgroup))\n</code></pre>"},{"location":"docs/api/common/system.html#nos.common.system.is_apple","title":"is_apple","text":"<pre><code>is_apple() -&gt; bool\n</code></pre> <p>Check if CPU is Apple.</p> Source code in <code>nos/common/system.py</code> <pre><code>def is_apple() -&gt; bool:\n    \"\"\"Check if CPU is Apple.\"\"\"\n    return platform.system() == \"Darwin\"\n</code></pre>"},{"location":"docs/api/common/system.html#nos.common.system.is_apple_silicon","title":"is_apple_silicon","text":"<pre><code>is_apple_silicon() -&gt; bool\n</code></pre> <p>Check if CPU is Apple Silicon.</p> <p>Note (spillai):     &gt;&gt; arch = \"arm64\" if is_apple_silicon() else \"x86_64\"</p> Source code in <code>nos/common/system.py</code> <pre><code>def is_apple_silicon() -&gt; bool:\n    \"\"\"Check if CPU is Apple Silicon.\n\n    Note (spillai):\n        &gt;&gt; arch = \"arm64\" if is_apple_silicon() else \"x86_64\"\n    \"\"\"\n    info = cpu_info()\n    brand = info[\"brand_raw\"]\n    return \"apple m1\" in brand.lower() or \"apple m2\" in brand.lower()\n</code></pre>"},{"location":"docs/api/common/system.html#nos.common.system.is_aws_inf1","title":"is_aws_inf1","text":"<pre><code>is_aws_inf1() -&gt; bool\n</code></pre> <p>Check if instance is AWS Inf1.</p> Source code in <code>nos/common/system.py</code> <pre><code>def is_aws_inf1() -&gt; bool:\n    \"\"\"Check if instance is AWS Inf1.\"\"\"\n    return sh(\"lspci | grep -i Inferentia1\") is not None\n</code></pre>"},{"location":"docs/api/common/system.html#nos.common.system.is_aws_inf2","title":"is_aws_inf2","text":"<pre><code>is_aws_inf2() -&gt; bool\n</code></pre> <p>Check if instance is AWS Inf2.</p> Source code in <code>nos/common/system.py</code> <pre><code>def is_aws_inf2() -&gt; bool:\n    \"\"\"Check if instance is AWS Inf2.\"\"\"\n    return sh(\"lspci | grep -i Inferentia2\") is not None\n</code></pre>"},{"location":"docs/api/common/system.html#nos.common.system.has_nvidia_docker","title":"has_nvidia_docker","text":"<pre><code>has_nvidia_docker() -&gt; bool\n</code></pre> <p>Check if NVIDIA Docker is available.</p> Source code in <code>nos/common/system.py</code> <pre><code>def has_nvidia_docker() -&gt; bool:\n    \"\"\"Check if NVIDIA Docker is available.\"\"\"\n    return sh(\"nvidia-docker\") is not None\n</code></pre>"},{"location":"docs/api/common/system.html#nos.common.system.has_nvidia_docker_runtime_enabled","title":"has_nvidia_docker_runtime_enabled","text":"<pre><code>has_nvidia_docker_runtime_enabled() -&gt; bool\n</code></pre> <p>Check if NVIDIA Docker runtime is available.</p> Source code in <code>nos/common/system.py</code> <pre><code>def has_nvidia_docker_runtime_enabled() -&gt; bool:\n    \"\"\"Check if NVIDIA Docker runtime is available.\"\"\"\n    return sh(\"docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi\") is not None\n</code></pre>"},{"location":"docs/api/common/system.html#nos.common.system.get_torch_info","title":"get_torch_info","text":"<pre><code>get_torch_info() -&gt; Optional[Dict[str, Any]]\n</code></pre> <p>Get torch info, if installed.</p> Source code in <code>nos/common/system.py</code> <pre><code>def get_torch_info() -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Get torch info, if installed.\"\"\"\n    try:\n        import torch\n\n        return {\n            \"version\": torch.__version__,\n        }\n    except ModuleNotFoundError:\n        return None\n</code></pre>"},{"location":"docs/api/common/system.html#nos.common.system.get_torch_cuda_info","title":"get_torch_cuda_info","text":"<pre><code>get_torch_cuda_info() -&gt; Optional[Dict[str, Any]]\n</code></pre> <p>Get torch CUDA info, if installed and available.</p> Source code in <code>nos/common/system.py</code> <pre><code>def get_torch_cuda_info() -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Get torch CUDA info, if installed and available.\"\"\"\n    try:\n        import torch\n\n        assert torch.cuda.is_available()\n        assert torch.cuda.device_count() &gt; 0\n        cuda_info = {\n            \"cuda_version\": torch.version.cuda,\n            \"cudnn_version\": torch.backends.cudnn.version(),\n            \"device_count\": torch.cuda.device_count(),\n            \"devices\": [],\n        }\n\n        # Note (spillai): NVIDIA SMI does not report the same order as torch.cuda\n        # See torch.cuda and PCI_BUS_ID for more details.\n        try:\n            smi_df = get_nvidia_smi(df=True)\n            cuda_info[\"driver_version\"] = smi_df.driver_version[0]\n        except Exception as e:\n            logger.error(f\"Failed to get nvidia-smi details: {e}\")\n            cuda_info[\"driver_version\"] = None\n\n        # Get GPU details via torch.cuda\n        for device_id in range(torch.cuda.device_count()):\n            device = torch.cuda.get_device_properties(device_id)\n            device_info = {\n                \"device_id\": device_id,\n                \"device_name\": device.name,\n                \"device_capability\": f\"{device.major}.{device.minor}\",\n                \"total_memory\": device.total_memory,\n                \"total_memory_str\": f\"{device.total_memory / 1024 / 1024 / 1024:.2f} GB\",\n                \"multi_processor_count\": device.multi_processor_count,\n            }\n            cuda_info[\"devices\"].append(device_info)\n        return cuda_info\n    except (ImportError, AssertionError):\n        return None\n</code></pre>"},{"location":"docs/api/common/system.html#nos.common.system.get_torch_mps_info","title":"get_torch_mps_info","text":"<pre><code>get_torch_mps_info() -&gt; Optional[Dict[str, Any]]\n</code></pre> <p>Get torch MPS info, if installed and available.</p> Source code in <code>nos/common/system.py</code> <pre><code>def get_torch_mps_info() -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Get torch MPS info, if installed and available.\"\"\"\n    try:\n        import torch\n\n        assert torch.backends.mps.is_available()\n        mps_info = {\n            \"is_macos13_or_newer\": torch.backends.mps.is_macos13_or_newer(),\n        }\n        return mps_info\n    except (ImportError, AssertionError):\n        return None\n</code></pre>"},{"location":"docs/api/common/system.html#nos.common.system.get_docker_info","title":"get_docker_info","text":"<pre><code>get_docker_info() -&gt; Optional[Dict[str, Any]]\n</code></pre> <p>Get docker version, if installed.</p> Source code in <code>nos/common/system.py</code> <pre><code>def get_docker_info() -&gt; Optional[Dict[str, Any]]:\n    \"\"\"Get docker version, if installed.\"\"\"\n    version = None\n    try:\n        import docker\n\n        version = docker.__version__\n    except (ImportError, docker.errors.APIError):\n        pass\n\n    compose_version = sh(\"docker compose version\") or sh(\"docker-compose version\")\n    return {\n        \"version\": sh(\"docker --version\"),\n        \"sdk_version\": version,\n        \"compose_version\": compose_version,\n    }\n</code></pre>"},{"location":"docs/api/common/system.html#nos.common.system.get_system_info","title":"get_system_info","text":"<pre><code>get_system_info(docker: bool = False, gpu: bool = False) -&gt; Dict[str, Any]\n</code></pre> <p>Get system information (including CPU, GPU, RAM, etc.)</p> Source code in <code>nos/common/system.py</code> <pre><code>def get_system_info(docker: bool = False, gpu: bool = False) -&gt; Dict[str, Any]:\n    \"\"\"Get system information (including CPU, GPU, RAM, etc.)\"\"\"\n    cpu = cpu_info()\n    vmem = virtual_memory()\n\n    info = {\n        \"system\": {\n            \"system\": platform.system(),\n            \"release\": platform.release(),\n            \"version\": platform.version(),\n            \"machine\": platform.machine(),\n            \"architecture\": platform.architecture(),\n            \"processor\": platform.processor(),\n            \"python_implementation\": platform.python_implementation(),\n        },\n        \"cpu\": {\n            \"model\": cpu[\"brand_raw\"],\n            \"architecture\": cpu[\"arch_string_raw\"],\n            \"cores\": {\n                \"physical\": cpu_count(logical=False),\n                \"total\": cpu_count(logical=True),\n            },\n            \"frequency\": cpu_freq().max,\n            \"frequency_str\": f\"{(cpu_freq().max / 1000):.2f} GHz\",\n        },\n        \"memory\": {\n            \"total\": vmem.total,\n            \"used\": vmem.used,\n            \"available\": vmem.available,\n        },\n    }\n    if docker:\n        info[\"docker\"] = get_docker_info()\n    if gpu:\n        torch_cuda_info = get_torch_cuda_info()\n        torch_mps_info = get_torch_mps_info()\n        if torch_cuda_info is not None:\n            info[\"gpu\"] = torch_cuda_info\n        elif torch_mps_info is not None:\n            info[\"gpu\"] = torch_mps_info\n        else:\n            info[\"gpu\"] = None\n    return info\n</code></pre>"},{"location":"docs/api/common/system.html#nos.common.system.check_runtime_dependencies","title":"check_runtime_dependencies","text":"<pre><code>check_runtime_dependencies(deps: Union[str, List[str]]) -&gt; bool\n</code></pre> <p>Check if runtime dependencies are installed.</p> Source code in <code>nos/common/system.py</code> <pre><code>def check_runtime_dependencies(deps: Union[str, List[str]]) -&gt; bool:\n    \"\"\"Check if runtime dependencies are installed.\"\"\"\n    if isinstance(deps, str):\n        deps = [deps]\n\n    # Check if all dependencies are installed\n    dependency_table = {\"docker\": has_docker, \"docker-compose\": has_docker_compose, \"nvidia-docker\": has_nvidia_docker}\n\n    satisfied = True\n    for dep in deps:\n        if dep not in dependency_table:\n            raise ValueError(f\"Invalid dependency: {dep}, available dependencies: {dependency_table.keys()}\")\n        satisfied &amp;= dependency_table[dep]()\n        if not satisfied:\n            return False\n    return satisfied\n</code></pre>"},{"location":"docs/api/common/tasks.html","title":"nos.common.tasks","text":""},{"location":"docs/api/common/tasks.html#nos.common.tasks","title":"nos.common.tasks","text":""},{"location":"docs/api/common/tasks.html#nos.common.tasks.TaskType","title":"TaskType","text":"<p>Task types.</p> Source code in <code>nos/common/tasks.py</code> <pre><code>class TaskType(str, Enum):\n    \"\"\"Task types.\"\"\"\n\n    OBJECT_DETECTION_2D = \"object_detection_2d\"\n    \"\"\"2D object detection.\"\"\"\n    IMAGE_SEGMENTATION_2D = \"image_segmentation_2d\"\n    \"\"\"2D image segmentation.\"\"\"\n    DEPTH_ESTIMATION_2D = \"depth_estimation_2d\"\n    \"\"\"2D depth estimation.\"\"\"\n\n    IMAGE_CLASSIFICATION = \"image_classification\"\n    \"\"\"Image classification.\"\"\"\n    IMAGE_GENERATION = \"image_generation\"\n    \"\"\"Image generation.\"\"\"\n    IMAGE_SUPER_RESOLUTION = \"image_super_resolution\"\n    \"\"\"Image super-resolution.\"\"\"\n\n    IMAGE_EMBEDDING = \"image_embedding\"\n    \"\"\"Image embedding.\"\"\"\n    TEXT_EMBEDDING = \"text_embedding\"\n    \"\"\"Text embedding.\"\"\"\n\n    TEXT_GENERATION = \"text_generation\"\n    AUDIO_TRANSCRIPTION = \"audio_transcription\"\n\n    CUSTOM = \"custom\"\n    \"\"\"Custom task type.\"\"\"\n</code></pre>"},{"location":"docs/api/common/tasks.html#nos.common.tasks.TaskType.OBJECT_DETECTION_2D","title":"OBJECT_DETECTION_2D  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>OBJECT_DETECTION_2D = 'object_detection_2d'\n</code></pre> <p>2D object detection.</p>"},{"location":"docs/api/common/tasks.html#nos.common.tasks.TaskType.IMAGE_SEGMENTATION_2D","title":"IMAGE_SEGMENTATION_2D  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMAGE_SEGMENTATION_2D = 'image_segmentation_2d'\n</code></pre> <p>2D image segmentation.</p>"},{"location":"docs/api/common/tasks.html#nos.common.tasks.TaskType.DEPTH_ESTIMATION_2D","title":"DEPTH_ESTIMATION_2D  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DEPTH_ESTIMATION_2D = 'depth_estimation_2d'\n</code></pre> <p>2D depth estimation.</p>"},{"location":"docs/api/common/tasks.html#nos.common.tasks.TaskType.IMAGE_CLASSIFICATION","title":"IMAGE_CLASSIFICATION  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMAGE_CLASSIFICATION = 'image_classification'\n</code></pre> <p>Image classification.</p>"},{"location":"docs/api/common/tasks.html#nos.common.tasks.TaskType.IMAGE_GENERATION","title":"IMAGE_GENERATION  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMAGE_GENERATION = 'image_generation'\n</code></pre> <p>Image generation.</p>"},{"location":"docs/api/common/tasks.html#nos.common.tasks.TaskType.IMAGE_SUPER_RESOLUTION","title":"IMAGE_SUPER_RESOLUTION  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMAGE_SUPER_RESOLUTION = 'image_super_resolution'\n</code></pre> <p>Image super-resolution.</p>"},{"location":"docs/api/common/tasks.html#nos.common.tasks.TaskType.IMAGE_EMBEDDING","title":"IMAGE_EMBEDDING  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>IMAGE_EMBEDDING = 'image_embedding'\n</code></pre> <p>Image embedding.</p>"},{"location":"docs/api/common/tasks.html#nos.common.tasks.TaskType.TEXT_EMBEDDING","title":"TEXT_EMBEDDING  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>TEXT_EMBEDDING = 'text_embedding'\n</code></pre> <p>Text embedding.</p>"},{"location":"docs/api/common/tasks.html#nos.common.tasks.TaskType.CUSTOM","title":"CUSTOM  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>CUSTOM = 'custom'\n</code></pre> <p>Custom task type.</p>"},{"location":"docs/api/common/types.html","title":"nos.common.types","text":""},{"location":"docs/api/common/types.html#nos.common.types","title":"nos.common.types","text":""},{"location":"docs/api/common/types.html#nos.common.types.TensorSpec","title":"TensorSpec","text":"Source code in <code>nos/common/types.py</code> <pre><code>@dataclass(frozen=True)\nclass TensorSpec:\n    shape: Optional[Tuple[Optional[int], ...]] = None\n    \"\"\"Base tensor specification with at most 4 dimensions.\n\n    This class is used to capture the shape and dtype of a tensor.\n    Tensor shapes are specified as a tuple of integers, where\n    the first dimension is the batch size. Each of the dimensions\n    are optional, and can be set to None to support dynamic dimension.\n    For e.g., a model that supports variable batch size has\n    shape=(None, 224, 224, 3).\n\n    Examples:\n        ImageSpec:\n            - (H, W, C): (height, width, channels)\n        EmbeddingSpec:\n            - (D): (dims, )\n    \"\"\"\n    dtype: str = None\n    \"\"\"Tensor dtype. (uint8, int32, int64, float32, float64)\"\"\"\n\n    @field_validator(\"shape\")\n    @classmethod\n    def validate_shape(cls, shape: Optional[Tuple[Optional[int], ...]]):\n        \"\"\"Validate the shape.\"\"\"\n        if shape and (len(shape) &lt; 1 or len(shape) &gt; 4):\n            raise ValueError(f\"Invalid tensor shape [shape={shape}].\")\n        else:\n            return shape\n\n    @field_validator(\"dtype\", mode=\"before\")\n    @classmethod\n    def validate_dtype(cls, dtype: str):\n        \"\"\"Validate the dtype.\"\"\"\n        if dtype and not hasattr(np, dtype):\n            raise ValueError(f\"Invalid dtype [dtype={dtype}].\")\n        else:\n            return dtype\n\n    @property\n    def nbytes(self) -&gt; Optional[int]:\n        \"\"\"Return the number of bytes required to store the tensor.\"\"\"\n        try:\n            return np.prod(self.shape) * np.dtype(self.dtype).itemsize\n        except TypeError:\n            return None\n</code></pre>"},{"location":"docs/api/common/types.html#nos.common.types.TensorSpec.shape","title":"shape  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>shape: Optional[Tuple[Optional[int], ...]] = None\n</code></pre> <p>Base tensor specification with at most 4 dimensions.</p> <p>This class is used to capture the shape and dtype of a tensor. Tensor shapes are specified as a tuple of integers, where the first dimension is the batch size. Each of the dimensions are optional, and can be set to None to support dynamic dimension. For e.g., a model that supports variable batch size has shape=(None, 224, 224, 3).</p> <p>Examples:</p> <p>ImageSpec:     - (H, W, C): (height, width, channels) EmbeddingSpec:     - (D): (dims, )</p>"},{"location":"docs/api/common/types.html#nos.common.types.TensorSpec.dtype","title":"dtype  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>dtype: str = None\n</code></pre> <p>Tensor dtype. (uint8, int32, int64, float32, float64)</p>"},{"location":"docs/api/common/types.html#nos.common.types.TensorSpec.nbytes","title":"nbytes  <code>property</code>","text":"<pre><code>nbytes: Optional[int]\n</code></pre> <p>Return the number of bytes required to store the tensor.</p>"},{"location":"docs/api/common/types.html#nos.common.types.TensorSpec.validate_shape","title":"validate_shape  <code>classmethod</code>","text":"<pre><code>validate_shape(shape: Optional[Tuple[Optional[int], ...]])\n</code></pre> <p>Validate the shape.</p> Source code in <code>nos/common/types.py</code> <pre><code>@field_validator(\"shape\")\n@classmethod\ndef validate_shape(cls, shape: Optional[Tuple[Optional[int], ...]]):\n    \"\"\"Validate the shape.\"\"\"\n    if shape and (len(shape) &lt; 1 or len(shape) &gt; 4):\n        raise ValueError(f\"Invalid tensor shape [shape={shape}].\")\n    else:\n        return shape\n</code></pre>"},{"location":"docs/api/common/types.html#nos.common.types.TensorSpec.validate_dtype","title":"validate_dtype  <code>classmethod</code>","text":"<pre><code>validate_dtype(dtype: str)\n</code></pre> <p>Validate the dtype.</p> Source code in <code>nos/common/types.py</code> <pre><code>@field_validator(\"dtype\", mode=\"before\")\n@classmethod\ndef validate_dtype(cls, dtype: str):\n    \"\"\"Validate the dtype.\"\"\"\n    if dtype and not hasattr(np, dtype):\n        raise ValueError(f\"Invalid dtype [dtype={dtype}].\")\n    else:\n        return dtype\n</code></pre>"},{"location":"docs/api/common/types.html#nos.common.types.ImageSpec","title":"ImageSpec","text":"<p>Image tensor specification with dimensions (H, W, C).</p> Source code in <code>nos/common/types.py</code> <pre><code>@dataclass(frozen=True)\nclass ImageSpec(TensorSpec):\n    \"\"\"Image tensor specification with dimensions (H, W, C).\"\"\"\n\n    @field_validator(\"shape\", mode=\"before\")\n    @classmethod\n    def validate_shape(cls, shape: Tuple[Optional[int], ...]):\n        \"\"\"Validate the shape.\"\"\"\n        if shape and len(shape) != 3:\n            raise ValidationError(f\"Invalid image shape [shape={shape}].\")\n        else:\n            return shape\n</code></pre>"},{"location":"docs/api/common/types.html#nos.common.types.ImageSpec.validate_shape","title":"validate_shape  <code>classmethod</code>","text":"<pre><code>validate_shape(shape: Tuple[Optional[int], ...])\n</code></pre> <p>Validate the shape.</p> Source code in <code>nos/common/types.py</code> <pre><code>@field_validator(\"shape\", mode=\"before\")\n@classmethod\ndef validate_shape(cls, shape: Tuple[Optional[int], ...]):\n    \"\"\"Validate the shape.\"\"\"\n    if shape and len(shape) != 3:\n        raise ValidationError(f\"Invalid image shape [shape={shape}].\")\n    else:\n        return shape\n</code></pre>"},{"location":"docs/api/common/types.html#nos.common.types.EmbeddingSpec","title":"EmbeddingSpec","text":"<p>Embedding tensor specification with dimensions (D).</p> Source code in <code>nos/common/types.py</code> <pre><code>@dataclass(frozen=True)\nclass EmbeddingSpec(TensorSpec):\n    \"\"\"Embedding tensor specification with dimensions (D).\"\"\"\n\n    @field_validator(\"shape\", mode=\"before\")\n    @classmethod\n    def validate_shape(cls, shape: Tuple[Optional[int]]):\n        \"\"\"Validate the shape.\"\"\"\n        if shape and len(shape) != 1:\n            raise ValidationError(f\"Invalid embedding shape [shape={shape}].\")\n        else:\n            return shape\n</code></pre>"},{"location":"docs/api/common/types.html#nos.common.types.EmbeddingSpec.validate_shape","title":"validate_shape  <code>classmethod</code>","text":"<pre><code>validate_shape(shape: Tuple[Optional[int]])\n</code></pre> <p>Validate the shape.</p> Source code in <code>nos/common/types.py</code> <pre><code>@field_validator(\"shape\", mode=\"before\")\n@classmethod\ndef validate_shape(cls, shape: Tuple[Optional[int]]):\n    \"\"\"Validate the shape.\"\"\"\n    if shape and len(shape) != 1:\n        raise ValidationError(f\"Invalid embedding shape [shape={shape}].\")\n    else:\n        return shape\n</code></pre>"},{"location":"docs/api/common/types.html#nos.common.types.Batch","title":"Batch","text":"<p>Generic annotation/type-hint for batched data.</p> <p>Inherits from typing.Annotated[T, x] (PEP 593) where T is the type, and x is the metadata. The metadata is tpyically ignored, but can be used to allow additional type checks and annotations on the type.</p> Source code in <code>nos/common/types.py</code> <pre><code>class Batch(Generic[T]):\n    \"\"\"Generic annotation/type-hint for batched data.\n\n    Inherits from typing.Annotated[T, x] (PEP 593) where T is the type,\n    and x is the metadata. The metadata is tpyically ignored,\n    but can be used to allow additional type checks and annotations\n    on the type.\n    \"\"\"\n\n    __slots__ = ()\n\n    @typing._tp_cache\n    def __class_getitem__(cls, params):\n        \"\"\"Support Batch[T, batch_size].\n\n        Annotated requires atleast 2 parameters [type, metadata].\n        Here `batch_size` is optional (i.e. Batch[T],\n        is equivalent to Batch[T, None]).\n        \"\"\"\n        if not isinstance(params, tuple):\n            params = (params, None)\n        if len(params) != 2:\n            raise TypeError(f\"Invalid Batch parameters (T, batch_size), provided params={params}.\")\n        object_type, batch_size = params\n        if batch_size is not None:\n            if isinstance(batch_size, int):\n                if batch_size &lt; 1 or batch_size &gt;= 65536:\n                    raise ValueError(f\"Invalid batch size [batch_size={batch_size}].\")\n            else:\n                raise TypeError(f\"Invalid batch size type [type(batch_size)={type(batch_size)}].\")\n        return Annotated[cls, object_type, batch_size]\n</code></pre>"},{"location":"docs/api/common/types.html#nos.common.types.Batch.__class_getitem__","title":"__class_getitem__","text":"<pre><code>__class_getitem__(params)\n</code></pre> <p>Support Batch[T, batch_size].</p> <p>Annotated requires atleast 2 parameters [type, metadata]. Here <code>batch_size</code> is optional (i.e. Batch[T], is equivalent to Batch[T, None]).</p> Source code in <code>nos/common/types.py</code> <pre><code>@typing._tp_cache\ndef __class_getitem__(cls, params):\n    \"\"\"Support Batch[T, batch_size].\n\n    Annotated requires atleast 2 parameters [type, metadata].\n    Here `batch_size` is optional (i.e. Batch[T],\n    is equivalent to Batch[T, None]).\n    \"\"\"\n    if not isinstance(params, tuple):\n        params = (params, None)\n    if len(params) != 2:\n        raise TypeError(f\"Invalid Batch parameters (T, batch_size), provided params={params}.\")\n    object_type, batch_size = params\n    if batch_size is not None:\n        if isinstance(batch_size, int):\n            if batch_size &lt; 1 or batch_size &gt;= 65536:\n                raise ValueError(f\"Invalid batch size [batch_size={batch_size}].\")\n        else:\n            raise TypeError(f\"Invalid batch size type [type(batch_size)={type(batch_size)}].\")\n    return Annotated[cls, object_type, batch_size]\n</code></pre>"},{"location":"docs/api/common/types.html#nos.common.types.TensorT","title":"TensorT","text":"Source code in <code>nos/common/types.py</code> <pre><code>class TensorT(Generic[T]):\n    __slots__ = ()\n\n    @typing._tp_cache\n    def __class_getitem__(cls, params):\n        \"\"\"Support TensorT[type, tensor_spec].\n\n        Annotated requires atleast 2 parameters [type, tensor_spec].\n        Here `tensor_spec` is optional (i.e. TensorT[T],\n        is equivalent to TensorT[T, None]).\n\n        Examples:\n            TensorT[np.ndarray, TensorSpec()] := Annotated[TensorT, np.ndarray, TensorSpec()]\n            TensorT[torch.Tensor, TensorSpec()] := Annotated[TensorT, torch.Tensor, TensorSpec()]\n        \"\"\"\n        if not isinstance(params, tuple):\n            params = (params, TensorSpec())\n        if len(params) != 2:\n            raise TypeError(f\"Invalid TensorT parameters (T, tensort_spec), provided params={params}.\")\n        object_type, tensor_spec = params\n        if tensor_spec is not None:\n            if not isinstance(tensor_spec, TensorSpec):\n                raise TypeError(f\"Invalid tensor_spec metadata [tensor_spec={type(tensor_spec)}].\")\n        return Annotated[cls, object_type, tensor_spec]\n</code></pre>"},{"location":"docs/api/common/types.html#nos.common.types.TensorT.__class_getitem__","title":"__class_getitem__","text":"<pre><code>__class_getitem__(params)\n</code></pre> <p>Support TensorT[type, tensor_spec].</p> <p>Annotated requires atleast 2 parameters [type, tensor_spec]. Here <code>tensor_spec</code> is optional (i.e. TensorT[T], is equivalent to TensorT[T, None]).</p> <p>Examples:</p> <p>TensorT[np.ndarray, TensorSpec()] := Annotated[TensorT, np.ndarray, TensorSpec()] TensorT[torch.Tensor, TensorSpec()] := Annotated[TensorT, torch.Tensor, TensorSpec()]</p> Source code in <code>nos/common/types.py</code> <pre><code>@typing._tp_cache\ndef __class_getitem__(cls, params):\n    \"\"\"Support TensorT[type, tensor_spec].\n\n    Annotated requires atleast 2 parameters [type, tensor_spec].\n    Here `tensor_spec` is optional (i.e. TensorT[T],\n    is equivalent to TensorT[T, None]).\n\n    Examples:\n        TensorT[np.ndarray, TensorSpec()] := Annotated[TensorT, np.ndarray, TensorSpec()]\n        TensorT[torch.Tensor, TensorSpec()] := Annotated[TensorT, torch.Tensor, TensorSpec()]\n    \"\"\"\n    if not isinstance(params, tuple):\n        params = (params, TensorSpec())\n    if len(params) != 2:\n        raise TypeError(f\"Invalid TensorT parameters (T, tensort_spec), provided params={params}.\")\n    object_type, tensor_spec = params\n    if tensor_spec is not None:\n        if not isinstance(tensor_spec, TensorSpec):\n            raise TypeError(f\"Invalid tensor_spec metadata [tensor_spec={type(tensor_spec)}].\")\n    return Annotated[cls, object_type, tensor_spec]\n</code></pre>"},{"location":"docs/api/common/types.html#nos.common.types.ImageT","title":"ImageT","text":"Source code in <code>nos/common/types.py</code> <pre><code>class ImageT(Generic[T]):\n    __slots__ = ()\n\n    @typing._tp_cache\n    def __class_getitem__(cls, params):\n        \"\"\"Support TensorT[type, image_spec].\n\n        Annotated requires atleast 2 parameters [type, image_spec].\n        Here `image_spec` is optional (i.e. ImageT[T], is equivalent to TensorT[T, ImageSpec()]).\n\n        Examples:\n            ImageT[PIL.Image.Image, ImageSpec()] := Annotated[ImageT, Image, ImageSpec()]\n        \"\"\"\n        if not isinstance(params, tuple):\n            params = (params, ImageSpec())\n        if len(params) != 2:\n            raise TypeError(f\"Invalid ImageT parameters (T, tensort_spec), provided params={params}.\")\n        object_type, image_spec = params\n        if image_spec is not None:\n            if not isinstance(image_spec, ImageSpec):\n                raise TypeError(f\"Invalid image_spec metadata [tensor_spec={type(image_spec)}].\")\n        return Annotated[cls, object_type, image_spec]\n</code></pre>"},{"location":"docs/api/common/types.html#nos.common.types.ImageT.__class_getitem__","title":"__class_getitem__","text":"<pre><code>__class_getitem__(params)\n</code></pre> <p>Support TensorT[type, image_spec].</p> <p>Annotated requires atleast 2 parameters [type, image_spec]. Here <code>image_spec</code> is optional (i.e. ImageT[T], is equivalent to TensorT[T, ImageSpec()]).</p> <p>Examples:</p> <p>ImageT[PIL.Image.Image, ImageSpec()] := Annotated[ImageT, Image, ImageSpec()]</p> Source code in <code>nos/common/types.py</code> <pre><code>@typing._tp_cache\ndef __class_getitem__(cls, params):\n    \"\"\"Support TensorT[type, image_spec].\n\n    Annotated requires atleast 2 parameters [type, image_spec].\n    Here `image_spec` is optional (i.e. ImageT[T], is equivalent to TensorT[T, ImageSpec()]).\n\n    Examples:\n        ImageT[PIL.Image.Image, ImageSpec()] := Annotated[ImageT, Image, ImageSpec()]\n    \"\"\"\n    if not isinstance(params, tuple):\n        params = (params, ImageSpec())\n    if len(params) != 2:\n        raise TypeError(f\"Invalid ImageT parameters (T, tensort_spec), provided params={params}.\")\n    object_type, image_spec = params\n    if image_spec is not None:\n        if not isinstance(image_spec, ImageSpec):\n            raise TypeError(f\"Invalid image_spec metadata [tensor_spec={type(image_spec)}].\")\n    return Annotated[cls, object_type, image_spec]\n</code></pre>"},{"location":"docs/benchmarks/index.html","title":"Index","text":""},{"location":"docs/benchmarks/index.html#flops-reference","title":"FLOPs Reference","text":"<p>Below are the raw TFLOPs of the different GPUs available from cloud providers.</p> Vendor Model Arch FP32 Mixed-precision FP16 Source NVIDIA A100 Ampere 19.5 156 312 Datasheet NVIDIA A10G Ampere 35 35 70 Datasheet NVIDIA A6000 Ampere 38 ? ? Datasheet NVIDIA V100 Volta 14 112 28 Datasheet NVIDIA T4 Turing 8.1 65 ? Datasheet NVIDIA P4 Pascal 5.5 N/A N/A Datasheet NVIDIA P100 Pascal 9.3 N/A 18.7 Datasheet NVIDIA K80 Kepler 8.73 N/A N/A Datasheet NVIDIA A40 Ampere 37 150 150 Datasheet AMD MI250 CDNA 90.5 - 362 Datasheet AMD MI250X CDNA 95.7 184.8 383 Datasheet AWS inf1 Inferentia 16 128 256 Datasheet AWS inf2 Inferentia 32 256 512 Datasheet Habana Gaudi Goya 32 256 512 Datasheet Tenstorrent Grayskull e300 Sparsity 1.6 12.8 25.6 Datasheet Apple M1 - 8 64 128 Datasheet Apple M1 Ultra - 10 80 160 Datasheet Apple M1 Max - 12 96 192 Datasheet Apple M2 - 10 80 160 Datasheet Apple M2 Pro - 12 96 192 Datasheet"},{"location":"docs/blog/index.html","title":"Index","text":""},{"location":"docs/blog/introducing-nos-blog.html","title":"Introducing NOS Blog!","text":"<p>At Autonomi AI, we build infrastructure tools to make AI fast, easy and affordable. We\u2019re in the early development years of the \u201cLinux OS for AI\u201d, where the commoditization of open-source models and tools will be the critical to the safe and ubiquitous use of AI. Needless to say, it\u2019s the most exciting and ambitious infrastructure project our generation is going to witness in the coming decade. </p> <p>A few weeks back, we open-sourced NOS - a fast and flexible inference server for\u00a0PyTorch\u00a0that can run a whole host of open-source AI models (LLMs, Stable Diffusion, CLIP, Whisper, Object Detection etc) all under one-roof. Today, we\u2019re finally excited to launch the NOS blog.</p>","tags":["infra","tools"]},{"location":"docs/blog/introducing-nos-blog.html#why-are-we-building-yet-another-ai-inference-server","title":"\ud83c\udfaf Why are we building yet another AI inference server?","text":"<p>Most inference API implementations today deeply couple the API framework (FastAPI, Flask) with the modeling backend (PyTorch, TF etc) - in other words, it doesn\u2019t let you separate the concerns for the AI backend (e.g. AI hardware, drivers, model compilation, execution runtime, scale out, memory efficiency, async/batched execution, multi-model management etc) from your AI application (e.g. auth, observability, telemetry, web integrations etc), especially if you\u2019re looking to build a production-ready application.</p> <p>We\u2019ve made it very easy for developers to host new PyTorch models as APIs and take them to production without having to worry about any of the backend infrastructure concerns. We build on some awesome projects like FastAPI, Ray, Hugging Face, transformers and diffusers.</p> <p>We\u2019ve been big believers of multi-modal from the very beginning, and you can do all of it with NOS today.\u00a0Give us a \ud83c\udf1f on Github if you're stoked -- NOS can run locally on your Linux desktop (with a gaming GPU), in any cloud GPU (NVIDIA L4, A100s, etc) and even on CPUs. Very soon, we'll support running models on Apple Silicon and custom AI accelerators such as Inferentia2 from Amazon Web Services (AWS).</p> <p>What's coming?</p> <p>Over the coming weeks, we\u2019ll be announcing some awesome features that we believe will make the power of large foundation models more accessible, cheaper and easy-to-use than ever before. </p>","tags":["infra","tools"]},{"location":"docs/blog/introducing-nos-blog.html#nos-in-a-nutshell","title":"\ud83e\udd5c NOS, in a nutshell","text":"<p>NOS was built from the ground-up, with developers in mind. Here are a few things we think developers care about:</p> <ul> <li>\ud83e\udd77 Flexible: Support for OSS models with custom runtimes with pip, conda and cuda/driver dependencies.</li> <li>\ud83d\udd0c Pluggable: Simple API over a high-performance gRPC or REST API that supports batched requests, and streaming.</li> <li>\ud83d\ude80 Scalable: Serve multiple custom models simultaneously on a single or multi-GPU instance, without having to worry about memory management and model scaling.</li> <li>\ud83c\udfdb\ufe0f Local: Local execution means that you control your data, and you\u2019re free to build NOS for domains that are more restrictive with data-privacy.</li> <li>\u2601\ufe0f Cloud-agnostic: Fully containerized means that you can develop, test and deploy NOS locally, on-prem, on any cloud or AI CSP.</li> <li>\ud83d\udce6 Extensible: Written entirely in Python so it\u2019s easily hackable and extensible with an Apache-2.0 License for commercial use.</li> </ul> <p>Go ahead and check out our playground, and try out some of the more recent models with NOS.</p>","tags":["infra","tools"]},{"location":"docs/blog/introducing-nos-blog.html#relevant-links","title":"\ud83d\udd17 Relevant Links","text":"<ul> <li>\u2b50\ufe0f Github: https://github.com/autonomi-ai/nos</li> <li>\ud83d\udc69\u200d\ud83d\udcbb Playground: https://github.com/autonomi-ai/nos-playground</li> <li>\ud83d\udcda Docs: https://docs.nos.run/</li> <li>\ud83d\udcac Discord, X / Twitter, LinkedIn</li> </ul>","tags":["infra","tools"]},{"location":"docs/blog/-getting-started-with-nos-tutorials.html","title":"\ud83d\udcda Getting started with NOS tutorials","text":"<p>We are thrilled to announce a new addition to our resources - the NOS Tutorials! This series of tutorials is designed to empower users with the knowledge and tools needed to leverage NOS for serving models efficiently and effectively. Whether you're a seasoned developer or just starting out, our tutorials offer insights into various aspects of using NOS, making your journey with model serving a breeze.</p> <p>Over the next few weeks, we'll walk you through the process of using NOS to serve models, from the basics to more advanced topics. We'll also cover how to use NOS in a production environment, ensuring you have all the tools you need to take your projects to the next level. Finally, keep yourself updated on NOS by giving us a \ud83c\udf1f on Github.</p> <p>Can't wait? Show me the code!</p> <p>If you can't wait to get started, head over to our tutorials page on Github to dive right in to the code!</p>","tags":["infra","tools","tutorials"]},{"location":"docs/blog/-getting-started-with-nos-tutorials.html#whats-inside-the-nos-tutorials","title":"\ud83c\udf1f What\u2019s Inside the NOS Tutorials?","text":"<p>The NOS Tutorials encompass a wide range of topics, each focusing on different facets of model serving. Here's a sneak peek into what you can expect:</p>","tags":["infra","tools","tutorials"]},{"location":"docs/blog/-getting-started-with-nos-tutorials.html#1-serving-custom-models-01-serving-custom-models","title":"1. Serving custom models: <code>01-serving-custom-models</code>","text":"<p>Dive into the world of custom GPU models with NOS. This tutorial shows you how easy it is to wrap your PyTorch code with NOS, and serve them via a REST / gRPC API.</p>","tags":["infra","tools","tutorials"]},{"location":"docs/blog/-getting-started-with-nos-tutorials.html#2-serving-multiple-methods-02-serving-multiple-methods","title":"2. Serving multiple methods: <code>02-serving-multiple-methods</code>","text":"<p>Learn how to expose several custom methods of a model for serving. This tutorial is perfect for those looking to tailor their model's functionality to specific requirements, enhancing its utility and performance.</p>","tags":["infra","tools","tutorials"]},{"location":"docs/blog/-getting-started-with-nos-tutorials.html#3-serve-llms-with-streaming-support-03-llm-streaming-chat","title":"3. Serve LLMs with streaming support: <code>03-llm-streaming-chat</code>","text":"<p>Get hands-on with serving an LLM with streaming support. This tutorial focuses on using <code>TinyLlama/TinyLlama-1.1B-Chat-v0.1</code>, showcasing how to implement streaming capabilities with NOS for smoother, more efficient language model interactions.</p>","tags":["infra","tools","tutorials"]},{"location":"docs/blog/-getting-started-with-nos-tutorials.html#4-serve-multiple-models-on-the-same-gpu-04-serving-multiple-models","title":"4. Serve multiple models on the same GPU: <code>04-serving-multiple-models</code>","text":"<p>Step up your game by serving multiple models on the same GPU. This tutorial explores the integration of models like <code>TinyLlama/TinyLlama-1.1B-Chat-v0.1</code> and distil-whisper/distil-small.en, enabling multi-modal applications such as audio transcription combined with summarization on a single GPU.</p>","tags":["infra","tools","tutorials"]},{"location":"docs/blog/-getting-started-with-nos-tutorials.html#5-serving-models-in-production-with-docker-05-serving-with-docker","title":"5. Serving models in production with Docker <code>05-serving-with-docker</code>","text":"<p>Enter the realm of production environments with our Docker tutorial. This guide is essential for anyone looking to use NOS in a more structured, scalable environment. You'll learn how to deploy your production NOS images with Docker and Docker Compose, ensuring your model serving works with existing ML infrastructure as reliably as possible.</p> <p>Stay tuned!</p> <p>\ud83d\udd17 Stay tuned, as we'll continuously update the section with more tutorials and resources to keep you ahead in the ever-evolving world of model serving!</p> <p>Happy Model Serving!</p> <p>This blog post is brought to you by the NOS Team - committed to making model serving fast, efficient, and accessible to all!</p>","tags":["infra","tools","tutorials"]},{"location":"docs/blog/serving-llms-on-a-budget.html","title":"Serving LLMs on a budget","text":"<p>Deploying Large Language Models (LLMs) and Mixture of Experts (MoEs) are all the rage today, and for good reason. They are the most powerful and closest open-source models in terms of performance to OpenAI GPT-3.5 today. However, it turns out that deploying these models can still be somewhat of a lift for most ML engineers and researchers, both in terms of engineering work and operational costs. For example, the recently announced Mixtral 8x7B requires 2x NVIDIA A100-80G GPUs, which can cost upwards of $5000 / month (on-demand) on CSPs.</p> <p>With recent advancements in model compression, quantization and model mixing, we are now seeing an exciting race unfold to deploy these expert models on a budget, without sacrificing significantly on performance. In this blog post, we'll show you how to deploy the mlabonne/phixtral-4x2_8 model on a single NVIDIA L4 GPU for under $160 / month and easily scale-out a dirt-cheap, dedicated inference service of your own. We'll be using SkyPilot to deploy and manage our NOS service on spot (pre-emptible) instances, making them especially cost-efficient.</p>","tags":["llm","integrations","tutorials","budget"]},{"location":"docs/blog/serving-llms-on-a-budget.html#what-is-phixtral","title":"\ud83e\udde0 What is Phixtral?","text":"<p>Inspired inspired by the mistralai/Mixtral-8x7B-v0.1 architecture, mlabonne/phixtral-4x2_8 is the first Mixure of Experts (MoE) made with 4 microsoft/phi-2 models that was recently MIT licensed. The general idea behind mixture-of-experts is to combine the capabilities of multiple models to achieve better performance than each individual model. They are significantly more memory-efficient for inference too, but that's a post for a later date. In this case, we combine the capabilities of 4 microsoft/phi-2 models to achieve better performance than each of the individual 2.7B parameter models it's composed of. </p> Breakdown of the mlabonne/phixtral-4x2_8 model <p>Here's the breakdown of the 4 models that make up the mlabonne/phixtral-4x2_8 model:</p> <pre><code>base_model: cognitivecomputations/dolphin-2_6-phi-2\ngate_mode: cheap_embed\nexperts:\n  - source_model: cognitivecomputations/dolphin-2_6-phi-2\n    positive_prompts: [\"\"]\n  - source_model: lxuechen/phi-2-dpo\n    positive_prompts: [\"\"]\n  - source_model: Yhyu13/phi-2-sft-dpo-gpt4_en-ep1\n    positive_prompts: [\"\"]\n  - source_model: mrm8488/phi-2-coder\n    positive_prompts: [\"\"]\n</code></pre> <p>You can go to the original model card here for more details on how the model was merged using mergekit.</p> <p>Now, let's take a look at the performance of the mlabonne/phixtral-4x2_8 model on the Nous Suite compared to other models in the 2.7B parameter range. </p> Model AGIEval GPT4All TruthfulQA Bigbench Average mlabonne/phixtral-4x2_8 33.91 70.44 48.78 37.82 47.78 dolphin-2_6-phi-2 33.12 69.85 47.39 37.2 46.89 phi-2-dpo 30.39 71.68 50.75 34.9 46.93 phi-2 27.98 70.8 44.43 35.21 44.61","tags":["llm","integrations","tutorials","budget"]},{"location":"docs/blog/serving-llms-on-a-budget.html#serving-phixtral-on-a-budget-with-skypilot-and-nos","title":"\ud83d\udcb8 Serving Phixtral on a budget with SkyPilot and NOS","text":"<p>Let's now see how we can deploy the mlabonne/phixtral-4x2_8 model on a single NVIDIA L4 GPU for under $160 / month. We'll be using SkyPilot to deploy and manage our NOS service on spot (pre-emptible) instances, making them especially cost-efficient.</p> <p>What's SkyPilot?</p> <p>If you're new to SkyPilot, we recommend you go through our NOS x SkyPilot integration page first to familiarize yourself with the tool.</p>","tags":["llm","integrations","tutorials","budget"]},{"location":"docs/blog/serving-llms-on-a-budget.html#1-define-your-custom-model-and-serve-specification","title":"1. Define your custom model and serve specification","text":"<p>In this example, we'll be using the <code>llm-streaming-chat</code> tutorial on NOS playground. First, we'll define our custom phixtral chat model <code>phixtral_chat.py</code> and a <code>serve.phixtral.yaml</code> serve specification that will be used by NOS to serve our model. The relevant files are shown below:</p> <pre><code>(nos-py38) nos-playground/examples/llm-streaming-chat $ tree .\n\u251c\u2500\u2500 models\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 phixtral_chat.py\n\u251c\u2500\u2500 serve.phixtral.yaml\n</code></pre> <p>The entire chat interface is defined in the <code>StreamingChat</code> module in <code>phixtral_chat.py</code>, where the <code>chat</code> method returns a string iterable for the gRPC / HTTP server to stream back model predictions to the client. </p> <p>The <code>serve.phixtral.yaml</code> serve specification defines the custom chat model, and a custom runtime that NOS uses to execute our model. Follow the annotations below to understand the different components of the serve specification.</p> serve.phixtral.yaml<pre><code>images: (1)\n  llm-py310-cu121: (2)\n    base: autonomi/nos:latest-py310-cu121 (3)\n    pip: (4)\n      - bitsandbytes\n      - transformers\n      - einops\n      - accelerate\n\nmodels: (5)\n  mlabonne/phixtral-4x2_8: (6)\n    model_cls: StreamingChat (7)\n    model_path: models/phixtral_chat.py (8)\n    init_kwargs:\n      model_name: mlabonne/phixtral-4x2_8\n    default_method: chat\n    runtime_env: llm-gpu\n    deployment: (9)\n      resources:\n        device: auto\n        device_memory: 7Gi (10)\n</code></pre> <ol> <li>Specifies the custom runtime images that will be used to serve our model.</li> <li>Specifies the name of the custom runtime image (referenced below in <code>runtime_env</code>). </li> <li>Specifies the base NOS image to use for the custom runtime image. We provide a few pre-built images on dockerhub. </li> <li>Specifies the pip dependencies to install in the custom runtime image.</li> <li>Specifies all the custom models we intend to serve.</li> <li>Specifies the unique name of the custom model (model identifier).</li> <li>Specifies the model class to use for the custom model.</li> <li>Specifies the path to the model class definition.</li> <li>Specifies the deployment resources needed for the custom model.</li> <li>Specifies the GPU memory to allocate for the custom model.</li> </ol>","tags":["llm","integrations","tutorials","budget"]},{"location":"docs/blog/serving-llms-on-a-budget.html#2-test-your-custom-model-locally-with-nos","title":"2. Test your custom model locally with NOS","text":"<p>In order to start the NOS server locally, we can simply run the following command:</p> <pre><code>nos serve up -c serve.phixtral.yaml --http\n</code></pre> <p>This will build the custom runtime image, and start the NOS server locally, exposing an OpenAI compatible HTTP proxy on port <code>8000</code>. This will allow you to chat with your custom LLM endpoint using any OpenAI API compatible client. </p>","tags":["llm","integrations","tutorials","budget"]},{"location":"docs/blog/serving-llms-on-a-budget.html#3-deploy-your-nos-service-with-skypilot","title":"3. Deploy your NOS service with SkyPilot","text":"<p>Now that we have defined our serve YAML specification, let's deploy this service on GCP using SkyPilot. In this example, we're going to use SkyPilot's <code>sky serve</code> command to deploy our NOS service on spot (pre-emptible) instances on GCP. </p> Deploy on any cloud provider (AWS, Azure, GCP, OCI, Lambda Labs, etc.) <p>SkyPilot supports deploying NOS services on any cloud provider. In this example, we're going to use GCP, but you can easily deploy on AWS, Azure, or any other cloud provider of your choice. You can override <code>gcp</code> by providing the <code>--cloud</code> flag to <code>sky serve up</code>.</p> <p>Let's define a serving configuration in a <code>service-phixtral.sky.yaml</code> file. This YAML specification will be used by SkyPilot to deploy and manage our NOS service on pre-emptible instances, automatically provisioning and recovering from failovers, setting up new instances when needed on server pre-emptions. </p> service-phixtral.sky.yaml<pre><code>name: service-phixtral\n\nfile_mounts:\n  /app: ./app (1)\n\nresources:\n  cloud: gcp\n  accelerators: L4:1\n  use_spot: True (2)\n  ports:\n    - 8000\n\nservice:\n  readiness_probe: (3)\n    path: /v1/health \n  replicas: 2 (4)\n\nsetup: |\n  sudo apt-get install -y docker-compose-plugin\n  pip install torch-nos\n\nrun: |\n  cd /app &amp;&amp; nos serve up -c serve.phixtral.yaml --http (5)\n</code></pre> <ol> <li>Setup file-mounts to mount the local <code>./app</code> directory so that the <code>serve.phixtral.yaml</code> and <code>models/</code> directory are available on the remote instance.</li> <li>Use spot (pre-emptible) instances instead of on-demand instances.</li> <li>Define the readiness probe path for the service. This allows the SkyPilot controller to check the health of the service and recover from failures if needed.</li> <li>Define the number of replicas to deploy.</li> <li>Define the <code>run</code> command to execute on each replica. In this case, we're simply starting the NOS server with the phixtral model deployed on init. </li> </ol> <p>To deploy our NOS service, we can simply run the following command:</p> <pre><code>sky serve up -n service-mixtral service-mixtral.sky.yaml\n</code></pre> <p>SkyPilot will automatically pick the cheapest region and zone to deploy our service, and provision the necessary cloud resources to deploy the NOS server. In this case, you'll notice that SkyPilot provisioned two NVIDIA L4 GPU instances on GCP in the <code>us-central1-a</code> availability zone. </p> <p>You should see the following output:</p> <pre><code>(nos-infra-py38) deployments/deploy-llms-with-skypilot $ sky serve up -n service-mixtral service-mixtral.sky.yaml\nService from YAML spec: service-mixtral.sky.yaml\nService Spec:\nReadiness probe method:           GET /v1/health\nReadiness initial delay seconds:  1200\nReplica autoscaling policy:       Fixed 2 replicas\nReplica auto restart:             True\nEach replica will use the following resources (estimated):\nI 01-19 16:01:58 optimizer.py:694] == Optimizer ==\nI 01-19 16:01:58 optimizer.py:705] Target: minimizing cost\nI 01-19 16:01:58 optimizer.py:717] Estimated cost: $0.2 / hour\nI 01-19 16:01:58 optimizer.py:717]\nI 01-19 16:01:58 optimizer.py:840] Considered resources (1 node):\nI 01-19 16:01:58 optimizer.py:910] ----------------------------------------------------------------------------------------------------\nI 01-19 16:01:58 optimizer.py:910]  CLOUD   INSTANCE              vCPUs   Mem(GB)   ACCELERATORS   REGION/ZONE     COST ($)   CHOSEN\nI 01-19 16:01:58 optimizer.py:910] ----------------------------------------------------------------------------------------------------\nI 01-19 16:01:58 optimizer.py:910]  GCP     g2-standard-4[Spot]   4       16        L4:1           us-central1-a   0.22          \u2714\nI 01-19 16:01:58 optimizer.py:910] ----------------------------------------------------------------------------------------------------\nI 01-19 16:01:58 optimizer.py:910]\nLaunching a new service 'service-mixtral'. Proceed? [Y/n]: y\nLaunching controller for 'service-mixtral'\n...\nI 01-19 16:02:14 cloud_vm_ray_backend.py:1912] Launching on GCP us-west1 (us-west1-a)\nI 01-19 16:02:30 log_utils.py:45] Head node is up.\nI 01-19 16:03:03 cloud_vm_ray_backend.py:1717] Successfully provisioned or found existing VM.\nI 01-19 16:03:05 cloud_vm_ray_backend.py:4558] Processing file mounts.\n...\nI 01-19 16:03:20 cloud_vm_ray_backend.py:3325] Setup completed.\nI 01-19 16:03:29 cloud_vm_ray_backend.py:3422] Job submitted with Job ID: 11\n\nService name: service-mixtral\nEndpoint URL: XX.XXX.X.XXX:30001\nTo see detailed info:           sky serve status service-mixtral [--endpoint]\nTo teardown the service:        sky serve down service-mixtral\n\nTo see logs of a replica:       sky serve logs service-mixtral [REPLICA_ID]\nTo see logs of load balancer:   sky serve logs --load-balancer service-mixtral\nTo see logs of controller:      sky serve logs --controller service-mixtral\n\nTo monitor replica status:      watch -n10 sky serve status service-mixtral\nTo send a test request:         curl -L XX.XX.X.XXX:30001\n</code></pre> <p>Once the service is deployed, you can get the IP address of the SkyPilot service via:. </p> <pre><code>sky serve status service-phixtral --endpoint\n</code></pre> <p>We'll refer to <code>&lt;sky-serve-ip&gt;</code> as the load balancer's IP address, that takes the full form of <code>&lt;sky-serve-ip&gt;:30001</code>. You should now be able to ping the load-balancer endpoint directly with <code>cURL</code> and see the following output:</p> <pre><code>$ curl -L http://&lt;sky-serve-ip&gt;:30001/v1/health\n{\"status\":\"ok\"}\n</code></pre>","tags":["llm","integrations","tutorials","budget"]},{"location":"docs/blog/serving-llms-on-a-budget.html#chatting-with-your-custom-phixtral-service","title":"\ud83d\udcac Chatting with your custom Phixtral service","text":"<p>You're now ready to chat with your hosted, custom LLM endpoint! Here's a quick demo of the mlabonne/phixtral-4x2_8 model served with NOS across 2 spot (pre-emptible) instances. </p> <p>On the top, you'll see the logs from both the serve replicas, and the corresponding chats that are happening concurrently on the bottom. SkyPilot handles the load-balancing and routing of requests to the replicas, while NOS handles the custom model serving and streaming inference. Below, we show you how you can chat with your hosted LLM endpoint using <code>cURL</code>, an OpenAI compatible client, or the OpenAI Python client.</p> Using cURLUsing an OpenAI compatible clientUsing the OpenAI Python client <pre><code>curl \\\n-X POST -L http://&lt;sky-serve-ip&gt;:30001/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"model\": \"mlabonne/phixtral-4x2_8\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Tell me a joke in 300 words\"}],\n    \"temperature\": 0.7, \"stream\": true\n  }'\n</code></pre> <p>Below, we show how you can use any OpenAI API compatible client to chat with your hosted LLM endpoint. We will use the popular llm CLI tool from Simon Willison to chat with our hosted LLM endpoint.</p> <pre><code># Install the llm CLI tool\n$ pip install llm\n\n# Install the llm-nosrun plugin to talk to your service\n$ llm install llm-nosrun\n\n# List the models\n$ llm models list\n\n# Chat with your endpoint\n$ NOSRUN_API_BASE=http://&lt;sky-serve-ip&gt;:30001/v1 llm -m mlabonne/phixtral-4x2_8 \"Tell me a joke in 300 words\"\n</code></pre> <p>Below, we show how you can use the OpenAI Python Client to chat with your hosted LLM endpoint.</p> <pre><code>import openai\n\n# Create a stream and print the output\nclient = openai.OpenAI(api_key=\"no-key-required\", base_url=f\"http://&lt;sky-serve-ip&gt;:30001/v1\")\nstream = client.chat.completions.create(\n    model=\"mlabonne/phixtral-4x2_8\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a joke in 300 words\"}],\n    stream=True,\n)\nfor chunk in stream:\n    print(chunk.choices[0].delta.content or \"\", end=\"\")\n</code></pre> <p>Info</p> <p>In these examples, we use SkyPilot's load-balancer port <code>30001</code>  which redirects HTTP traffic to one of the many NOS replicas (on port <code>8000</code>) in a round-robin fashion. This allows us to scale-out our service to multiple replicas, and load-balance requests across them.</p>","tags":["llm","integrations","tutorials","budget"]},{"location":"docs/blog/serving-llms-on-a-budget.html#whats-it-going-to-cost-me","title":"\ud83e\udd11 What's it going to cost me?","text":"<p>In the example above, we were able to deploy the mlabonne/phixtral-4x2_8 model on a single NVIDIA L4 GPU for $0.22 / hour / replica, or $160 / month / replica. This is a 45x improvement over the cost of deploying the mistralai/Mixtral-8x7B-v0.1 model on 2x NVIDIA A100-80G GPUs, which can cost upwards of $7000 / month (on-demand) on CSPs. As advancements in model compression, quantization and model mixing continue to improve, we expect more users to be able to fine-tune, distill and deploy these expert small-langauge models on a budget, without sacrificing significantly on performance.</p> <p>The table below shows the costs of deploying one of these popular MoE LLM models on a single GPU server on GCP. As you can see, the cost of deploying a single model can range from $500 to $7300 / month, depending on the model and of course CSP (kept fixed here). </p> Model Cloud Provider GPU VRAM Spot Cost / hr Cost / month mistralai/Mixtral-8x7B-v0.1 GCP 2x NVIDIA A100-80G ~94\u202fGB - $10.05 ~$7236 TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ GCP NVIDIA A100-40G ~25GB - $3.67 ~$2680 mlabonne/phixtral-4x2_8 GCP NVIDIA L4 ~9GB - $0.70 ~$500 mlabonne/phixtral-4x2_8 GCP NVIDIA L4 ~9GB \u2705 $0.22 ~$160 <p>However, the onus is on the developer to figure out the right instance type, spot instance strategy, and the right number of replicas to deploy to ensure that the service is both cost-efficient and performant. In the coming weeks, we're going to be introducing some exciting tools to help developers alleviate this pain and provide transparency to make the right infrastructure decisions for their services. Stay tuned!</p>","tags":["llm","integrations","tutorials","budget"]},{"location":"docs/blog/serving-llms-on-a-budget.html#wrapping-up","title":"\ud83c\udf81 Wrapping up","text":"<p>In this blog post, we showed you how to deploy the mlabonne/phixtral-4x2_8 model on a single NVIDIA L4 GPU for under $160 / month and scale-out a dirt-cheap inference service of your own. We used SkyPilot to deploy and manage our NOS service on spot (pre-emptible) instances, making them especially cost-efficient.</p> <p>In our next blog post, we\u2019ll take it one step further. We'll explore how you can serve multiple models on the same GPU so that your infrastructure costs don\u2019t have to scale with the number of models you serve. The TL;DR is that you will soon be able to serve multiple models with fixed and predictable pricing, making model serving more accessible and cost-efficient than ever before.</p> <p></p>","tags":["llm","integrations","tutorials","budget"]},{"location":"docs/blog/introducing-the-nos-inferentia2-runtime.html","title":"Introducing the NOS Inferentia2 Runtime","text":"<p>We are excited to announce the availability of the AWS Inferentia2 runtime on NOS - a.k.a. our <code>inf2</code> runtime. This runtime is designed to easily serve models on AWS Inferentia2, a high-performance, purpose-built chip for inference. In this blog post, we will introduce the AWS Inferentia2 runtime, and show you how to trivially deploy a model on the AWS Inferentia2 device using NOS. If you have followed our previous tutorial on serving LLMs on a budget (on NVIDIA hardware), you will be pleasantly surprised to see how easy it is to deploy a model on the AWS Inferentia2 device using the pre-baked NOS <code>inf2</code> runtime we provide.</p>","tags":["embeddings","integrations","inferentia"]},{"location":"docs/blog/introducing-the-nos-inferentia2-runtime.html#what-is-aws-inferentia2","title":"\u26a1\ufe0f What is AWS Inferentia2?","text":"<p>AWS Inferentia2 (Inf2 for short) is the second-generation inference accelerator from AWS. Inf2 instances raise the performance of Inf1 (originally launched in 2019) by delivering 3x higher compute performance, 4x larger total accelerator memory, up to 4x higher throughput, and up to 10x lower latency. Inf2 instances are the first inference-optimized instances in Amazon EC2 to support scale-out distributed inference with ultra-high-speed connectivity between accelerators. </p> <p>Relative to the AWS G5 instances (NVIDIA A10G), Inf2 instances promise up to 50% better performance-per-watt. Inf2 instances are ideal for applications such as natural language processing, recommender systems, image classification and recognition, speech recognition, and language translation that can take advantage of scale-out distributed inference. </p> Instance Size Inf2 Accelerators Accelerator Memory (GB) vCPU Memory (GiB) On-Demand Price Spot Price inf2.xlarge 1 32 4 16 $0.76 $0.32 inf2.8xlarge 1 32 32 128 $1.97 $0.79 inf2.24xlarge 6 192 96 384 $6.49 $2.45 inf2.48xlarge 12 384 192 768 $12.98 $5.13","tags":["embeddings","integrations","inferentia"]},{"location":"docs/blog/introducing-the-nos-inferentia2-runtime.html#nos-inference-runtime","title":"\ud83c\udfc3\u200d\u2642\ufe0f NOS Inference Runtime","text":"<p>The NOS inference server supports custom runtime environments through the use of the InferenceServiceRuntime class - a high-level interface for defining new containerized and hardware-aware runtime environments. NOS already ships with runtime environments for NVIDIA GPUs (<code>gpu</code>) and Intel/ARM CPUs (<code>cpu</code>). Today, we're adding the NOS Inferentia2 runtime (<code>inf2</code>) with the AWS Neuron drivers, the AWS Neuron SDK and NOS pre-installed. This allows developers to quickly develop applications for AWS Inferentia2, without wasting any precious time on the complexities of setting up the AWS Neuron SDK and the AWS Inferentia2 driver environments.</p>","tags":["embeddings","integrations","inferentia"]},{"location":"docs/blog/introducing-the-nos-inferentia2-runtime.html#deploying-a-pytorch-model-on-inferentia2-with-nos","title":"\ud83d\udce6 Deploying a PyTorch model on Inferentia2 with NOS","text":"<p>Deploying PyTorch models on AWS Inferentia2 chips presents a unique set of challenges, distinct from the experience with NVIDIA GPUs. This is primarily due to the static graph execution requirement of ASICs, requiring the user to trace and compile models ahead-of-time, making them less accessible to entry-level developers. In some cases, custom model tracing and compilation are essential steps to fully utilize the AWS Inferentia2 accelerators. This demands a deep understanding of the HW-specific deployment/compiler toolchain (TensorRT, AWS Neuron SDK), the captured and data-dependent traced PyTorch graph, and the underlying HW-specific kernel/op-support to name just a few challenges. </p> <p>Simplifying AI hardware access with NOS</p> <p>NOS aims to bridge this gap and streamline the deployment process, making it more even accessible for both entry-level and expert developers to leverage the powerful inference capabilities of AWS Inferentia2 for their inference needs. </p>","tags":["embeddings","integrations","inferentia"]},{"location":"docs/blog/introducing-the-nos-inferentia2-runtime.html#1-define-your-custom-inf2-model","title":"1. Define your custom <code>inf2</code> model","text":"<p>In this example, we'll be using the <code>inf2/embeddings</code> sentence embedding tutorial on NOS. First, we'll define our custom <code>EmbeddingServiceInf2</code> model <code>models/embeddings_inf2.py</code> and a <code>serve.yaml</code> serve specification that will be used by NOS to serve our model on the AWS Inferentia2 device. The relevant files are shown below:</p> Directory structure of <code>nos/examples/inf2/embeddings</code><pre><code>$ tree .\n\u251c\u2500\u2500 job-inf2-embeddings-deployment.yaml\n\u251c\u2500\u2500 models\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 embeddings_inf2.py  (1)\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 serve.yaml  (2)\n\u2514\u2500\u2500 tests\n    \u251c\u2500\u2500 test_embeddings_inf2_client.py  (3)\n    \u2514\u2500\u2500 test_embeddings_inf2.py\n</code></pre> <ol> <li>Main python module that defines the <code>EmbeddingServiceInf2</code> model.</li> <li>The <code>serve.yaml</code> serve specification that defines the custom <code>inf2</code> runtime and registers the <code>EmbeddingServiceInf2</code> model with NOS.</li> <li>The pytest test for calling the <code>EmbeddingServiceInf2</code> service via gRPC.</li> </ol> <p>The embeddings interface is defined in the <code>EmbeddingServiceInf2</code> module in <code>models/embeddings_inf2.py</code>, where the <code>__call__</code> method returns the embedding of the text prompt using <code>BAAI/bge-small-en-v1.5</code> embedding model. </p>","tags":["embeddings","integrations","inferentia"]},{"location":"docs/blog/introducing-the-nos-inferentia2-runtime.html#2-define-the-custom-inf2-runtime-with-the-nos-serve-specification","title":"2. Define the custom <code>inf2</code> runtime with the NOS serve specification","text":"<p>The <code>serve.yaml</code> serve specification defines the custom embedding model, and a custom <code>inf2</code> runtime that NOS uses to execute our model. Follow the annotations below to understand the different components of the serve specification.</p> serve.yaml<pre><code>images:\n  embeddings-inf2:\n    base: autonomi/nos:latest-inf2  (1)\n    env:\n      NOS_LOGGING_LEVEL: DEBUG\n      NOS_NEURON_CORES: 2\n    run:\n      - python -m pip config set global.extra-index-url https://pip.repos.neuron.amazonaws.com\n      - pip install sentence-transformers  (2)\n\nmodels:\n  BAAI/bge-small-en-v1.5:\n    model_cls: EmbeddingServiceInf2\n    model_path: models/embeddings_inf2.py\n    default_method: __call__\n    runtime_env: embeddings-inf2  (3)\n</code></pre> <ol> <li>Specifies the base runtime image to use - we use the pre-baked <code>autonomi/nos:latest-inf2</code> runtime image to build our custom runtime image. This custom NOS runtime comes pre-installed with the AWS Neuron drivers and the AWS Neuron SDK.</li> <li>Installs the <code>sentence-transformers</code> library, which is used to embed the text prompt using the <code>BAAI/bge-small-en-v1.5</code> model.</li> <li>Specifies the custom runtime environment to use for the specific model deployment - <code>embeddings-inf2</code> - which is used to execute the <code>EmbeddingServiceInf2</code> model.</li> </ol> <p>In this example, we'll be using the Huggingface Optimum library to help us simplify the deployment process to the Inf2 chip. However, for custom model architectures and optimizations, we have built our own PyTorch tracer and compiler for a growing list of popular models on the Huggingface Hub. </p> Need support for custom models on AWS Inferentia2? <p>If you're interested in deploying a custom model on the AWS Inferentia2 chip with NOS, please reach out to us on our GitHub Issues page or at support@autonomi.ai, and we'll be happy to help you out.</p>","tags":["embeddings","integrations","inferentia"]},{"location":"docs/blog/introducing-the-nos-inferentia2-runtime.html#3-deploy-the-embedding-service-on-aws-inf2xlarge-with-skypilot","title":"3. Deploy the embedding service on AWS <code>inf2.xlarge</code> with SkyPilot","text":"<p>Now that we have defined our custom model, let's deploy this service on AWS Inferentia2 using SkyPilot. In this example, we're going to use SkyPilot's <code>sky launch</code> command to deploy our NOS service on an AWS <code>inf2.xlarge</code> on-demand instance. </p> <p>Before we launch the service, let's look at the <code>job-inf2-embeddings-deployment.yaml</code> file that we will use to provision the <code>inf2</code> instance and deploy the <code>EmbeddingServiceInf2</code> model.</p> job-inf2-embeddings-deployment.yaml<pre><code>file_mounts: (1)\n  /app: .\n\nresources:\n  cloud: aws\n  region: us-west-2\n  instance_type: inf2.xlarge (2)\n  image_id: ami-096319086cc3d5f23 # us-west-2 (3)\n  disk_size: 256\n  ports: 8000\n\nsetup: |\n  sudo apt-get install -y docker-compose-plugin\n\n  cd /app\n  cd /app &amp;&amp; python3 -m venv .venv &amp;&amp; source .venv/bin/activate\n  pip install git+https://github.com/autonomi-ai/nos.git pytest (4)\n\nrun: |\n  source /app/.venv/bin/activate\n  cd /app &amp;&amp; nos serve up -c serve.yaml --http (5)\n</code></pre> <ol> <li>Mounts the local <code>./app</code> directory so that the <code>serve.yaml</code>, <code>models/</code> and <code>tests/</code> directories are available on the remote instance.</li> <li>Specifies the AWS Inferentia2 instance type to use - we use the <code>inf2.xlarge</code> instance type.</li> <li>Specifies the Amazon Machine Instance (AMI) use that come pre-installed with AWS Neuron drivers.</li> <li>We simply need <code>pytest</code> for testing the client-side logic in <code>tests/test_embeddings_inf2_client.py</code></li> <li>Starts the NOS server with the <code>serve.yaml</code> specification. The runtime flag <code>--runtime inf2</code> is optional, and automatically detected by NOS as illustrated here.</li> </ol> <p>Provisioning <code>inf2.xlarge</code> instances</p> <p>To provision an <code>inf2.xlarge</code> instance, you will need to have an AWS account and the necessary service quotas set for the <code>inf2</code> instance nodes. For more information on service quotas, please refer to the AWS documentation.</p> <p>Using SkyPilot with <code>inf2</code> instances</p> <p>Due to a job submission bug in the SkyPilot CLI for <code>inf2</code> instances, you will need to use the <code>skypilot-nightly[aws]</code> (<code>pip install skypilot-nightly[aws]</code>) package to provision <code>inf2</code> instances correctly with the <code>sky launch</code> command below. </p> <p>Let's deploy the <code>inf2</code> embeddings service using the following command: <pre><code>sky launch -c inf2-embeddings-service job-inf2-embeddings-deployment.yaml\n</code></pre></p> <code>sky launch</code> output <p>You should see the following output from the <code>sky launch</code> command: <pre><code>(nos-infra-py38) inf2/embeddings spillai-desktop [ sky launch -c inf2-embeddings-service job-inf2-embeddings-deployment.yaml\nTask from YAML spec: job-inf2-embeddings-deployment.yaml\nI 01-31 21:48:06 optimizer.py:694] == Optimizer ==\nI 01-31 21:48:06 optimizer.py:717] Estimated cost: $0.8 / hour\nI 01-31 21:48:06 optimizer.py:717]\nI 01-31 21:48:06 optimizer.py:840] Considered resources (1 node):\nI 01-31 21:48:06 optimizer.py:910] ------------------------------------------------------------------------------------------\nI 01-31 21:48:06 optimizer.py:910]  CLOUD   INSTANCE      vCPUs   Mem(GB)   ACCELERATORS   REGION/ZONE   COST ($)   CHOSEN\nI 01-31 21:48:06 optimizer.py:910] ------------------------------------------------------------------------------------------\nI 01-31 21:48:06 optimizer.py:910]  AWS     inf2.xlarge   4       16        Inferentia:1   us-west-2     0.76          \u2714\nI 01-31 21:48:06 optimizer.py:910] ------------------------------------------------------------------------------------------\nI 01-31 21:48:06 optimizer.py:910]\nLaunching a new cluster 'inf2-embeddings-service'. Proceed? [Y/n]: y\nI 01-31 21:48:18 cloud_vm_ray_backend.py:4389] Creating a new cluster: 'inf2-embeddings-service' [1x AWS(inf2.xlarge, {'Inferentia': 1}, image_id={'us-west-2': 'ami-096319086cc3d5f23'}, ports=['8000'])].\nI 01-31 21:48:18 cloud_vm_ray_backend.py:4389] Tip: to reuse an existing cluster, specify --cluster (-c). Run `sky status` to see existing clusters.\nI 01-31 21:48:18 cloud_vm_ray_backend.py:1386] To view detailed progress: tail -n100 -f /home/spillai/sky_logs/sky-2024-01-31-21-48-06-108390/provision.log\nI 01-31 21:48:19 provisioner.py:79] Launching on AWS us-west-2 (us-west-2a,us-west-2b,us-west-2c,us-west-2d)\nI 01-31 21:49:37 provisioner.py:429] Successfully provisioned or found existing instance.\nI 01-31 21:51:03 provisioner.py:531] Successfully provisioned cluster: inf2-embeddings-service\nI 01-31 21:51:04 cloud_vm_ray_backend.py:4418] Processing file mounts.\nI 01-31 21:51:05 cloud_vm_ray_backend.py:4450] To view detailed progress: tail -n100 -f ~/sky_logs/sky-2024-01-31-21-48-06-108390/file_mounts.log\nI 01-31 21:51:05 backend_utils.py:1286] Syncing (to 1 node): . -&gt; ~/.sky/file_mounts/app\nI 01-31 21:51:06 cloud_vm_ray_backend.py:3158] Running setup on 1 node.\n...\n(task, pid=23904) \u2713 Launching docker compose with command: docker compose -f\n(task, pid=23904) /home/ubuntu/.nos/tmp/serve/docker-compose.app.yml up\n(task, pid=23904)  Network serve_default  Creating\n(task, pid=23904)  Network serve_default  Created\n(task, pid=23904)  Container serve-nos-server-1  Creating\n(task, pid=23904)  Container serve-nos-server-1  Created\n(task, pid=23904)  Container serve-nos-http-gateway-1  Creating\n(task, pid=23904)  Container serve-nos-http-gateway-1  Created\n(task, pid=23904) Attaching to serve-nos-http-gateway-1, serve-nos-server-1\n(task, pid=23904) serve-nos-http-gateway-1  | WARNING:  Current configuration will not reload as not all conditions are met, please refer to documentation.\n(task, pid=23904) serve-nos-server-1        |  \u2713 InferenceExecutor :: Backend initializing (as daemon) ...\n(task, pid=23904) serve-nos-server-1        |  \u2713 InferenceExecutor :: Backend initialized (elapsed=2.9s).\n(task, pid=23904) serve-nos-server-1        |  \u2713 InferenceExecutor :: Connected to backend.\n(task, pid=23904) serve-nos-server-1        |  \u2713 Starting gRPC server on [::]:50051\n(task, pid=23904) serve-nos-server-1        |  \u2713 InferenceService :: Deployment complete (elapsed=0.0s)\n(task, pid=23904) serve-nos-server-1        | (EmbeddingServiceInf2 pid=404) 2024-01-31 21:53:58.566 | INFO     | nos.neuron.device:setup_environment:36 - Setting up neuron env with 2 cores\n...\n(task, pid=23904) serve-nos-server-1        | (EmbeddingServiceInf2 pid=404) 2024-02-01T05:54:36Z Compiler status PASS\n(task, pid=23904) serve-nos-server-1        | (EmbeddingServiceInf2 pid=404) 2024-01-31 21:54:46.928 | INFO     | EmbeddingServiceInf2:__init__:61 - Saved model to /app/.nos/cache/neuron/BAAI/bge-small-en-v1.5-bs-1-sl-384\n(task, pid=23904) serve-nos-server-1        | (EmbeddingServiceInf2 pid=404) 2024-01-31 21:54:47.037 | INFO     | EmbeddingServiceInf2:__init__:64 - Loaded neuron model: BAAI/bge-small-en-v1.5\n...\n(task, pid=23904) serve-nos-server-1        | 2024-01-31 22:25:43.710 | INFO     | nos.server._service:Run:360 - Executing request [model=BAAI/bge-small-en-v1.5, method=None]\n(task, pid=23904) serve-nos-server-1        | 2024-01-31 22:25:43.717 | INFO     | nos.server._service:Run:362 - Executed request [model=BAAI/bge-small-en-v1.5, method=None, elapsed=7.1ms]\n</code></pre></p> <p>Once complete, you should see the following (trimmed) output from the <code>sky launch</code> command: <pre><code>\u2713 InferenceExecutor :: Backend initializing (as daemon) ...\n\u2713 InferenceExecutor :: Backend initialized (elapsed=2.9s).\n\u2713 InferenceExecutor :: Connected to backend.\n\u2713 Starting gRPC server on [::]:50051\n\u2713 InferenceService :: Deployment complete (elapsed=0.0s)\nSetting up neuron env with 2 cores\n...\nCompiler status PASS\nSaved model to /app/.nos/cache/neuron/BAAI/bge-small-en-v1.5-bs-1-sl-384\nLoaded neuron model: BAAI/bge-small-en-v1.5\n</code></pre></p>","tags":["embeddings","integrations","inferentia"]},{"location":"docs/blog/introducing-the-nos-inferentia2-runtime.html#3-test-your-custom-model-on-aws-inf2-instance","title":"3. Test your custom model on AWS Inf2 instance","text":"<p>Once the service is deployed, you should be able to simply make a cURL request to the <code>inf2</code> instance to test the server-side logic of the embeddings model.</p> Using cURL (remote)Using the gRPC client (on the <code>inf2</code> instance) <pre><code>export IP=$(sky status --ip inf2-embeddings-service)\n\ncurl \\\n-X POST http://${IP}:8000/v1/infer \\\n-H 'Content-Type: application/json' \\\n-d '{\n    \"model_id\": \"BAAI/bge-small-en-v1.5\",\n    \"inputs\": {\n        \"texts\": [\"fox jumped over the moon\"]\n    }\n}'\n</code></pre> <p>Optionally, you can also test the gRPC service using the provided <code>tests/test_embeddings_inf2_client.py</code>. For this test however, you'll need to ssh into the <code>inf2</code> instance and run the following command.</p> <pre><code>ssh inf2-embeddings-service\n</code></pre> <p>Once you're on the <code>inf2.xlarge</code> instance, you can run <code>pytest -sv tests/test_embeddings_inf2_client.py</code> to test the server-side logic of the embeddings model. </p> <pre><code>$ pytest -sv tests/test_embeddings_inf2_client.py\n</code></pre> <p>Here's a simplified version of the test to execute the embeddings model.</p> <pre><code>from nos.client import Client\n\n# Create the client\nclient = Client(\"[::]:50051\")\nassert client.WaitForServer()\n\n# Load the embeddings model\nmodel = client.Module(\"BAAI/bge-small-en-v1.5\")\n\n# Embed text with the model\ntexts = \"What is the meaning of life?\"\nresponse = model(texts=texts)\n</code></pre>","tags":["embeddings","integrations","inferentia"]},{"location":"docs/blog/introducing-the-nos-inferentia2-runtime.html#whats-it-going-to-cost-me","title":"\ud83e\udd11 What's it going to cost me?","text":"<p>The table below shows the costs of deploying one of these latency-optimized (<code>bsize=1</code>) embedding services on a single Inf2 instance on AWS. While the costs are only one part of the equation, it is important to note that the AWS Inf2 instances are ~25% cheaper than the NVIDIA A10G instances, and offer a more cost-effective solution for inference workloads on AWS. In the coming weeks, we'll be digging into evaluating the performance of the Inf2 instances with respect to their NVIDIA GPU counterparts on inference metrics such as latency/throughput and cost metrics such as number of requests / $, montly costs etc.</p> Model Cloud Instance Spot Cost / hr Cost / month # of Req. / $ BAAI/bge-small-en-v1.5 <code>inf2.xlarge</code> - $0.75 ~$540 ~685K / $1 BAAI/bge-small-en-v1.5 <code>inf2.xlarge</code> \u2705 $0.32 ~$230 ~1.6M / $1","tags":["embeddings","integrations","inferentia"]},{"location":"docs/blog/introducing-the-nos-inferentia2-runtime.html#wrapping-up","title":"\ud83c\udf81 Wrapping up","text":"<p>In this post, we introduced the new NOS <code>inf2</code> runtime that allows developers to easily develop, and serve models on the AWS Inferentia2 chip. With more cost-efficient, and inference-optimized chips coming to market (Google TPUs, Groq, Tenstorrent etc), we believe it is important for developers to be able to easily access and deploy models on these devices. The specialized NOS Inference Runtime aims to do just that - a fast, and frictionless way to deploy models onto any of the AI accelerators, be it NVIDIA GPUs or AWS Inferentia2 chips, in the cloud, or on-prem.</p> <p>Thanks for reading, and we hope you found this post useful - and finally, give NOS a try. If you have any questions, or would like to learn more about the NOS <code>inf2</code> runtime, please reach out to us on our GitHub Issues page or join us on Discord.  </p>","tags":["embeddings","integrations","inferentia"]},{"location":"docs/cli/profile.html","title":"Profile","text":""},{"location":"docs/cli/profile.html#profiling-models","title":"\u23f1\ufe0f Profiling Models","text":"<p><code>nos profile</code> supports model benchmarking across a number of axes including iterations per second, memory footprint and utilization on CPU/GPU. Currently, the nos profiler itself runs natively in the execution environment (i.e. outside the NOS server), so you'll need to install both the <code>server</code> and <code>test</code> dependencies alongside your existing NOS installation.</p> <pre><code>pip install torch-nos[server,test]\n</code></pre>"},{"location":"docs/cli/profile.html#nos-profile-commands","title":"NOS Profile Commands","text":""},{"location":"docs/cli/profile.html#all","title":"all","text":"<p>Profile all models</p> <p>If you have the time, you can construct a profiling catalog on your machine in its entirety with: <pre><code>nos profile all\n</code></pre></p>"},{"location":"docs/cli/profile.html#rebuild-catalog","title":"rebuild-catalog","text":"<p>Generate a fresh catalog with  <pre><code>nos profile rebuild-catalog\n</code></pre></p>"},{"location":"docs/cli/profile.html#model","title":"model","text":"<p>You can also profile specific models with  <pre><code>nos profile model -m openai/clip-vit-large-patch14\n</code></pre></p>"},{"location":"docs/cli/profile.html#method","title":"method","text":"<p>Or an entire method type with <pre><code>nos profile method -m encode_image\n</code></pre> to benchmark e.g. all image embedding models.</p>"},{"location":"docs/cli/profile.html#list","title":"list","text":"<p>Dump the nos profiling catalog with <code>nos profile list</code></p> <p></p>"},{"location":"docs/cli/serve.html","title":"<code>nos serve</code> CLI","text":""},{"location":"docs/cli/serve.html#serve_cli","title":"serve_cli","text":"<p>NOS gRPC/REST Serve CLI.</p> <p>Usage:</p> <pre><code>serve [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --install-completion  Install completion for the current shell.\n  --show-completion     Show completion for the current shell, to copy it or\n                        customize the installation.\n</code></pre>"},{"location":"docs/cli/serve.html#build","title":"build","text":"<p>Build the NOS server locally.</p> <p>Usage:</p> <pre><code>serve build [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -c, --config TEXT  Serve configuration filename.\n  --target TEXT      Serve a specific target.\n  -t, --tag TEXT     Image tag f-string.  [default: autonomi/nos:{target}]\n  -p, --prod         Run with production flags (slimmer images, no dev.\n                     dependencies).\n</code></pre>"},{"location":"docs/cli/serve.html#down","title":"down","text":"<p>Tear down the NOS server.</p> <p>Usage:</p> <pre><code>serve down [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -v, --verbose  Verbose output.\n</code></pre>"},{"location":"docs/cli/serve.html#generate","title":"generate","text":"<p>Generate the NOS server Dockerfiles, without building it.</p> <p>Usage:</p> <pre><code>serve generate [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -c, --config TEXT  Serve configuration filename.\n  --target TEXT      Serve a specific target.\n  -t, --tag TEXT     Image tag f-string.  [default: autonomi/nos:{target}]\n  -p, --prod         Run with production flags (slimmer images, no dev.\n                     dependencies).\n</code></pre>"},{"location":"docs/cli/serve.html#up","title":"up","text":"<p>Spin up the NOS server locally.</p> <p>Usage:</p> <pre><code>serve up [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -c, --config TEXT       Serve configuration filename.\n  -r, --runtime TEXT      Runtime environment to use.\n  --target TEXT           Serve a specific target.\n  -t, --tag TEXT          Image tag f-string.  [default:\n                          autonomi/nos:{target}]\n  --http                  Serve with HTTP gateway.\n  --http-host TEXT        HTTP host to use.  [default: 0.0.0.0]\n  --http-port INTEGER     HTTP port to use.  [default: 8000]\n  --http-workers INTEGER  HTTP max workers.  [default: 1]\n  --logging-level TEXT    Logging level.  [default: INFO]\n  --home-directory TEXT   Override the NOS_HOME variable with a custom\n                          location.  [default: ~/.nosd]\n  -d, --daemon            Run in daemon mode.\n  --reload                Reload on file changes.\n  --build                 Only build the custom image, without serving it.\n  -p, --prod              Run with production flags (slimmer images, no dev.\n                          dependencies).\n  --env-file TEXT         Provide an environment file for secrets.\n  --debug                 Debug intermediate outputs (Dockerfile, docker-\n                          compose.yml).\n  -v, --verbose           Verbose output.\n</code></pre>"},{"location":"docs/cli/serve.html#serve-yaml-specification","title":"Serve YAML Specification","text":"<p>The <code>serve</code> CLI uses a YAML specification file to fully configure the runtime and the models that need to be served. The full specification is available here. </p> serve.yaml<pre><code># `images` define the runtime environment for your custom model\n# You can specify a base image, system packages, pip dependencies, \n# environment variables, and even execute bespoke scripts to define \n# your custom rutnime environment. We use `agi-pack` to build the\n# runtime environment as a docker container.\nimages:\n  # `custom-gpu` is the name of the runtime environment we build here\n  # The custom docker image encapsulates all the necessary runtime \n  # dependencies we need to run our custom model.\n  custom-gpu:\n    # `base` is the base image for the runtime environment\n    # that we build our custom runtime environment on top of.\n    base: autonomi/nos:latest-gpu\n    # `system` defines the system packages for the runtime environment\n    system:\n      - git\n    # `pip` defines the pip dependencies for the runtime environment\n    pip:\n      - accelerate&gt;=0.23.0\n      - transformers&gt;=4.35.1\n    # `env` defines the environment variables for the runtime environment\n    env:\n      NOS_LOGGING_LEVEL: INFO\n    # `run` defines the scripts to run to build the runtime environment\n    run:\n      - python -c 'import torch; print(torch.__version__)'\n\n# `models` define the custom model class, model path, and deployment\n# configuration for your custom model. You can specify the runtime\n# environment for your model, and list out all the models and their\n# CPU / GPU resource requirements.\nmodels:\n  # `custom/custom-model-a` is the name of the custom model we register\n  # with the NOS server. Each model is uniquely identified by its name, and\n  # can have its own runtime environment defined and scaled independently.\n  custom/custom-model-a:\n    # `runtime_env` defines the runtime environment for this custom model\n    runtime_env: custom-gpu\n    # `model_cls` defines the custom model class for this custom model\n    # that we have defined under the `model_path` location. \n    model_cls: CustomModel\n    model_path: models/model.py\n    # `default_method` defines the default method to call on the custom model\n    # if no `method` signature is specified in the request.\n    default_method: __call__\n    # `init_kwargs` defines the keyword arguments to pass to the custom model\n    # class constructor. This is useful for instantiating custom models with\n    # different configurations.\n    init_kwargs:\n      model_name: custom/custom-model-a\n    # `deployment` defines the deployment specification for this custom model.\n    # Each model can be individually profiled, optimized and scaled, allowing \n    # NOS to fully utilize the underlying hardware resources.\n    deployment:\n      resources:\n        device: auto\n        device_memory: 4Gi\n      num_replicas: 2\n\n  # `custom/custom-model-b` is the name of the second custom model we register\n  # with the NOS server. Multiple models with unique names can be registered, \n  # with each model having its own runtime environment (if needed). In this case,\n  # we use the same runtime environment for both models.\n  custom/custom-model-b:\n    runtime_env: custom-gpu\n    model_cls: CustomModel\n    model_path: models/model.py\n    default_method: __call__\n    init_kwargs:\n      model_name: custom/custom-model-b\n    # For deployment here, we explicitly specify the cpu and memory resources\n    # for each replica of the model. \n    deployment:\n      resources:\n        cpu: 2\n        memory: 2Gi\n        device: cpu\n      num_replicas: 2\n</code></pre> <p>Check out our custom model serving tutorial here to learn more about how to use the <code>serve.yaml</code> file to serve custom models with NOS.</p>"},{"location":"docs/cli/serve.html#debugging-the-server","title":"Debugging the server","text":"<p>When using the <code>serve</code> CLI, you can enable verbose logging by setting the <code>--logging-level</code> argument to <code>DEBUG</code>:</p> <pre><code>$ nos serve -c &lt;serve.yaml&gt; --logging-level DEBUG\n</code></pre> <p>This can be especially useful when debugging issues with your server, especially around model registry, resource allocation and model logic.</p>"},{"location":"docs/cli/system.html","title":"<code>nos system</code> CLI","text":"<p>NOS provides some basic utilities for listing and inspecting your system information.</p>"},{"location":"docs/cli/system.html#system_cli","title":"system_cli","text":"<p>Get system information (including CPU, GPU, RAM, etc.)</p> <p>Usage:</p> <pre><code>info [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --install-completion  Install completion for the current shell.\n  --show-completion     Show completion for the current shell, to copy it or\n                        customize the installation.\n</code></pre>"},{"location":"docs/cli/system.html#get-system-information","title":"Get system information","text":"<pre><code>nos system info\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 System \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 {                                                                            \u2502\n\u2502   \"system\": {                                                                \u2502\n\u2502     \"system\": \"Linux\",                                                       \u2502\n\u2502     \"release\": \"5.19.0-41-generic\",                                          \u2502\n\u2502     \"version\": \"#42~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue Apr 18 17:40:00 U \u2502\n\u2502     \"machine\": \"x86_64\",                                                     \u2502\n\u2502     \"architecture\": [                                                        \u2502\n\u2502       \"64bit\",                                                               \u2502\n\u2502       \"ELF\"                                                                  \u2502\n\u2502     ],                                                                       \u2502\n\u2502     \"processor\": \"x86_64\",                                                   \u2502\n\u2502     \"python_implementation\": \"CPython\"                                       \u2502\n\u2502   },                                                                         \u2502\n\u2502   \"cpu\": {                                                                   \u2502\n\u2502     \"model\": \"AMD Ryzen Threadripper 3970X 32-Core Processor\",               \u2502\n\u2502     \"architecture\": \"x86_64\",                                                \u2502\n\u2502     \"cores\": {                                                               \u2502\n\u2502       \"physical\": 32,                                                        \u2502\n\u2502       \"total\": 64                                                            \u2502\n\u2502     },                                                                       \u2502\n\u2502     \"frequency\": 3300.0,                                                     \u2502\n\u2502     \"frequency_str\": \"3.30 GHz\"                                              \u2502\n\u2502   },                                                                         \u2502\n\u2502   \"memory\": {                                                                \u2502\n\u2502     \"total\": 134905909248,                                                   \u2502\n\u2502     \"used\": 9529114624,                                                      \u2502\n\u2502     \"available\": 119143944192                                                \u2502\n\u2502   },                                                                         \u2502\n\u2502   \"torch\": {                                                                 \u2502\n\u2502     \"version\": \"2.0.1\"                                                       \u2502\n\u2502   },                                                                         \u2502\n\u2502   \"docker\": {                                                                \u2502\n\u2502     \"version\": \"Docker version 24.0.0, build 98fdcd7\",                       \u2502\n\u2502     \"sdk_version\": \"6.1.0\",                                                  \u2502\n\u2502     \"compose_version\": \"Docker Compose version v2.17.3\"                      \u2502\n\u2502   },                                                                         \u2502\n\u2502   \"gpu\": {                                                                   \u2502\n\u2502     \"cuda_version\": \"11.7\",                                                  \u2502\n\u2502     \"cudnn_version\": 8500,                                                   \u2502\n\u2502     \"device_count\": 3,                                                       \u2502\n\u2502     \"devices\": [                                                             \u2502\n\u2502       {                                                                      \u2502\n\u2502         \"device_id\": 0,                                                      \u2502\n\u2502         \"device_name\": \"NVIDIA GeForce RTX 4090\",                            \u2502\n\u2502         \"device_capability\": \"8.9\",                                          \u2502\n\u2502         \"total_memory\": 25393692672,                                         \u2502\n\u2502         \"total_memory_str\": \"23.65 GB\",                                      \u2502\n\u2502         \"multi_processor_count\": 128                                         \u2502\n\u2502       },                                                                     \u2502\n\u2502       {                                                                      \u2502\n\u2502         \"device_id\": 1,                                                      \u2502\n\u2502         \"device_name\": \"NVIDIA GeForce RTX 2080 Ti\",                         \u2502\n\u2502         \"device_capability\": \"7.5\",                                          \u2502\n\u2502         \"total_memory\": 11543379968,                                         \u2502\n\u2502         \"total_memory_str\": \"10.75 GB\",                                      \u2502\n\u2502         \"multi_processor_count\": 68                                          \u2502\n\u2502       },                                                                     \u2502\n\u2502       {                                                                      \u2502\n\u2502         \"device_id\": 2,                                                      \u2502\n\u2502         \"device_name\": \"NVIDIA GeForce RTX 2080 Ti\",                         \u2502\n\u2502         \"device_capability\": \"7.5\",                                          \u2502\n\u2502         \"total_memory\": 11546394624,                                         \u2502\n\u2502         \"total_memory_str\": \"10.75 GB\",                                      \u2502\n\u2502         \"multi_processor_count\": 68                                          \u2502\n\u2502       }                                                                      \u2502\n\u2502     ],                                                                       \u2502\n\u2502     \"driver_version\": \"530.41.03\"                                            \u2502\n\u2502   }                                                                          \u2502\n\u2502 }                                                                            \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\nGPU detected, fetching nvidia-smi information within docker.\n...\n</code></pre>"},{"location":"docs/concepts/architecture-overview.html","title":"What is NOS?","text":"<p>NOS (<code>torch-nos</code>) is a fast and flexible Pytorch inference server, specifically designed for optimizing and running lightning-fast inference of popular foundational AI models.</p> <p>Optimizing and serving models for production AI inference is still difficult, often leading to notoriously expensive cloud bills and often underutilized GPUs. That\u2019s why we\u2019re building NOS - a fast inference server for modern AI workloads. With a few lines of code, developers can optimize, serve, and auto-scale Pytorch model inference without having to deal with the complexities of ML compilers, HW-accelerators, or distributed inference. Simply put, NOS allows AI teams to cut inference costs up to 10x, speeding up development time and time-to-market.</p>"},{"location":"docs/concepts/architecture-overview.html#core-features","title":"\u26a1\ufe0f Core Features","text":"<ul> <li>\ud83d\udd0b Batteries-included: Server-side inference with all the necessary batteries (model hub, batching/parallelization, fast I/O, model-caching, model resource management via ModelManager, model optimization via ModelSpec)</li> <li>\ud83d\udce1 Client-Server architecture: Multiple lightweight clients can leverage powerful server-side inference workers running remotely without the bloat of GPU libraries, runtimes or 3rd-party libraries.</li> <li>\ud83d\udcaa High device-utilization:  With better model management, client\u2019s won\u2019t have to wait on model inference and instead can take advantage of the full GPU resources available. Model multiplexing, and efficient bin-packing of models allow us to leverage the resources optimally (without requiring additional user input).</li> <li>\ud83d\udce6 Custom model support: NOS allows you to easily add support for custom models with a few lines of code. We provide a simple API to register custom models with NOS, and allow you to optimize and run models on any hardware (NVIDIA, custom ASICs) without any model compilation or runtime management (see example).</li> <li>\u23e9 Concurrency: NOS is built to efficiently serve AI models, ensuring concurrency, parallelism, optimal memory management, and automatic garbage collection. It is particularly well-suited for multi-modal AI applications.</li> </ul>"},{"location":"docs/concepts/architecture-overview.html#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":""},{"location":"docs/concepts/architecture-overview.html#core-components","title":"\ud83d\udee0\ufe0f Core Components","text":"<p>NOS is built to efficiently serve AI models, ensuring concurrency, parallelism, optimal memory management, and automatic garbage collection. It is particularly well-suited for multi-modal AI applications. Finally, NOS is built to be modular and extensible. The core components of NOS are:</p> <ul> <li><code>ModelManager</code>: The Model Manager is responsible for managing and serving AI models with various policies like FIFO and LRU (not implemented). It ensures that the maximum number of concurrent models is not exceeded.<ul> <li>FIFO and LRU (not implemented) eviction policies.</li> <li>Control the maximum number of concurrent models.</li> <li>Load, add, and evict models as needed.</li> <li>Prevent Out-Of-Memory errors with automatic model cleanup.</li> </ul> </li> <li> <p><code>ModelHandle</code>:The <code>ModelHandle</code> is the core component for serving AI models. It allows you to interact with and scale models as needed. Each Model Handle can have multiple replicas for parallelism.</p> <ul> <li>Call models directly or submit tasks to replicas.</li> <li>Scale models up or down dynamically.</li> <li>Submit tasks to specific methods of the model.</li> <li>Garbage collect models when they are evicted.</li> </ul> </li> <li> <p><code>InferenceService</code>: Ray-executor based inference service that executes inference requests.</p> </li> <li> <p><code>InferenceRuntimeService</code>: Dockerized runtime environment for server-side remote execution</p> </li> </ul>"},{"location":"docs/concepts/model-manager.html","title":"HW-aware execution","text":"<p>The model manager is the main workhorse of the NOS server. It is responsible for managing the lifecycle of models, executing inference requests across a pool of workers that may live on heterogenous hardware devices. The model manager is responsible for the following:</p> <ul> <li> <p>\ud83d\udce1 Remote Model Execution: The model manager is responsible for the remote inference / execution of models concurrently ac across a pool of workers. We use Ray to distribute the inference workloads. In addition to this, the manager also maintains a the functional specification of the model allowing NOS to optionally trace and compile relevant code-paths that are critical for low-latency execution. This allows developers to further optimize and increase performance of their models without explicitly generating compilation or traced artifacts through these ahead-of-time tool chains. In essence, the model manager provides a logical handle for remote model execution while leveraging the appropriate hardware devices available for model inference (CPUs, GPUs, ASICs).</p> </li> <li> <p>\ud83d\ude80 Distributed Inference: On top of remote model execution, NOS leverages Ray to scale models across large number of GPUs/HW-accelerators to fully utilize large compute clusters, and auto-scaling models based on load. This allows NOS to scale models across multiple nodes, and leverage the full compute capacity of the cluster.</p> </li> <li> <p>\ud83d\udcaa Automatic Batched Execution:  By profiling the model, we can determine the optimal batch size for maximum throughput. We can then batch inference requests to maximize throughput. The model manager takes care of batching requests implicitly, allowing users to simply submit tasks asynchronously while the model manager effectively dispatches the inference tasks to an appropriately scaled pool of workers.</p> </li> <li> <p>\ud83d\udd27 Tuneable Perforamnce: One of the difficulties with using Pytorch is that you would have manaully tune your models for the underlying HW without much visibility into the memory profiles of each model. The onus is on the developer to manually profile each model they run, and appropriately allocate the model on the device memory. This becomes especially inconvenient when dealing with tens of models that may have different memory and execution profiles. With the model manager, we make this transparent to the user so that they can fully utilize their underlying HW without having to manaully tune their model with every deployment.</p> </li> <li> <p>\ud83e\udde0 Device Memory Management: Large models can take up a lot of device memory, and it is important to free up memory when it is no longer needed. The model manager is responsible for garbage collecting models that are no longer needed. We implement a FIFO eviction policy for garbage collection, but this can be extended to other policies such as LRU, LFU etc. While FIFO (first-in first-out) is a simple and predictable policy, it is also the most effective in practice.</p> </li> <li> <p>\ud83e\uddf0 Model Compilation: Coming Soon!</p> </li> </ul>"},{"location":"docs/concepts/model-spec.html","title":"Model specification","text":"<p>In NOS, the <code>ModelSpec</code> class is a serializable specification of a model that captures all the relevant information for instantatiation, execution and runtime profile of a model.</p>"},{"location":"docs/concepts/model-spec.html#model-specification","title":"Model Specification","text":"<ul> <li>Deterministic: We benchmark the models during model registry, so you are guaranteed execution runtimes and device resource-usage. More specifically, the model specification will allow us to measure memory consumption and FLOPs ahead-of-time and enable more efficient device-memory usage in production.</li> <li>Scalable: Registered models can be independently scaled up for batch inference or parallel execution with Ray actors.</li> <li>Optimizable: Every registered model can be inspected, compiled and optimized with a unique and configurable runtime-engine (TensorRT, ONNX, AITemplate etc). This allows us to benchmark models before they enter production, and run models at the optimal (or configurable) operating point.</li> </ul>"},{"location":"docs/concepts/runtime-environments.html","title":"Runtime environments","text":"<p>The NOS inference server supports custom runtime environments through the use of the <code>InferenceServiceRuntime</code> class and the configurations defined within. This class provides a high-level interface for defining new custom runtime environments that can be used with NOS.</p>"},{"location":"docs/concepts/runtime-environments.html#nos-inference-runtime","title":"\u26a1\ufe0f NOS Inference Runtime","text":"<p>We use docker to configure different worker configurations to run workloads in different runtime environments. The configured runtime environments are specified in the InferenceServiceRuntime class, which wraps the generic [<code>DockerRuntime</code>] class. For convenience, we have pre-built some runtime environments that can be used out-of-the-box <code>cpu</code>, <code>gpu</code>, <code>inf2</code> etc.</p> <p>This is the general flow of how the runtime environments are configured: - Configure runtime environments including <code>cpu</code>, <code>gpu</code>, <code>inf2</code> etc in the <code>InferenceServiceRuntime</code> <code>config</code> dictionary. - Start the server with the appropriate runtime environment via the <code>--runtime</code> flag. - The ray cluster is now configured within the appropriate runtime environment and has access to the appropriate libraries and binaries.</p> <p>For custom runtime support, we use Ray to configure different worker configurations (custom conda environment, with resource naming) to run workers on different runtime environments (see below).</p>"},{"location":"docs/concepts/runtime-environments.html#supported-runtimes","title":"\ud83c\udfc3\u200d\u2642\ufe0f Supported Runtimes","text":"<p>The following runtimes are supported by NOS:</p> Status Name Pyorch HW Base Size Description \u2705 <code>autonomi/nos:latest-cpu</code> <code>2.1.1</code> CPU <code>debian:buster-slim</code> 1.1 GB CPU-only runtime. \u2705 <code>autonomi/nos:latest-gpu</code> <code>2.1.1</code> NVIDIA GPU <code>nvidia/cuda:11.8.0-base-ubuntu22.04</code> 3.9 GB GPU runtime. \u2705 <code>autonomi/nos:latest-inf2</code> <code>1.13.1</code> AWS Inferentia2 <code>debian:buster-slim</code> 1.7 GB Inf2 runtime with torch-neuronx. Coming Soon <code>trt</code> <code>2.0.1</code> NVIDIA GPU <code>nvidia/cuda:11.8.0-base-ubuntu22.04</code> GPU runtime with TensorRT (8.4.2.4)."},{"location":"docs/concepts/runtime-environments.html#adding-a-custom-runtime","title":"\ud83d\udee0\ufe0f Adding a custom runtime","text":"<p>To define a new custom runtime environment, you can extend the <code>InferenceServiceRuntime</code> class and add new configurations to the existing <code>configs</code> variable.</p>"},{"location":"docs/concepts/runtime-environments.html#nos.server._runtime.InferenceServiceRuntime.configs","title":"nos.server._runtime.InferenceServiceRuntime.configs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>configs = {'cpu': InferenceServiceRuntimeConfig(image=NOS_DOCKER_IMAGE_CPU, name=f'{NOS_INFERENCE_SERVICE_CONTAINER_NAME}-cpu', kwargs={'nano_cpus': int(6000000000.0), 'mem_limit': '6g', 'log_config': {'type': JSON, 'config': {'max-size': '100m', 'max-file': '10'}}}), 'gpu': InferenceServiceRuntimeConfig(image=NOS_DOCKER_IMAGE_GPU, name=f'{NOS_INFERENCE_SERVICE_CONTAINER_NAME}-gpu', device='gpu', kwargs={'nano_cpus': int(8000000000.0), 'mem_limit': '12g', 'log_config': {'type': JSON, 'config': {'max-size': '100m', 'max-file': '10'}}}), 'trt': InferenceServiceRuntimeConfig(image='autonomi/nos:latest-trt', name=f'{NOS_INFERENCE_SERVICE_CONTAINER_NAME}-trt', device='gpu', kwargs={'nano_cpus': int(8000000000.0), 'mem_limit': '12g', 'log_config': {'type': JSON, 'config': {'max-size': '100m', 'max-file': '10'}}}), 'inf2': InferenceServiceRuntimeConfig(image='autonomi/nos:latest-inf2', name=f'{NOS_INFERENCE_SERVICE_CONTAINER_NAME}-inf2', device='inf2', environment=_default_environment({'NEURON_RT_VISIBLE_CORES': 2}), kwargs={'nano_cpus': int(8000000000.0), 'log_config': {'type': JSON, 'config': {'max-size': '100m', 'max-file': '10'}}})}\n</code></pre>"},{"location":"docs/concepts/what-is-nos.html","title":"What is nos","text":"<p>NOS (<code>torch-nos</code>) is a fast and flexible Pytorch inference server, specifically designed for optimizing and running lightning-fast inference of popular foundational AI models.</p> <p>Optimizing and serving models for production AI inference is still difficult, often leading to notoriously expensive cloud bills and often underutilized GPUs. That\u2019s why we\u2019re building NOS - a fast inference server for modern AI workloads. With a few lines of code, developers can optimize, serve, and auto-scale Pytorch model inference without having to deal with the complexities of ML compilers, HW-accelerators, or distributed inference. Simply put, NOS allows AI teams to cut inference costs up to 10x, speeding up development time and time-to-market.</p>"},{"location":"docs/concepts/what-is-nos.html#core-features","title":"\u26a1\ufe0f Core Features","text":"<ul> <li>\ud83d\udd0b Batteries-included: Server-side inference with all the necessary batteries (model hub, batching/parallelization, fast I/O, model-caching, model resource management via ModelManager, model optimization via ModelSpec)</li> <li>\ud83d\udce1 Client-Server architecture: Multiple lightweight clients can leverage powerful server-side inference workers running remotely without the bloat of GPU libraries, runtimes or 3rd-party libraries.</li> <li>\ud83d\udcaa High device-utilization:  With better model management, client\u2019s won\u2019t have to wait on model inference and instead can take advantage of the full GPU resources available. Model multiplexing, and efficient bin-packing of models allow us to leverage the resources optimally (without requiring additional user input).</li> <li>\ud83d\udce6 Custom model support: NOS allows you to easily add support for custom models with a few lines of code. We provide a simple API to register custom models with NOS, and allow you to optimize and run models on any hardware (NVIDIA, custom ASICs) without any model compilation or runtime management (see example).</li> </ul>"},{"location":"docs/concepts/what-is-nos.html#who-is-nos-for","title":"\ud83d\udc69\u200d\ud83d\udcbb Who is NOS for?","text":"<p>If you've dug this far into the NOS docs (welcome!) you're probably interested in running/serving Pytorch models.</p> <p>You may have experience with one of the following:</p> <ul> <li>Deploying to AWS/GCP/Azure/on-prem manually with your own Docker Containers, dependency management etc.</li> <li>Using a deployment service like AWS Sagemaker</li> <li>Hitting an Inference API from OpenAI, Huggingface, Replicate etc.</li> </ul> <p>Each of the above trade off between cost, iteration speed and flexibility. Inference APIs in particular have taken off for hobbyist developers and enterprises alike that aren't interested in building out their own infra to run Image Generation, ASR, Object Detection etc. Black-box inference APIs come with drawbacks, however:</p> <ul> <li>Developers are forced to choose between paying for each and every request even during prototyping, or falling back to a different execution flow with substantial gaps between dev and prod environments.</li> <li>They offer Limited flexibility with regards to model selection/performance optimization. Inference APIs are a fixed quantity.</li> <li>They may raise privacy concerns as user data must go outside the wire for inferencing on vendor servers</li> <li>Stability issues when using poorly maintained third party APIs</li> </ul> <p>We built NOS because we wanted an inference server combining best-practices in model-serving, distributed-inference, auto-scaling all in a single, easy-to-user containerized system that you can simply run with a few lines of Python.</p>"},{"location":"docs/concepts/what-is-nos.html#model-containers","title":"\ud83d\udce6 Model Containers","text":"<p>Deep learning containers have been around for quite a while, and generally come in the form of a Docker Image pre-rolled with framework/HW dependencies on top of a base Linux build. More recently, toolchains like Cog have made wrapping individual model prediction interfaces into containers quick and easy via a DSL, and we expect this trend to continue. That said, We believe a full-featured Inference Server should be able to do much more, including:</p> <ul> <li>Serving multiple models dynamically from a single host, eliminating the need for cold starts as workloads change</li> <li>Scaling up and down according to inference traffic</li> <li>Taking full advantage of HW acceleration and memory optimization to eliminate unnecessary copies for larger input types  (images/videos)</li> <li>Providing a superior developer experience with more error verbosity than 404s from a REST endpoint.</li> </ul> <p>The NOS server / client provide these out of the box with a minimum of installation headache.</p>"},{"location":"docs/concepts/what-is-nos.html#give-it-a-try-and-share-your-feedback","title":"\ud83d\udcac Give it a try and share your feedback!","text":"<p>NOS is meant to simplify iteration and deployment of popular Generative and Robotics AI workflows. We encourage the community to give feedback and suggest improvements, and welcome contributions from folks eager to help democratize fast, efficient inference!</p>"},{"location":"docs/demos/animate-diff.html","title":"Running animate-diff as a custom model.","text":"<p>Animatediff generates short (~1s) gifs from text prompts. We don't support this out of the box in NOS, but in this tutorial we'll cover how to define a configuration for a custom model with dependencies different from those in the base NOS server image.</p> <ol> <li>Define a serve.yaml with required dependencies</li> </ol> <p>In <code>examples/animatediff</code> you will find two files: <code>serve.yaml</code> defines the custom image and  model information, which in turn references <code>models/model.py</code> containing the model implementation we will make available through NOS. The <code>serve.yaml</code> is set up as follows: <pre><code>images:\n    animate-diff-gpu:\n        base: autonomi/nos:latest-gpu\n        pip:\n        - diffusers==0.24.0\n        - transformers==4.35.2\n        - accelerate==0.23.0\n\nmodels:\n    animate-diff:\n    model_cls: AnimateDiff\n    model_path: models/model.py\n    default_method: __call__\n    runtime_env: animate-diff-gpu\n</code></pre></p> <p>Our custom model will run inside of <code>animate-diff-gpu</code>, which is derived from  <code>latest-gpu</code> and adds a few huggingface packages version locked to avoid any dependency issues for this specific model. If your model runs out of the box with the base nos dependencies then this shouldn't be necessary.</p> <p>Next we define the model itself by specifying a few fields. The <code>model_cls</code> maps to the  AnimateDiff class defined in <code>models/model.py</code>. The <code>model_path</code> should link to the model implementation file. <code>default_method</code> will be the entrypoint that gets called when we register a client module against our custom model id (<code>animate-diff</code>). Finally, we  set the <code>runtime_env</code> to the image we defined above (<code>animate-diff-gpu</code>).</p> <ol> <li>Serving our custom model</li> </ol> <p>We can now serve the model inside of <code>examples/aniamtediff</code> with  <pre><code>nos serve up -c serve.yaml\n</code></pre></p> <p>We're now ready to run animatediff.</p> <ol> <li>Generate a gif</li> </ol> <p>We create a client module as we would for any default NOS model and pass in a prompt:</p> <pre><code>from nos.client import Client, TaskType\n\nclient = Client()\nclient.WaitForServer()\nclient.IsHealthy()\n\nmodel = client.Module(\"animate-diff\")\nresponse = model(prompts=[\"astronaut on the moon, hdr, 4k\"], _stream=True)\n\nfrom PIL import Image, ImageSequence\n\nframes = [frame for frame in response]\nframes[0].save('output.gif', save_all=True, append_images=frames[1:], loop=0)\n</code></pre>"},{"location":"docs/demos/discord-bot.html","title":"Building an image generation bot with NOS + Discord","text":"<ol> <li> <p>Registering a discord bot to create our API key</p> <p>This can be done through the discord developer guide. You will need a discord account as well as a server you wish to add the bot to. When we're finished, <code>nos-bot</code> will accept image generation requests from users on this server and post the resulting images to the main channel. Once you have your bot API key, add it to your local environment like so: <pre><code>export BOT_TOKEN=$YOUR_DISCORD_API_TOKEN\n</code></pre></p> </li> <li> <p>Setting up a NOS client to generate images</p> <p>NOS comes with an endpoint for Stable Diffusion V2 from HuggingFace, so all we need to do is init a server on our machine and verify we can connect to it from the client: <pre><code>import nos\nfrom nos.client import Client, TaskType\nimport os\n\n# Init nos server\nnos.init(runtime='gpu')\nclient = Client()\nclient.WaitForServer()\nassert client.IsHealthy()\n</code></pre></p> <p>See <code>examples/notebook/inference-client-example.ipynb</code> for a better overview. Nos will initialize a GPU-ready container on our machine and return to the client that it's ready to go.</p> </li> <li> <p>The Discord interface</p> <p>Next we need a way to handle message requests on our server. Discord.py makes callbacks pretty easy:</p> <pre><code>intents = discord.Intents.default()\nintents.message_content = True\n\nbot = commands.Bot(command_prefix='$', intents=intents)\n\n@bot.command()\nasync def generate(ctx, *, prompt):\n    response = client.Run(\n        \"stabilityai/stable-diffusion-2\",\n        inputs={\n            \"prompts\": [prompt],\n            \"width\": 512,\n            \"height\": 512,\n            \"num_images\": 1\n        }\n    )\n    image, = response[\"images\"]\n\n    tmp_file_path = \"image.png\"\n    image.save(tmp_file_path)\n    with open(tmp_file_path, \"rb\") as img_file:\n        await ctx.send(file=discord.File(img_file))\n\n    os.remove(tmp_file_path)\n</code></pre> <p>We need the <code>message_content</code> intent so we can access the contents of user messages to retrieve image prompts. We'll parse generation requests as <code>$generate image prompt here...</code>. Any messages beginning with <code>$generate</code> will be sent to Nos for image generation. The rest of our message handler is pretty straightforward: we use the client to generate a set of images, then we retrieve the first result from the list, save it locally, and call <code>ctx.send</code> to upload the image with the discord <code>File</code> interface.</p> </li> <li> <p>Time to run the server</p> <p>The full server is only ~40 LOC: <pre><code>#!/usr/bin/env python\nimport io\nimport os\n\nimport discord\nfrom discord.ext import commands\n\nimport nos\nfrom nos.client import Client\n\n# Init nos server\nnos.init(runtime='gpu')\nclient = Client()\nclient.WaitForServer()\nassert client.IsHealthy()\n\nintents = discord.Intents.default()\nintents.message_content = True\n\nbot = commands.Bot(command_prefix='$', intents=intents)\n\n@bot.command()\nasync def generate(ctx, *, prompt):\n    response = client.Run(\n        \"stabilityai/stable-diffusion-2\",\n        inputs={\n            \"prompts\": [prompt],\n            \"width\": 512,\n            \"height\": 512,\n            \"num_images\": 1\n        }\n    )\n    image, = response[\"images\"]\n\n    image_bytes = io.BytesIO()\n    img.save(image_bytes, format=\"PNG\")\n    image_bytes.seek(0)\n    await ctx.send(file=discord.File(image_bytes, filename=f\"{ctx.message.id}.png\"))\n\nbot_token = os.environ.get(\"BOT_TOKEN\")\nif bot_token is None:\n    raise Exception(\"BOT_TOKEN environment variable not set\")\n\nbot.run(bot_token)\n</code></pre></p> <p>We should be all set, kick off the bot with <code>python examples/discord/app/bot.py</code></p> <p></p> </li> </ol>"},{"location":"docs/demos/profiling-models-with-nos.html","title":"Profiling models with NOS","text":"<p>(Originally published at https://scottloftin.substack.com/p/lets-build-an-ml-sre-bot-with-nos)</p> <p>In this post, we'll be going over a set of experiments we've run with the NOS profiler and Skypilot to automatically answer questions about your infra using retrieval on top of pricing and performance data.</p> <p>Figuring out the best platform for a given model begins with benchmarking, and unfortunately today this is still somewhat painful on Nvidia hardware let alone other platforms. Most folks rely on leaderboards published by Anyscale, Huggingface, Martian (my personal favorite!), and many others, but setting aside debates over methodology and fairness, we are still mostly looking at quoted numbers for a top-level inference API without a lot of knobs to turn on the underlying HW. NOS provides a profiling tool that can benchmark Text/Image embedding, Image Generation and Language across GPUs and across clouds. Let\u2019s start with some local numbers on a 2080:</p> <pre><code>nos profile method --method encode_image\nnos profile list\n</code></pre> <p></p> <p>We see a breakdown across four different image embedding models including the method and task (interchangeable in this case, each CLIP variant will support both Image and Text embedding as methods), the Iterations per Second, GPU memory footprint (how much space did this model have to allocate) and finally the GPU utilization, which measures how efficiently we are using the HW (in a very broad sense). A few things to note: the image size is fixed to 224X224X1 across all runs with a batch size of 1. In practice, the Iterations/Second will depend tremendously on tuning the batch size and image resolution for our target HW, which will be the subject of a followup post. For now, we\u2019ll take these numbers at face value and see what we can work out about how exactly to run a large embedding workload. We\u2019re going to use Skypilot to deploy the profiler to a Tesla T4 instance on GCP:</p> <pre><code>sky launch -c nos-profiling-service skyserve.dev.yaml --gpus=t4\n</code></pre> <p>This will run the above model variants on our chosen instance and write everything to a bucket in GCS as a JSON file, which I\u2019ve already downloaded:</p> <pre><code>[\n    {\n        \"namespace\":\"nos::openai\\/clip-vit-base-patch32\",\n        \"profiling_data\":{\n            \"init::memory_gpu::pre\":1001259008,\n            \"init::memory_cpu::pre\":18951331840,\n            \"init::memory_gpu::post\":1640890368,\n            \"init::memory_cpu::post\":19440001024,\n            \"forward::memory_gpu::pre\":1640890368,\n            \"forward::memory_cpu::pre\":19440001024,\n            \"forward::memory_gpu::post\":1642987520,\n            \"forward::memory_cpu::post\":19440001024,\n            \"forward_warmup::execution\":{\n                \"num_iterations\":436,\n                \"total_ms\":2003.6902427673,\n                \"cpu_utilization\":9.8,\n                \"gpu_utilization\":38\n            },\n            \"forward::execution\":{\n                \"num_iterations\":1101,\n                \"total_ms\":5002.1767616272,\n                \"cpu_utilization\":9.8,\n                \"gpu_utilization\":38\n            },\n            \"cleanup::memory_gpu::pre\":1642987520,\n            \"cleanup::memory_cpu::pre\":19443097600,\n            \"cleanup::memory_gpu::post\":1003356160,\n            \"cleanup::memory_cpu::post\":19445149696,...\n</code></pre> <p>This snippet shows a breakdown of memory allocation and runtime across various stages of execution (we\u2019re mostly interested in forward::execution). Even this short list of profiling info is quite a lot to pick apart: we would ordinarily be doing a lot of envelope math and might scrub through each iteration in the chrome profiler, but lets see if we can streamline things a bit with more 'modern' tools.</p>"},{"location":"docs/demos/profiling-models-with-nos.html#our-infrabot-chat-assistant","title":"\ud83e\udd16 Our InfraBot Chat Assistant","text":"<p>The OpenAI assistants API is somewhat unstable at the moment, but after a few tries it was able to ingest the profiling catalog as raw JSON and answer a few questions about its contents. Let\u2019s start simple:</p> <p>Hey InfraBot, can you list the models in the profiling catalog by iterations per second?</p> <p></p> <p>Ok, our raw profiling data is slowly becoming more readable. Let\u2019s see how this all scales with the number of embeddings:</p> <p>Can you compute how long it would take to generate embeddings with each model for workload sizes in powers of 10, starting at 1000 image embeddings and ending at 10,000,000. Please plot these for each model in a graph.</p> <p></p> <p>Reasonable: runtime will depend linearly on total embeddings (again, we\u2019re using batch size 1 for illustration purposes).</p> <p>The more interesting question is: what hardware should we use given price constraints? While ChatGPT can probably provide a solid high level answer describing the tradeoffs between on-demand, spot and reserved instances, as well as the value of performance of the underlying card relative to time spent on other tasks like copying data, we\u2019ll need to provide hard numbers on instance prices if we want something concrete.</p>"},{"location":"docs/demos/profiling-models-with-nos.html#adding-pricing-information-with-skypilot","title":"\ud83d\udcb5 Adding Pricing Information with Skypilot","text":"<p>Skypilot proves a utility for fetching pricing and availability for a variety of instance types across the big 3 CSPs (AWS, Azure and GCP). I was able to generate a summary below (lightly edited for formatting):</p> <pre><code>GPU  QTY  CLOUD  INSTANCE_TYPE          HOURLY_PRICE  HOURLY_SPOT_PRICE  \nT4   1    AWS    g4dn.xlarge            $ 0.526       $ 0.063            \nT4   1    AWS    g4dn.2xlarge           $ 0.752       $ 0.091            \nT4   1    AWS    g4dn.4xlarge           $ 1.204       $ 0.141            \nT4   1    AWS    g4dn.8xlarge           $ 2.176       $ 0.255            \nT4   1    AWS    g4dn.16xlarge          $ 4.352       $ 0.483            \nT4   4    AWS    g4dn.12xlarge          $ 3.912       $ 0.528            \nT4   8    AWS    g4dn.metal             $ 7.824       $ 0.915            \nT4   1    Azure  Standard_NC4as_T4_v3   $ 0.526       $ 0.053            \nT4   1    Azure  Standard_NC8as_T4_v3   $ 0.752       $ 0.075            \nT4   1    Azure  Standard_NC16as_T4_v3  $ 1.204       $ 0.120            \nT4   4    Azure  Standard_NC64as_T4_v3  $ 4.352       $ 0.435            \nT4   1    GCP    (attachable)           $ 0.350       $ 0.070            \nT4   2    GCP    (attachable)           $ 0.700       $ 0.140            \nT4   4    GCP    (attachable)           $ 1.400       $ 0.279    \n</code></pre> <p>Ok, lets add some dollar signs to our plot above:</p> <p>Can you compute how much it would cost on a T4 with 1 GPU to generate embeddings with the cheapest model for workloads of powers of 10, starting at 1000 image embeddings and ending at 10,000,000. Please plot these in a graph.</p> <p></p> <p>The above looks reasonable assuming a minimum reservation of 1 hour (we aren\u2019t doing serverless; we need to pay for the whole instance for the whole hour in our proposed cloud landscape). For 10 million embeddings, the total is something like 13 hours, so assuming an on-demand price of $0.35 we have $0.35*13 ~= $4.55, pretty close to the graph. But what if we wanted to index something like YouTube with ~500PB of videos? Ok, maybe not the whole site, but a substantial subset, maybe 10^11 images. If we extrapolate the above we\u2019re looking at $40,000 in compute, which we would probably care about fitting to our workload. In particular, we might go with a reserved rather than an on-demand instance for a ~%50 discount, but at what point does that pay off? Unfortunately at time of writing, Skypilot doesn\u2019t seem to include reserved instance pricing by default, but for a single instance type it\u2019s easy enough to track down and feed to InfraBot: a 1 Year commitment brings us down to $0.220 per GPU, and a 3 Year commitment to $0.160 per GPU. It\u2019s still higher than the spot price of course, but at this scale its reasonable to assume some SLA that prevents us from halting indexing on preemption. Let\u2019s see if we can find a break-even point.</p> <p>Can you add the cost to reserve a 1 and 3 year instance? A 1 year reservation is $0.220 per gou per hour, and a 3 year reservation is $0.160 per gpu per hour.</p> <p></p> <p>Looks like we need to go a little further to the right</p> <p>Ok can you do the same plot, but at 10^9, 10^10, and 10^11</p> <p></p> <p>10^10 embeddings at $0.35/hr is about $4,860, so this looks roughly correct. 10 Billion embeddings is about 100,000 Hours of (low resolution) video at full 30FPS, so while it\u2019s quite large its not completely unheard of for a larger video service.</p>"},{"location":"docs/demos/profiling-models-with-nos.html#the-verdict","title":"The Verdict?","text":"<p>This is a very simple example of a line of questioning that might confront someone managing an ML service, with a fairly simple answer that was straightforward to generate with an already non-bleeding-edge LLM. With the right catalog data and a lot of patience, it stands to reason that we could answer far more sophisticated questions about inference costs and even automatically act on these decision points by generating Infra-as-Code on the fly. In any case, it\u2019s a useful way to pick apart what are normally very opaque inference stats, and something I\u2019ve been pulling out of my own toolkit more and more. Just make sure to check the math every now and again!</p>"},{"location":"docs/demos/video-search.html","title":"Build a video search engine","text":"<p>In this demo, we\u2019ll use NOS to build an end-to-end semantic video search application. Let's first start the nos server.</p> <pre><code>import nos\n\nnos.init(runtime=\"auto\")\n</code></pre>"},{"location":"docs/demos/video-search.html#frame-inference","title":"Frame Inference","text":"<p>Let's embed the video frame-by-frame with NOS. We'll start by connecting a client to the NOS server:</p> <pre><code>from nos.common.io.video.opencv import VideoReader\nfrom nos.client import Client\nfrom nos.test.utils import get_benchmark_video\n\n# Start the client\nclient = Client()\nclient.WaitForServer()\nclient.IsHealthy()\n\n# Load the video\nFILENAME = get_benchmark_video()\nvideo = VideoReader(str(FILENAME))\n</code></pre> <p>Now lets use the client to embed the video frame by frame into a stack of feature vectors. This should take a couple of minutes:</p> <pre><code>import numpy as np\nfrom PIL import Image\n\nfrom nos.common import tqdm\nfrom nos.test.utils import get_benchmark_video\n\n# Initialize the openai/clip model as a module\nclip = client.Module(\"openai/clip\", shm=True)\n\n# Extract features from the video on a frame-level basis\n# Note: we resize the image to 224x224 before encoding\nfeatures = [\n    clip.encode_image(images=Image.fromarray(img).resize((224, 224)))[\"embedding\"] \n    for img in tqdm(video)\n]\n\n# Stack and normalize the features so that they are unit vectors\nvideo_features = np.vstack(features)\nvideo_features /= np.linalg.norm(video_features, axis=1, keepdims=True)\n</code></pre> <p>Let's define our search function. we'll embed the text query (using the NOS openai/clip endpoint) and dot it with the video features to generate per-frame similarity scores before returning the top result.</p> <pre><code>def search_video(query: str, video_features: np.ndarray, topk: int = 3):\n    \"\"\"Semantic video search demo in 8 lines of code\"\"\"\n    # Encode text and normalize\n    text_features = clip.encode_text(texts=query)[\"embedding\"].copy()\n    text_features /= np.linalg.norm(text_features, axis=1, keepdims=True)\n\n    # Compute the similarity between the search query and each video frame\n    similarities = (video_features @ text_features.T)\n    best_photo_idx = similarities.flatten().argsort()[-topk:][::-1]\n\n    # Display the top k frames\n    results = np.hstack([video[int(frame_id)] for frame_id in best_photo_idx])\n    filepath = '_'.join(query.split(' ')) + '.png'\n    display(Image.fromarray(results).resize((600, 400)))\n</code></pre> <p>Now let's try out a few queries:</p> <p><pre><code>search_video(\"bakery with bread on the shelves\", video_features, topk=1)\n</code></pre> </p> <pre><code>search_video(\"red car on a street\", video_features, topk=1)\n</code></pre> <p></p> <pre><code>search_video(\"bridge over river\", video_features, topk=1)\n</code></pre> <p></p>"},{"location":"docs/guides/running-custom-models.html","title":"Running custom models","text":"<p>Advanced topic</p> <p>This guide is for advanced users of the NOS server-side custom model registry. If you're looking for a way to quickly define your custom model and runtime for serving purposes, we recommend you go through the serving custom models guide first. </p> <p>In this guide, we will walk through how to run custom models with NOS. We will use the OpenAI CLIP model from the popular HuggingFace library to load the model, and then use <code>nos</code> to wrap and execute the model at scale.</p>"},{"location":"docs/guides/running-custom-models.html#defining-the-custom-model","title":"\ud83d\udc69\u200d\ud83d\udcbb Defining the custom model","text":"<p>Here we're using the popular OpenAI CLIP for extracting embeddings using the Huggingface <code>transformers</code> <code>CLIPModel</code>.</p> <pre><code>from typing import Union, List\nfrom PIL import Image\n\nimport numpy as np\nimport torch\n\nclass CLIP:\n    \"\"\"Text and image encoding using OpenAI CLIP\"\"\"\n    def __init__(self, model_name: str = \"openai/clip-vit-base-patch32\"):\n        from transformers import CLIPModel\n        from transformers import CLIPProcessor, CLIPTokenizer\n\n        self.processor = CLIPProcessor.from_pretrained(model_name)\n        self.tokenizer = CLIPTokenizer.from_pretrained(model_name)\n\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.model = CLIPModel.from_pretrained(model_name).to(device)\n        self.model.eval()\n        self.device = self.model.device\n\n    def encode_image(self, images: Union[Image.Image, np.ndarray, List[Image.Image], List[np.ndarray]]):\n        \"\"\"Encode image into an embedding.\"\"\"\n        with torch.inference_mode():\n            inputs = self.processor(images=images, return_tensors=\"pt\").to(self.device)\n            return self.model.get_image_features(**inputs).cpu().numpy()\n\n    def encode_text(self, texts: Union[str, List[str]]) -&gt; np.ndarray:\n        \"\"\"Encode text into an embedding.\"\"\"\n        with torch.inference_mode():\n            if isinstance(texts, str):\n                texts = [texts]\n            inputs = self.tokenizer(\n                texts,\n                padding=True,\n                return_tensors=\"pt\",\n            ).to(self.device)\n            text_features = self.model.get_text_features(**inputs)\n            return text_features.cpu().numpy()\n</code></pre>"},{"location":"docs/guides/running-custom-models.html#wrapping-the-custom-model","title":"\ud83d\udce6 Wrapping the custom model","text":"<p>In the section below, we'll show you a straightforward way to wrap the CLIP model with <code>nos</code> and run it at scale. In theory, you can wrap any custom Python class that is serializable with <code>cloudpickle</code>. Models are wrapped with the <code>ModelSpec</code> class, which is a serializable specification of a model. In this example, we'll use the <code>ModelSpec.from_cls</code> method to wrap the CLIP model.</p> <pre><code>from nos.common import ModelSpec\nfrom nos.manager import ModelManager, ModelHandle\n\n# Create a model spec from the CLIP class\nspec = ModelSpec.from_cls(\n    CLIP,\n    init_args=(),\n    init_kwargs={\"model_name\": \"openai/clip-vit-base-patch32\"},\n)\n\n# Create a model manager to load the CLIP model\nhandle: ModelHandle = manager.load(spec)\n\n# Encode images just like using custom methods `encode_image`\nimg_embedding = handle.encode_image(images=[img])\n\n# Encode text just like using custom methods `encode_text`\ntxt_embedding = handle.encode_text(texts=[\"fox jumped over the moon\"])\n</code></pre> <p>As you can see, we can use the <code>ModelHandle</code> to call the underlying methods <code>encode_image</code> and <code>encode_text</code> just like we would with the original <code>CLIP</code> class. The <code>ModelHandle</code> is a logical handle for the model that allows us to run the model at scale without having to worry about the underlying details of the model.</p>"},{"location":"docs/guides/running-custom-models.html#scaling-the-model","title":"\ud83d\ude80 Scaling the model","text":"<p>Once the model handle has been created, we can also use it to scale the model across multiple GPUs, or even multiple nodes. <code>ModelHandle</code> exposes a <code>scale()</code> method that allows you to manually specify the number of replicas to scale the model. Optionally, you can also specify a more advanced NOS feature where the number of replicas is automatically inferred based on the memory overhead of the model via <code>scale(replicas=\"auto\")</code>.</p> <p>We continue considering the example above and scale the model to 4 replicas. In order to use all the underlying replicas effectively, we need to ensure that the calls to the underlying methods <code>encode_image</code> and <code>encode_text</code> are no longer blocking. In other words, we need to ensure that the calls to the underlying methods are asynchronous and can fully utilize the model replicas without blocking on each other. NOS provides a few convenience methods to <code>submit</code> tasks and retrieve results asynchronously using it's <code>handle.results</code> API.</p> <pre><code># Scale the above model handle to 4 replicas\nhandle.scale(replicas=4)\nprint(handle)\n\n# Asynchronously encode images using the `encode_image.submit()`.\n# Every custom method is patched with a `submit()` method that allows you to asynchronously\n# submit tasks to the underlying model replicas.\ndef encode_images_imap(images_generator):\n    # Iterate over the images generator\n    for images in images_generator:\n        # Submit the task to the underlying model replicas\n        handle.encode_image.submit(images=images)\n        # Wait for the results to be ready before yielding the next batch\n        if handle.results.ready():\n            yield handle.results.get_next()\n    # Yeild all the remaining results\n    while not handle.results.ready():\n        yield handle.results.get_next()\n\nimages_generator = VideoReader(FILENAME)\n# Encode images asynchronously\nfor embedding in encode_images_imap(images_generator=images):\n    # Do something with the image embeddings\n</code></pre> <p>In the example above, we load images from a video file and asynchronously submit <code>encode_image</code> tasks to the 4 replicas we trivially created using the <code>handle.scale(replicas=4)</code> call. We showed how you could implement a strawman, yet performant <code>imap</code> implementation that asynchronously submits tasks to the underlying replicas and yields the results as they become available. This allows us to fully utilize the underlying replicas without blocking on each other, and thereby fully utilizing the underlying hardware.</p>"},{"location":"docs/guides/running-custom-models.html#running-models-in-a-custom-runtime-environment","title":"\ud83d\udee0\ufe0f Running models in a custom runtime environment","text":"<p>For custom models that require execution in a custom runtime environment (e.g. with <code>TensorRT</code> or other library dependencies), we can specify the runtime environment via the <code>runtime_env</code> argument in the <code>ModelSpec</code>.</p> <pre><code>class CustomModel:\n    \"\"\"Custom inference model with scikit-learn.\"\"\"\n\n    def __init__(self, model_name: str = \"fake_model\"):\n        \"\"\"Initialize the model.\"\"\"\n        import sklearn  # noqa: F401\n\n    def __call__(self, images: Union[np.ndarray, List[np.ndarray]], n: int = 1) -&gt; np.ndarray:\n        ...\n\n\n# Create a model spec with a custom runtime environment (i.e. with scikit-learn installed)\nspec = ModelSpec.from_cls(\n    CustomModel,\n    init_args=(),\n    init_kwargs={\"model_name\": \"fake_model\"},\n    runtime_env=RuntimeEnv.from_packages([\"scikit-learn\"]),\n)\n</code></pre> <p>For more details about custom runtime environments, please see the runtime environments section.</p>"},{"location":"docs/guides/running-inference.html","title":"Running inference","text":"<p>Note</p> <p>In this section, we expect that you have already installed NOS and have already started the server.</p> <pre><code>import nos\n\nnos.init(runtime=\"auto\")\n</code></pre>"},{"location":"docs/guides/running-inference.html#connecting-to-the-nos-server","title":"Connecting to the NOS Server","text":"<p>You can now send inference requests using the NOS client. Let's start by importing the NOS client and creating an <code>Client</code> instance. The client instance is used to send inference requests to the NOS server via gRPC.</p> <pre><code>from nos.client import Client, TaskType\n\n# Create a client that connects to the server via gRPC (over 50051)\nclient = Client()\n\n# We provide helper functions to wait for the server to be ready\n# if the server is simultaneously spun up in a separate process.\nclient.WaitForServer()\n\n# Finally, we can check if the server is healthy.\nclient.IsHealthy()\n</code></pre>"},{"location":"docs/guides/running-inference.html#running-inference-with-the-clientmodule-interface","title":"Running Inference with the <code>client.Module</code> Interface","text":"<p>NOS provides a <code>client.Module</code> interface to get model handles for remote-model execution. Let's see an example of running <code>yolox/nano</code> to run 2D object detection on a sample image.</p> <pre><code># Get a model handle for yolox/nano\ndetect2d = client.Module(\"yolox/nano\")\n\n# Run inference on a sample image\nimg = Image.open(\"sample.jpg\")\npredictions = detect2d(images=[img])\n</code></pre> <p>In essense, the <code>client.Module</code> is an inference <code>Module</code>  that provides a logical handle for the model on the remote server. The model handle could contain multiple replicas, or live in a specialized runtime (GPU, ASICs), however, the user does not need to be aware of these abstractions. Instead, you can simply call the model as a regular Python function where the task gets dispatched to the associated set of remote workers.</p>"},{"location":"docs/guides/running-inference.html#more-examples","title":"More examples","text":""},{"location":"docs/guides/running-inference.html#text-to-image-generation-with-stablediffusionv2","title":"Text-to-image generation with StableDiffusionV2","text":"<pre><code>prompts = [\"fox jumped over the moon\", \"fox jumped over the sun\"]\nsdv2 = client.Module(\"stabilityai/stable-diffusion-2\")\nimages = sdv2(inputs={\n    \"prompts\": prompts, \"width\": 512, \"height\": 512, \"num_images\": 1\n})\nimages[0]\n</code></pre>"},{"location":"docs/guides/running-inference.html#image-embedding-with-openai-clip","title":"Image-embedding with OpenAI CLIP","text":"<pre><code>clip = client.Module(\"openai/clip\")\npredictions = clip.encode_image(inputs={\"images\": [img]})\npredictions[\"embedding\"].shape\n</code></pre>"},{"location":"docs/guides/running-inference.html#text-embedding-with-openai-clip","title":"Text-embedding with OpenAI CLIP","text":"<pre><code>clip = client.Run(\"openai/clip\")\npredictions = clip.encode_text(inputs={\n    \"texts\": [\"fox jumped over the mooon\", \"fox jumped over the sun\"]\n})\npredictions[\"embedding\"].shape\n</code></pre>"},{"location":"docs/guides/serving-custom-models.html","title":"Serving custom models","text":"<p>In this guide, we will walk you through how to serve custom models with NOS. We will use the WhisperX model to build a custom runtime environment with docker, load the model and serve it via a gRPC/REST API. Feel free to navigate to nos-playground/examples/whisperx for a full working example.</p> <p>Here's the a short demo of the serving developer-experience:</p> <p></p>"},{"location":"docs/guides/serving-custom-models.html#defining-the-custom-model","title":"\ud83d\udc69\u200d\ud83d\udcbb Defining the custom model","text":"<p>The first step is to define the custom model in <code>models/whisperx.py</code>. Here we're using the popular WhisperX for transcribing audio files. Let's define a simple <code>WhisperX</code> class that wraps the <code>whisperx</code> package, loads the model and transcribes an audio file to a Python dictionary given its path.</p> models/whisperx.py<pre><code>from pathlib import Path\nfrom typing import Any, Dict, List\n\nimport torch\n\nclass WhisperX:\n    def __init__(self, model_name: str = \"large-v2\"):\n        import whisperx\n\n        self.device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.device = torch.device(self.device_str)\n        self.model = whisperx.load_model(model_name, self.device_str, compute_type=\"float16\")\n        self._load_align_model = whisperx.load_align_model\n        self._align = whisperx.align\n\n    def transcribe(\n        self,\n        path: Path,\n        batch_size: int = 24,\n        align_output: bool = True,\n        language_code: str = \"en\",\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Transcribe the audio file.\"\"\"\n        with torch.inference_mode():\n            result: Dict[str, Any] = self.model.transcribe(str(path), batch_size=batch_size)\n            if align_output:\n                alignment_model, alignment_metadata = self._load_align_model(\n                    language_code=language_code, device=self.device_str\n                )\n                result = self._align(\n                    result[\"segments\"],\n                    alignment_model,\n                    alignment_metadata,\n                    str(path),\n                    self.device_str,\n                    return_char_alignments=False,\n                )\n        return result\n</code></pre> Modular custom model registry <p>You will note that this file has nothing to do with NOS or it's dependencies. It is simply a regular Python class that wraps the <code>whisperx</code> package and loads the model. The <code>transcribe</code> method simply calls the <code>transcribe</code> method of the whisperx <code>model</code> and returns the result. This modularity is very much intentional as we would like to make sure that developers are not required to lock-in to any particular serving framework (i.e. ours), and instead focus on their specific modeling needs. </p>"},{"location":"docs/guides/serving-custom-models.html#defining-the-custom-runtime-environment","title":"\ud83d\udce6 Defining the custom runtime environment","text":"<p>In the <code>models/whisperx.py</code> example shown above, we import <code>whisperx</code> which is a Python package that is not available in the default NOS runtime environment. To serve the model, we need to define a custom runtime environment that includes the <code>whisperx</code> package. We can do this by creating a custom docker runtime that installs the <code>whisperx</code> package and any other dependencies.  o With NOS, you simply define the custom runtime environment as part of the \"images\" key in a <code>serve.yaml</code> file. In the example below, we define a custom runtime environment called <code>whisperx-gpu</code> that is based on the <code>autonomi/nos:0.1.0-gpu</code> docker image. We then install the <code>whisperx</code> package and any other dependencies using the <code>pip</code> and <code>run</code> sub-commands.</p> <p>Note</p> <p>You can look at the full list of <code>serve</code> CLI options here. The full <code>serve.yaml</code> specification is available here.</p> serve.yaml<pre><code>images:\n  whisperx-gpu:\n    base: autonomi/nos:0.1.0-gpu\n    pip:\n      - torchaudio&gt;=2\n      - faster-whisper&gt;=0.8\n      - pyannote.audio==3.0.1\n      - transformers\n      - ffmpeg-python&gt;=0.2\n      - pandas\n      - setuptools&gt;=65\n      - nltk\n    workdir: /app/whisperx\n    run:\n      - pip install --no-deps git+https://github.com/m-bain/whisperX.git\n\nmodels:\n  mbain-whisperx:\n    model_cls: WhisperX\n    model_path: models/whisperx.py\n    default_method: transcribe\n    runtime_env: whisperx-gpu\n</code></pre>"},{"location":"docs/guides/serving-custom-models.html#registering-the-custom-whisperx-model","title":"\ud83d\udce6 Registering the custom whisperx model","text":"<p>The <code>serve.yaml</code> file also allows you to specify  the custom <code>mbain-whisperx</code> model that needs to be registered with NOS before you can serve it. The <code>model_cls</code> key specifies the class that we want to wrap and serve, and the <code>model_path</code> key specifies the corresponding path to the <code>whisperx.py</code> file. The <code>default_method: transcribe</code> key specifies the default method to call when the model is served. Finally, the <code>runtime_env: whisperx-gpu</code> key specifies the custom runtime docker environment that we defined above.</p> <p>Via the <code>serve.yaml</code>, NOS automatically registers the new WhisperX model under a unique model-id, i.e. <code>mbain-whisperx</code> in this example. You can use the model-id to serve the model via a gRPC/REST API. For example, in order to use the client to call the <code>transcribe</code> method of the <code>WhisperX</code> model, we can simply do the following:</p> client.py<pre><code>from nos.client import Client\n\nclient = Client()\n</code></pre> <p>Note</p> <p>While the <code>default_method</code> key allows you to specify a specific method to call, all methods of the class are also made available as callables through the exposed gRPC/REST API. For example, </p>"},{"location":"docs/guides/serving-custom-models.html#serving-the-custom-model","title":"\ud83d\ude80 Serving the custom model","text":"<p>Now that we have defined the custom model and runtime environment, we can serve the model with NOS. To do this, we simply run the <code>nos serve</code> command and specify the <code>serve.yaml</code> file. </p> <pre><code>nos serve up -c serve.yaml\n</code></pre> <p>Optionally, you can also start an HTTP gateway so that you can serve the model via a REST API. To do this, you can simply run the following command:</p> <pre><code>nos serve up -c serve.yaml --http\n</code></pre> <p>Note</p> <p>Under the hood, <code>nos serve</code> builds a new custom runtime image based on the <code>whisperx-gpu</code> runtime environment we defined above. It then dyanmically registers the <code>WhsiperX</code> model class and serves it with the NOS inference server, exposing its methods via a gRPC/REST API. In this case, serving is done in a containerized environment, along-side the sidecar HTTP gateway (if specified).</p>"},{"location":"docs/guides/serving-custom-models.html#using-the-custom-model","title":"\ud83d\udce1 Using the custom model","text":"<p>Once the model is served, we can use the client to call the <code>transcribe</code> method of the <code>WhisperX</code> model. </p> client.py<pre><code>from nos.client import Client\n\nclient = Client()\nclient.WaitForServer()  # Wait for the server to start\n\nmodel = client.Module(\"mbain-whisperx\")\nwith client.UploadFile(\"test.wav\") as remote_path:\n    response = model.transcribe(path=remote_path)\n    assert isinstance(response, dict)\n    assert \"segments\" in response\n\nfor item in response[\"segments\"]:\n    assert \"start\" in item\n    assert \"end\" in item\n    assert \"text\" in item\n</code></pre> <p>In the example above, we use the client to call the <code>transcribe</code> method of the <code>WhisperX</code> model. We first upload the <code>test.wav</code> file to the server and then call the <code>transcribe</code> method with the remote path. The <code>transcribe</code> method returns a dictionary with the transcribed segments, just like the original <code>WhisperX</code> model, except that you have used the client-side API to have a remote server do the inference for you. </p> <p>Note</p> <p>In this example, you could have also called <code>model(path=remote_path)</code> directly, since we registered <code>transcribe</code> as the <code>default_method</code> in the <code>serve.yaml</code> file.</p> <p>That's it! You have successfully served a custom model with NOS.</p>"},{"location":"docs/guides/starting-the-server.html","title":"Starting the server","text":"<p>The NOS gRPC server can be started in two ways:</p> <ul> <li>Via the NOS SDK using <code>nos.init(...)</code> (preferred for development)</li> <li>Via the NOS <code>serve</code> CLI.</li> <li>Via Docker Compose (recommended for production deployments)</li> </ul> <p>You can also start the server with the REST API proxy enabled as shown in the 2nd and 4th examples below.</p> Via SDKVia CLIVia Docker Compose (gRPC)Via Docker Compose (gRPC + REST) <p>You can start the nos server programmatically via the NOS SDK: <pre><code>import nos\n\nnos.init(runtime=\"auto\")\n</code></pre></p> <p>You can start the nos server via the NOS <code>serve</code> CLI: <pre><code>nos serve up\n</code></pre></p> <p>Optionally, to use the REST API, you can start an HTTP gateway proxy alongside the gRPC server: <pre><code>nos serve up --http\n</code></pre></p> <p>Navigate to <code>examples/docker</code> to see an example of the YAML specification. You can start the server with the following command:</p> <pre><code>docker-compose -f docker-compose.gpu.yml up\n</code></pre> docker-compose.gpu.yml<pre><code>services:\n  server-gpu:\n    image: autonomi/nos:latest-gpu\n    environment:\n      - NOS_HOME=/app/.nos\n      - NOS_LOGGING_LEVEL=INFO\n    volumes:\n      - ~/.nosd:/app/.nos\n      - /dev/shm:/dev/shm\n    ports:\n      - 50051:50051\n    ipc: host\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - capabilities: [gpu]\n</code></pre> <p>Navigate to <code>examples/docker</code> to see an example of the YAML specification. You can start the server with the following command:</p> <pre><code>docker-compose -f docker-compose.gpu-with-gateway.yml up\n</code></pre> docker-compose.gpu-with-gateway.yml<pre><code>services:\n  server:\n    image: autonomi/nos:latest-gpu\n    command: /app/entrypoint.sh --http\n    environment:\n      - NOS_HOME=/app/.nos\n      - NOS_LOGGING_LEVEL=INFO\n    volumes:\n      - ~/.nosd:/app/.nos\n      - /dev/shm:/dev/shm\n    ports:\n      - 8000:8000\n    ipc: host\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - capabilities: [gpu]\n</code></pre>"},{"location":"docs/guides/starting-the-server.html#api-reference","title":"API Reference","text":""},{"location":"docs/guides/starting-the-server.html#nos.init","title":"nos.init","text":"<pre><code>init(runtime: str = 'auto', port: int = DEFAULT_GRPC_PORT, utilization: float = 1.0, pull: bool = True, logging_level: Union[int, str] = logging.INFO, tag: Optional[str] = None) -&gt; Container\n</code></pre> <p>Initialize the NOS inference server (as a docker daemon).</p> <p>The method first checks to see if your system requirements are met, before pulling the NOS docker image from Docker Hub (if necessary) and starting the inference server (as a docker daemon). You can also specify the runtime to use (i.e. \"cpu\", \"gpu\"), and the port to use for the inference server.</p> <p>Parameters:</p> <ul> <li> <code>runtime</code>               (<code>str</code>, default:                   <code>'auto'</code> )           \u2013            <p>The runtime to use (i.e. \"auto\", \"local\", \"cpu\", \"gpu\"). Defaults to \"auto\". In \"auto\" mode, the runtime will be automatically detected.</p> </li> <li> <code>port</code>               (<code>int</code>, default:                   <code>DEFAULT_GRPC_PORT</code> )           \u2013            <p>The port to use for the inference server. Defaults to DEFAULT_GRPC_PORT.</p> </li> <li> <code>utilization</code>               (<code>float</code>, default:                   <code>1.0</code> )           \u2013            <p>The target cpu/memory utilization of inference server. Defaults to 1.</p> </li> <li> <code>pull</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <p>Pull the docker image before starting the inference server. Defaults to True.</p> </li> <li> <code>logging_level</code>               (<code>Union[int, str]</code>, default:                   <code>INFO</code> )           \u2013            <p>The logging level to use. Defaults to logging.INFO. Optionally, a string can be passed (i.e. \"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\").</p> </li> <li> <code>tag</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The tag of the docker image to use (\"latest\"). Defaults to None, where the appropriate version is used.</p> </li> </ul> Source code in <code>nos/server/__init__.py</code> <pre><code>def init(\n    runtime: str = \"auto\",\n    port: int = DEFAULT_GRPC_PORT,\n    utilization: float = 1.0,\n    pull: bool = True,\n    logging_level: Union[int, str] = logging.INFO,\n    tag: Optional[str] = None,\n) -&gt; docker.models.containers.Container:\n    \"\"\"Initialize the NOS inference server (as a docker daemon).\n\n    The method first checks to see if your system requirements are met, before pulling the NOS docker image from Docker Hub\n    (if necessary) and starting the inference server (as a docker daemon). You can also specify the runtime to use (i.e. \"cpu\", \"gpu\"),\n    and the port to use for the inference server.\n\n\n    Args:\n        runtime (str, optional): The runtime to use (i.e. \"auto\", \"local\", \"cpu\", \"gpu\"). Defaults to \"auto\".\n            In \"auto\" mode, the runtime will be automatically detected.\n        port (int, optional): The port to use for the inference server. Defaults to DEFAULT_GRPC_PORT.\n        utilization (float, optional): The target cpu/memory utilization of inference server. Defaults to 1.\n        pull (bool, optional): Pull the docker image before starting the inference server. Defaults to True.\n        logging_level (Union[int, str], optional): The logging level to use. Defaults to logging.INFO.\n            Optionally, a string can be passed (i.e. \"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\").\n        tag (str, optional): The tag of the docker image to use (\"latest\"). Defaults to None, where the\n            appropriate version is used.\n    \"\"\"\n    # Check arguments\n    available_runtimes = list(InferenceServiceRuntime.configs.keys()) + [\"auto\", \"local\"]\n    if runtime not in available_runtimes:\n        raise ValueError(f\"Invalid inference service runtime: {runtime}, available: {available_runtimes}\")\n\n    # If runtime is \"local\", return early with ray executor\n    if runtime == \"local\":\n        from nos.executors.ray import RayExecutor\n\n        executor = RayExecutor.get()\n        executor.init()\n        return\n\n    # Check arguments\n    if utilization &lt;= 0.25 or utilization &gt; 1:\n        raise ValueError(f\"Invalid utilization: {utilization}, must be in (0.25, 1].\")\n\n    if not isinstance(logging_level, (int, str)):\n        raise ValueError(f\"Invalid logging level: {logging_level}, must be an integer or string.\")\n    if isinstance(logging_level, int):\n        logging_level = logging.getLevelName(logging_level)\n    if logging_level not in (\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"):\n        raise ValueError(f\"Invalid logging level: {logging_level}\")\n\n    if tag is None:\n        tag = __version__\n    else:\n        if not isinstance(tag, str):\n            raise ValueError(f\"Invalid tag: {tag}, must be a string.\")\n        raise NotImplementedError(\"Custom tags are not yet supported.\")\n\n    # Determine runtime from system\n    if runtime == \"auto\":\n        runtime = InferenceServiceRuntime.detect()\n        logger.debug(f\"Auto-detected system runtime: {runtime}\")\n    else:\n        if runtime not in InferenceServiceRuntime.configs:\n            raise ValueError(\n                f\"Invalid inference service runtime: {runtime}, available: {list(InferenceServiceRuntime.configs.keys())}\"\n            )\n\n    # Check if the latest inference server is already running\n    # If the running container's tag is inconsistent with the current version,\n    # we will shutdown the running container and start a new one.\n    containers = InferenceServiceRuntime.list()\n    if len(containers) == 1:\n        logger.debug(\"Found an existing inference server running, checking if it is the latest version.\")\n        if InferenceServiceRuntime.configs[runtime].image not in containers[0].image.tags:\n            logger.info(\n                \"Active inference server is not the latest version, shutting down before starting the latest one.\"\n            )\n            _stop_container(containers[0])\n        else:\n            (container,) = containers\n            logger.info(\n                f\"Inference server already running (name={container.name}, image={container.image}, id={container.id[:12]}).\"\n            )\n            return container\n    elif len(containers) &gt; 1:\n        logger.warning(\"\"\"Multiple inference servers running, please report this issue to the NOS maintainers.\"\"\")\n        for container in containers:\n            _stop_container(container)\n    else:\n        logger.debug(\"No existing inference server found, starting a new one.\")\n\n    # Check system requirements\n    # Note: we do this after checking if the latest\n    # inference server is already running for convenience.\n    _check_system_requirements(runtime)\n\n    # Pull docker image (if necessary)\n    if pull:\n        _pull_image(InferenceServiceRuntime.configs[runtime].image)\n\n    # Start inference server\n    runtime = InferenceServiceRuntime(runtime=runtime)\n    logger.info(f\"Starting inference service: [name={runtime.cfg.name}, runtime={runtime}]\")\n\n    # Determine number of cpus, system memory before starting container\n    # Note (spillai): MacOSX compatibility issue where docker does not have access to\n    # the correct number of physical cores and memory.\n    cl = DockerRuntime.get()._client\n    num_cpus = cl.info().get(\"NCPU\", psutil.cpu_count(logical=False))\n    num_cpus = max(_MIN_NUM_CPUS, utilization * num_cpus)\n    mem_limit = (\n        min(cl.info().get(\"MemTotal\", psutil.virtual_memory().total), psutil.virtual_memory().available) / 1024**3\n    )\n    mem_limit = max(_MIN_MEM_GB, utilization * math.floor(mem_limit))\n    logger.debug(f\"Starting inference container: [num_cpus={num_cpus}, mem_limit={mem_limit}g]\")\n\n    # Start container\n    # TOFIX (spillai): If macosx, shared memory is not supported\n    shm_enabled = NOS_SHM_ENABLED if platform.system() == \"Linux\" else False\n    container = runtime.start(\n        nano_cpus=int(num_cpus * 1e9),\n        mem_limit=f\"{mem_limit}g\",\n        shm_size=f\"{_MIN_SHMEM_GB}g\",\n        ports={f\"{DEFAULT_GRPC_PORT}/tcp\": port},\n        environment={\n            \"NOS_LOGGING_LEVEL\": logging_level,\n            \"NOS_SHM_ENABLED\": int(shm_enabled),\n        },\n    )\n    logger.info(\n        f\"Inference service started: [name={runtime.cfg.name}, runtime={runtime}, image={container.image}, id={container.id[:12]}]\"\n    )\n    return container\n</code></pre>"},{"location":"docs/guides/starting-the-server.html#nos.shutdown","title":"nos.shutdown","text":"<pre><code>shutdown() -&gt; Optional[Union[Container, List[Container]]]\n</code></pre> <p>Shutdown the inference server.</p> Source code in <code>nos/server/__init__.py</code> <pre><code>def shutdown() -&gt; Optional[Union[docker.models.containers.Container, List[docker.models.containers.Container]]]:\n    \"\"\"Shutdown the inference server.\"\"\"\n    # Check if inference server is already running\n    containers = InferenceServiceRuntime.list()\n    if len(containers) == 1:\n        (container,) = containers\n        _stop_container(container)\n        return container\n    if len(containers) &gt; 1:\n        logger.warning(\"\"\"Multiple inference servers running, please report this issue to the NOS maintainers.\"\"\")\n        for container in containers:\n            _stop_container(container)\n        return containers\n    logger.info(\"No active inference servers found, ignoring shutdown.\")\n    return None\n</code></pre>"},{"location":"docs/integrations/discord-skypilot.html","title":"Discord skypilot","text":""},{"location":"docs/integrations/discord-skypilot.html#running-the-discord-bot-with-skypilot","title":"Running the discord bot with skypilot","text":"<p>Follow the instructions in the discord bot guide to generate an API key, and make sure this is written to a <code>.env</code> file as <code>DISCORD_BOT_TOKEN=$YOUR_API_KEY</code>. Deploy the discord image generation bot via skypilot with the provided <code>server.yml</code>:</p> <pre><code>sky launch -c nos-server-gcp server.yaml --env-file=.env\n</code></pre> <p>Note that your chosen instance (GCP, AWS, Azure) must be able to reach the discord server where you want the bot deployed.</p>"},{"location":"docs/integrations/skypilot.html","title":"SkyPilot","text":"<p>In this guide we'll show you how you can deploy the NOS inference server using SkyPilot on any of the popular Cloud Service Providers (CSPs) such as AWS, GCP or Azure. We'll use GCP as an example, but the steps are similar for other CSPs.</p> <p>What is SkyPilot?</p> <p>SkyPilot is a framework for running LLMs, AI, and batch jobs on any cloud, offering maximum cost savings, highest GPU availability, and managed execution. - SkyPilot Documentation.</p>"},{"location":"docs/integrations/skypilot.html#prerequisites","title":"\ud83d\udc69\u200d\ud83d\udcbb Prerequisites","text":"<p>You'll first need to install SkyPilot in your virtual environment / conda environment before getting started. Before getting started, we recommend you go through their quickstart to familiarize yourself with the SkyPilot tool.</p> <pre><code>$ pip install skypilot[gcp]\n</code></pre> <p>If you're installing SkyPilot for use with other cloud providers, you may install any of the relevant extras <code>skypilot[aws,gcp,azure]</code>. `</p>"},{"location":"docs/integrations/skypilot.html#optional-configure-cloud-credentials","title":"[OPTIONAL] Configure cloud credentials","text":"<p>Run <code>sky check</code> for more details and installation instructions.</p>"},{"location":"docs/integrations/skypilot.html#deploying-nos-on-gcp","title":"\ud83d\udce6 Deploying NOS on GCP","text":""},{"location":"docs/integrations/skypilot.html#1-define-your-skypilot-deployment-yaml","title":"1. Define your SkyPilot deployment YAML","text":"<p>First, let's create a <code>sky.yaml</code> YAML file with the following configuration. </p> <pre><code># NOS GPU server deployment on T4\n# Usage: sky launch -c nos-server sky.yaml\n\nname: nos-server\n\nresources:\n  accelerators: T4:1\n  cloud: gcp\n  ports:\n    - 8000\n\nsetup: |\n  # Setup conda environment\n  conda init bash\n  conda create -n nos python=3.10 -y\n  conda activate nos\n\n  # Install docker compose plugin\n  sudo apt-get install -y docker-compose-plugin\n\n  # Install torch-nos\n  pip install torch-nos\n\nrun: |\n  # Activate conda environment\n  conda activate nos\n\n  # Run the server (gRPC + HTTP)\n  nos serve up --http\n</code></pre> <p>Here, we are going to provision a single GPU server on GCP with an NVIDIA T4 GPU and expose ports <code>8000</code> (REST) and <code>50051</code> (gRPC) for the NOS server. </p>"},{"location":"docs/integrations/skypilot.html#2-launch-your-nos-server","title":"\ud83d\ude80 2. Launch your NOS server","text":"<p>Now, we can launch our NOS server on GCP with the following command:</p> <pre><code>$ sky launch -c nos-server sky.yaml\n</code></pre> <p>That's it! You should see the following output:</p> <pre><code>(nos-infra-py38) examples/skypilot spillai-desktop [ sky launch -c nos-server sky.yaml --cloud gcp\nTask from YAML spec: sky.yaml\nI 01-16 09:41:18 optimizer.py:694] == Optimizer ==\nI 01-16 09:41:18 optimizer.py:705] Target: minimizing cost\nI 01-16 09:41:18 optimizer.py:717] Estimated cost: $0.6 / hour\nI 01-16 09:41:18 optimizer.py:717]\nI 01-16 09:41:18 optimizer.py:840] Considered resources (1 node):\nI 01-16 09:41:18 optimizer.py:910] ---------------------------------------------------------------------------------------------\nI 01-16 09:41:18 optimizer.py:910]  CLOUD   INSTANCE       vCPUs   Mem(GB)   ACCELERATORS   REGION/ZONE     COST ($)   CHOSEN\nI 01-16 09:41:18 optimizer.py:910] ---------------------------------------------------------------------------------------------\nI 01-16 09:41:18 optimizer.py:910]  GCP     n1-highmem-4   4       26        T4:1           us-central1-a   0.59          \u2714\nI 01-16 09:41:18 optimizer.py:910] ---------------------------------------------------------------------------------------------\nI 01-16 09:41:18 optimizer.py:910]\nLaunching a new cluster 'nos-server'. Proceed? [Y/n]: y\nI 01-16 09:41:25 cloud_vm_ray_backend.py:4508] Creating a new cluster: 'nos-server' [1x GCP(n1-highmem-4, {'T4': 1}, ports=['8000', '50051'])].\nI 01-16 09:41:25 cloud_vm_ray_backend.py:4508] Tip: to reuse an existing cluster, specify --cluster (-c). Run `sky status` to see existing clusters.\nI 01-16 09:41:26 cloud_vm_ray_backend.py:1474] To view detailed progress: tail -n100 -f /home/spillai/sky_logs/sky-2024-01-16-09-41-16-157615/provision.log\nI 01-16 09:41:29 cloud_vm_ray_backend.py:1912] Launching on GCP us-central1 (us-central1-a)\nI 01-16 09:44:36 log_utils.py:45] Head node is up.\nI 01-16 09:45:43 cloud_vm_ray_backend.py:1717] Successfully provisioned or found existing VM.\nI 01-16 09:46:00 cloud_vm_ray_backend.py:4558] Processing file mounts.\nI 01-16 09:46:00 cloud_vm_ray_backend.py:4590] To view detailed progress: tail -n100 -f ~/sky_logs/sky-2024-01-16-09-41-16-157615/file_mounts.log\nI 01-16 09:46:00 backend_utils.py:1459] Syncing (to 1 node): ./app -&gt; ~/.sky/file_mounts/app\nI 01-16 09:46:05 cloud_vm_ray_backend.py:3315] Running setup on 1 node.\n...\n...\n...\n(nos-server, pid=12112) Status: Downloaded newer image for autonomi/nos:0.1.4-gpu\n(nos-server, pid=12112) docker.io/autonomi/nos:0.1.4-gpu\n(nos-server, pid=12112) 2024-01-16 17:49:09.415 | INFO     | nos.server:_pull_image:235 - Pulled new server image: autonomi/nos:0.1.4-gpu\n(nos-server, pid=12112) \u2713 Successfully generated docker-compose file\n(nos-server, pid=12112) (filename=docker-compose.sky_workdir.yml).\n(nos-server, pid=12112) \u2713 Launching docker compose with command: docker compose -f\n(nos-server, pid=12112) /home/gcpuser/.nos/tmp/serve/docker-compose.sky_workdir.yml up\n(nos-server, pid=12112)  Container serve-nos-server-1  Creating\n(nos-server, pid=12112)  Container serve-nos-server-1  Created\n(nos-server, pid=12112)  Container serve-nos-http-gateway-1  Creating\n(nos-server, pid=12112)  Container serve-nos-http-gateway-1  Created\n(nos-server, pid=12112) Attaching to serve-nos-http-gateway-1, serve-nos-server-1\n(nos-server, pid=12112) serve-nos-server-1        | Starting server with OMP_NUM_THREADS=4...\n(nos-server, pid=12112) serve-nos-http-gateway-1  | WARNING:  Current configuration will not reload as not all conditions are met, please refer to documentation.\n(nos-server, pid=12112) serve-nos-server-1        |  \u2713 InferenceExecutor :: Connected to backend.\n(nos-server, pid=12112) serve-nos-server-1        |  \u2713 Starting gRPC server on [::]:50051\n(nos-server, pid=12112) serve-nos-server-1        |  \u2713 InferenceService :: Deployment complete (elapsed=0.0s)\n(nos-server, pid=12112) serve-nos-http-gateway-1  | INFO:     Started server process [1]\n(nos-server, pid=12112) serve-nos-http-gateway-1  | INFO:     Waiting for application startup.\n(nos-server, pid=12112) serve-nos-http-gateway-1  | INFO:     Application startup complete.\n(nos-server, pid=12112) serve-nos-http-gateway-1  | INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n</code></pre>"},{"location":"docs/integrations/skypilot.html#3-check-the-status-of-your-nos-server","title":"\ud83d\udd0b 3. Check the status of your NOS server","text":"<p>You can check the status of your NOS server with the following command:</p> <pre><code>$ sky status\n</code></pre> <p>You should see the following output:</p> <pre><code>NAME                            LAUNCHED     RESOURCES                                                                  STATUS   AUTOSTOP  COMMAND\nnos-server                      1 min ago    1x GCP(n1-highmem-4, {'T4': 1}, ports=[8000, 50051])                       UP       -         sky launch -c nos-server-...\n</code></pre> <p>Congratulations! You've successfully deployed your NOS server on GCP. You can now access the NOS server from your local machine at <code>&lt;ip&gt;:8000</code> or <code>&lt;ip&gt;:50051</code>. In a new terminal, let's check the health of our NOS server with the following command:</p> <pre><code>$ curl http://$(sky status --ip nos-server):8000/v1/health\n</code></pre> <p>You should see the following output:</p> <pre><code>{\"status\": \"ok\"}\n</code></pre>"},{"location":"docs/integrations/skypilot.html#4-chat-with-your-hosted-llm-endpoint","title":"\ud83d\udcac 4. Chat with your hosted LLM endpoint","text":"<p>You can now chat with your hosted LLM endpoint. Since NOS exposes an OpenAI compatible API via it's <code>/v1/chat/completions</code> route, you can use any OpenAI compatible client to chat with your hosted LLM endpoint. </p> Using cURLUsing an OpenAI compatible clientUsing the OpenAI Python client <pre><code>curl \\\n    -X POST http://$(sky status --ip nos-server):8000/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n        \"messages\": [{\"role\": \"user\", \"content\": \"Tell me a joke in 300 words\"}],\n        \"temperature\": 0.7, \"stream\": true\n      }'\n</code></pre> <p>Below, we show how you can use any OpenAI API compatible client to chat with your hosted LLM endpoint. We will use the popular llm CLI tool from Simon Willison to chat with our hosted LLM endpoint.</p> <pre><code># Install the llm CLI tool\n$ pip install llm\n\n# Install the llm-nosrun plugin to talk to your service\n$ llm install llm-nosrun\n\n# List the models\n$ llm models list\n\n# Chat with your endpoint\n$ NOSRUN_API_BASE=http://$(sky status --ip nos-server):8000/v1 llm -m TinyLlama/TinyLlama-1.1B-Chat-v1.0 \"Tell me a joke in 300 words.\"\n</code></pre> <p>Note: You can also change the <code>NOSRUN_API_BASE</code> to <code>http://localhost:8000/v1</code> to talk to your local NOS server.</p> <p>Below, we show how you can use the OpenAI Python Client to chat with your hosted LLM endpoint.</p> <pre><code>import subprocess\n\nimport openai\n\n\n# Get the output of `sky status --ip nos-server` with subprocess\naddress = subprocess.check_output([\"sky\", \"status\", \"--ip\", \"nos-server\"]).decode(\"utf-8\").strip()\nprint(f\"Using address: {address}\")\n\n# Create a stream and print the output\nclient = openai.OpenAI(api_key=\"no-key-required\", base_url=f\"http://{address}:8000/v1\")\nstream = client.chat.completions.create(\n    model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a joke in 300 words\"}],\n    stream=True,\n)\nfor chunk in stream:\n    print(chunk.choices[0].delta.content or \"\", end=\"\")\n</code></pre> <p>On the first call to the server, the server will download the model from Huggingface, cache it locally and load it onto the GPU. Subsequent calls will not have any of this overhead as the GPU memory for the models will be pinned.</p>"},{"location":"docs/integrations/skypilot.html#5-stop-terminate-your-nos-server","title":"\ud83d\uded1 5. Stop / Terminate your NOS server","text":"<p>Once you're done using your server, you can stop it with the following command:</p> <pre><code>$ sky stop nos-server-gcp\n</code></pre> <p>Alternatively, you can terminate your server with the following command:</p> <pre><code>$ sky down nos-server-gcp\n</code></pre> <p>This will terminate the server and all associated resources (e.g. VMs, disks, etc.).</p>"},{"location":"docs/models/supported-models.html","title":"\ud83e\udde0 Models","text":"<p>This README lists the models supported by NOS, along with their corresponding links to Hugging Face or Torch Hub, and the supported devices (CPU or GPU). Navigate to our models page for more up-to-date information.</p> Modality Task Model Name Supported Devices API \ud83c\udfde\ufe0f Object Detection YOLOX CPU, GPU <pre><code>img = Image.open(\"test.png\")\n\nyolox = client.Module(\"yolox/nano\")\npredictions = yolox(images=img)\n# {\"bboxes\": ..., \"scores\": ..., \"labels\": ...}\n</code></pre> \ud83c\udfde\ufe0f Depth Estimation MiDaS CPU, GPU <pre><code>img = Image.open(\"test.png\")\n\nmodel = client.Module(\"isl-org/MiDaS\")\nresult = model(images=img)\n# {\"depths\": np.ndarray}\n</code></pre> \ud83d\udcdd, \ud83c\udfde\ufe0f Text-Image Embedding OpenAI - CLIP CPU, GPU <pre><code>img = Image.open(\"test.png\")\n\nclip = client.Module(\"openai/clip-vit-base-patch32\")\nimg_vec = clip.encode_image(images=img)\ntxt_vec = clip.encode_text(text=[\"fox jumped over the moon\"])\n</code></pre> \ud83d\udcdd, \ud83c\udfde\ufe0f Text/Input Conditioned Image Segmentation Facebook Research - Segment Anything CPU, GPU <pre><code>img = Image.open(\"test.png\")\n\nmodel = client.Module(\"facebook/sam-vit-large\")\noutputs: List[np.ndarray] = model(images=img, grid_size=20)\n</code></pre> \ud83d\udcdd, \ud83c\udfde\ufe0f Text-to-Image Generation Stability AI - Stable Diffusion XL GPU <pre><code>sdxl = client.Module(\"stabilityai/stable-diffusion-xl-base-1-0\")\nsdxl(prompts=[\"fox jumped over the moon\"],\n     width=1024, height=1024, num_images=1)\n</code></pre> \ud83d\udcdd, \ud83c\udfde\ufe0f Text-to-Image Generation Stability AI - Stable Diffusion 2.1 GPU <pre><code>sdv2 = client.Module(\"stabilityai/stable-diffusion-2-1\")\nsdv2(prompts=[\"fox jumped over the moon\"],\n     width=512, height=512, num_images=1)\n</code></pre> \ud83d\udcdd, \ud83c\udfde\ufe0f Text-to-Image Generation Stability AI - Stable Diffusion 2 GPU <pre><code>sdv2 = client.Module(\"stabilityai/stable-diffusion-2\")\nsdv2(prompts=[\"fox jumped over the moon\"],\n     width=512, height=512, num_images=1)\n</code></pre> \ud83d\udcdd, \ud83c\udfde\ufe0f Text-to-Image Generation RunwayML - Stable Diffusion v1.5 CPU, GPU <pre><code>sdv2 = client.Module(\"runwayml/stable-diffusion-v1-5\")\nsdv2(prompts=[\"fox jumped over the moon\"],\n     width=512, height=512, num_images=1)\n</code></pre> \ud83c\udf99\ufe0f Speech-to-Text OpenAI - Whisper GPU <pre><code>from base64 import b64encode\n\nwhisper = client.Module(\"openai/whisper-large-v2\")\nwith open(\"test.wav\", \"rb\") as f:\n    audio_data = f.read()\n    audio_b64 = b64encode(audio_data).decode(\"utf-8\")\n    transcription = whisper.transcribe(audio=audio_64)\n</code></pre> \ud83c\udf99\ufe0f Text-to-Speech Suno - Bark GPU <pre><code>bark = client.Module(\"suno/bark\")\naudio_data = bark(prompts=[\"fox jumped over the moon\"])\n</code></pre>"},{"location":"examples/inf2/embeddings/index.html","title":"Index","text":""},{"location":"examples/inf2/embeddings/index.html#embeddings-service","title":"Embeddings Service","text":"<p>Start the server via: <pre><code>nos serve up -c serve.yaml --http\n</code></pre></p> <p>Optionally, you can provide the <code>inf2</code> runtime flag, but this is automatically inferred.</p> <pre><code>nos serve up -c serve.yaml --http --runtime inf2\n</code></pre>"},{"location":"examples/inf2/embeddings/index.html#run-the-tests","title":"Run the tests","text":"<pre><code>pytest -sv ./tests/test_embeddings_inf2_client.py\n</code></pre>"},{"location":"examples/inf2/embeddings/index.html#call-the-service","title":"Call the service","text":"<p>You can also call the service via the REST API directly:</p> <pre><code>curl \\\n-X POST http://&lt;service-ip&gt;:8000/v1/infer \\\n-H 'Content-Type: application/json' \\\n-d '{\n    \"model_id\": \"BAAI/bge-small-en-v1.5\",\n    \"inputs\": {\n        \"texts\": [\"fox jumped over the moon\"]\n    }\n}'\n</code></pre>"},{"location":"examples/inf2/sdxl/index.html","title":"Index","text":""},{"location":"examples/inf2/sdxl/index.html#embeddings-service","title":"Embeddings Service","text":"<p>Start the server via: <pre><code>nos serve up -c serve.yaml --http\n</code></pre></p> <p>Optionally, you can provide the <code>inf2</code> runtime flag, but this is automatically inferred.</p> <pre><code>nos serve up -c serve.yaml --http --runtime inf2\n</code></pre>"},{"location":"examples/inf2/sdxl/index.html#run-the-tests","title":"Run the tests","text":"<pre><code>pytest -sv ./tests/test_sdxl_client.py\n</code></pre>"},{"location":"examples/inf2/sdxl/index.html#call-the-service","title":"Call the service","text":"<p>You can also call the service via the REST API directly:</p> <pre><code>curl \\\n-X POST http://&lt;service-ip&gt;:8000/v1/infer \\\n-H 'Content-Type: application/json' \\\n-d '{\n    \"model_id\": \"stabilityai/stable-diffusion-xl-base-1.0-inf2\",\n    \"inputs\": {\n        \"prompts\": [\"hippo with glasses in a library, cartoon styling\"],\n        \"width\": 1024, \"height\": 1024,\n        \"num_images\": 1,\n        \"num_inference_steps\": 50,\n        \"guidance_scale\": 7.5,\n        \"num_images\": 1\n    }\n}'\n</code></pre>"},{"location":"examples/tutorials/index.html","title":"Index","text":""},{"location":"examples/tutorials/index.html#nos-tutorials","title":"\ud83d\udce6 NOS Tutorials","text":"<p>The following tutorials give a brief overview of how to use NOS to serve models.</p> <ul> <li> <code>01-serving-custom-models</code>: Serve a custom GPU model with NOS.</li> <li> <code>02-serving-multiple-methods</code>: Expose several custom methods of a model for serving purposes.</li> <li> <code>03-llm-streaming-chat</code>: Serve an LLM with streaming support (<code>TinyLlama/TinyLlama-1.1B-Chat-v0.1</code>).</li> <li> <code>04-serving-multiple-models</code>: Serve multiple models such as <code>TinyLlama/TinyLlama-1.1B-Chat-v0.1</code> and distil-whisper/distil-small.en on the same GPU with custom model resources -- enable multi-modal applications like audio transcription + summarization on the same device.</li> <li> <code>05-serving-with-docker</code>: Use NOS in a production environment with Docker and Docker Compose.</li> </ul>"},{"location":"examples/tutorials/index.html#running-the-examples","title":"\ud83c\udfc3\u200d\u2642\ufe0f Running the examples","text":"<p>For each of the examples, you can run the following command to serve the model (in one of your terminals):</p> <pre><code>nos serve up -c serve.yaml\n</code></pre> <p>You can then run the tests in the <code>tests</code> directory to check if the model is served correctly:</p> <pre><code>pytest -sv ./tests\n</code></pre> <p>For HTTP tests, you'll need add the <code>--http</code> flag to the <code>nos serve</code> command:</p> <pre><code>nos serve up -c serve.yaml --http\n</code></pre>"},{"location":"examples/tutorials/01-serving-custom-models/index.html","title":"Index","text":""},{"location":"examples/tutorials/01-serving-custom-models/index.html#serving-custom-models","title":"Serving Custom Models","text":"<p>This tutorial shows how to serve a custom model with NOS.</p>"},{"location":"examples/tutorials/01-serving-custom-models/index.html#serve-the-model","title":"Serve the model","text":"<p>The <code>serve.yaml</code> file contains the specification of the custom image that will be used to build the docker runtime image and serve the model using this custom rumtime image. You can serve the model via:</p> <pre><code>nos serve up -c serve.yaml\n</code></pre>"},{"location":"examples/tutorials/01-serving-custom-models/index.html#run-the-tests-via-the-grpc-client","title":"Run the tests (via the gRPC client)","text":"<p>You can now run the tests to check that the model is served correctly:</p> <pre><code>python tests/test_model.py\n</code></pre>"},{"location":"examples/tutorials/02-serving-multiple-methods/index.html","title":"Index","text":""},{"location":"examples/tutorials/02-serving-multiple-methods/index.html#serving-custom-models-with-multiple-methods","title":"Serving Custom Models with multiple methods","text":"<p>This tutorial shows how to serve multiple methods of a custom model with NOS.</p>"},{"location":"examples/tutorials/02-serving-multiple-methods/index.html#serve-the-model","title":"Serve the model","text":"<p>The <code>serve.yaml</code> file contains the specification of the custom image that will be used to build the docker runtime image and serve the model using this custom rumtime image. You can serve the model via:</p> <pre><code>nos serve up -c serve.yaml\n</code></pre> <p>Note: You will notice that in this example, we are deploying the model on the CPU. If you want to deploy the model on GPUs, you can change the <code>deployment.resources.device</code> to <code>gpu</code> in the <code>serve.yaml</code>.</p>"},{"location":"examples/tutorials/02-serving-multiple-methods/index.html#run-the-tests-via-the-grpc-client","title":"Run the tests (via the gRPC client)","text":"<p>You can now run the tests to check that the model is served correctly:</p> <pre><code>python tests/test_model.py\n</code></pre>"},{"location":"examples/tutorials/02-serving-multiple-methods/index.html#call-the-model-via-the-rest-api","title":"Call the model (via the REST API)","text":"<p>You can also call the model via the REST API. In order to do so, you need to start the server with <code>--http</code> flag to enable the HTTP proxy.</p> <pre><code>nos serve up -c serve.yaml --http\n</code></pre> <p>You can then call the model's specific method via:</p> <pre><code>curl \\\n-X POST http://localhost:8000/v1/infer \\\n-H 'Content-Type: application/json' \\\n-d '{\n    \"model_id\": \"custom/clip-model-cpu\",\n    \"method\": \"encode_text\",\n    \"inputs\": {\n        \"texts\": [\"fox jumped over the moon\"]\n    }\n}'\n</code></pre>"},{"location":"examples/tutorials/03-llm-streaming-chat/index.html","title":"Index","text":""},{"location":"examples/tutorials/03-llm-streaming-chat/index.html#serving-llms-with-streaming-support","title":"Serving LLMs with streaming support","text":"<p>This tutorial shows how to serve an LLM with streaming support.</p>"},{"location":"examples/tutorials/03-llm-streaming-chat/index.html#serve-the-model","title":"Serve the model","text":"<p>The <code>serve.yaml</code> file contains the specification of the custom image that will be used to build the docker runtime image and serve the model using this custom rumtime image. You can serve the model via:</p> <pre><code>nos serve up -c serve.yaml --http\n</code></pre>"},{"location":"examples/tutorials/03-llm-streaming-chat/index.html#run-the-tests-via-the-grpc-client","title":"Run the tests (via the gRPC client)","text":"<p>You can now run the tests to check that the model is served correctly:</p> <pre><code>python tests/test_grpc_chat.py\n</code></pre>"},{"location":"examples/tutorials/03-llm-streaming-chat/index.html#run-the-tests-via-the-resthttp-client","title":"Run the tests (via the REST/HTTP client)","text":"<p>You can also run the tests to check that the model is served correctly via the REST API:</p> <pre><code>python tests/test_http_chat.py\n</code></pre>"},{"location":"examples/tutorials/03-llm-streaming-chat/index.html#use-curl-to-call-the-model-via-the-rest-api","title":"Use cURL to call the model (via the REST API)","text":"<p>NOS also exposes an OpenAI API compatible endpoint for such custom LLM models. You can call the model via the <code>/chat/completions</code> route:</p> <pre><code>curl \\\n-X POST http://localhost:8000/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"model\": \"tinyllama-1.1b-chat\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Tell me a story of 1000 words with emojis\"}],\n    \"temperature\": 0.7, \"stream\": true\n  }'\n</code></pre>"},{"location":"examples/tutorials/05-serving-with-docker/index.html","title":"Serving with <code>docker</code> and <code>docker-compose</code>","text":"<p>This tutorial shows how to serve the NOS server directly with <code>docker</code> or <code>docker-compose</code>.</p>"},{"location":"examples/tutorials/05-serving-with-docker/index.html#serving-with-docker","title":"Serving with <code>docker</code>","text":"<p>To run the NOS gRPC server with <code>docker</code> simply run:</p> <p>For CPU: <pre><code>docker run --rm \\\n    -e NOS_HOME=/app/.nos \\\n    -v $(HOME)/.nos:/app/.nos \\\n    -v /dev/shm:/dev/shm \\\n    -p 50051:50051 \\\n    autonomi/nos:latest-cpu\n</code></pre></p> <p>For running the GPU server, you need to install <code>nvidia-docker</code> and run the following command: <pre><code>docker run --rm \\\n    --name nos-grpc-server \\\n    --gpus all \\\n    -e NOS_HOME=/app/.nos \\\n    -v $(HOME)/.nos:/app/.nos \\\n    -v /dev/shm:/dev/shm \\\n    -p 50051:50051 \\\n    autonomi/nos:latest-gpu\n</code></pre></p>"},{"location":"examples/tutorials/05-serving-with-docker/index.html#serving-with-docker-compose","title":"Serving with <code>docker-compose</code>","text":"<p>To run the NOS gRPC server and the HTTP gateway with <code>docker-compose</code> simply run:</p> <pre><code>docker-compose up -f docker-compose.yml\n</code></pre> <p>You should now see the logs both from the server and the gateway.</p> <pre><code>(nos-py38) tutorials/05-serving-with-docker desktop [ docker compose -f docker-compose.yml up\n[+] Running 2/2\n \u2714 Container 05-serving-with-docker-nos-grpc-server-1   Created                                                                                                                                                0.0s\n \u2714 Container 05-serving-with-docker-nos-http-gateway-1  Recreated                                                                                                                                              0.0s\nAttaching to 05-serving-with-docker-nos-grpc-server-1, 05-serving-with-docker-nos-http-gateway-1\n05-serving-with-docker-nos-grpc-server-1   | Starting server with OMP_NUM_THREADS=64...\n05-serving-with-docker-nos-http-gateway-1  | WARNING:  Current configuration will not reload as not all conditions are met, please refer to documentation.\n05-serving-with-docker-nos-grpc-server-1   |  \u2713 InferenceExecutor :: Connected to backend.\n05-serving-with-docker-nos-grpc-server-1   |  \u2713 Starting gRPC server on [::]:50051\n05-serving-with-docker-nos-grpc-server-1   |  \u2713 InferenceService :: Deployment complete (elapsed=0.0s)\n05-serving-with-docker-nos-http-gateway-1  | INFO:     Started server process [1]\n05-serving-with-docker-nos-http-gateway-1  | INFO:     Waiting for application startup.\n05-serving-with-docker-nos-http-gateway-1  | INFO:     Application startup complete.\n05-serving-with-docker-nos-http-gateway-1  | INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n</code></pre> <p>The HTTP gateway service (<code>nos-http-gateway</code>) in the <code>docker-compose.yml</code> file simply forwards the HTTP requests to the gRPC server (<code>nos-grpc-server</code>). This is especially useful when exposing the server via a REST API.</p> <p>Here's the full <code>docker-compose.yml</code> file:</p> <pre><code>version: \"3.8\"\n\nservices:\n  nos-http-gateway:\n    image: autonomi/nos:latest-gpu\n    command: nos-http-server --host 0.0.0.0 --port 8000 --workers 1\n    environment:\n      - NOS_HOME=/app/.nos\n      - NOS_LOGGING_LEVEL=INFO\n      - NOS_GRPC_HOST=nos-grpc-server\n      - NOS_HTTP_ENV=prod\n    volumes:\n      - ~/.nosd:/app/.nos\n      - /dev/shm:/dev/shm\n    ports:\n      - 8000:8000\n    ipc: host\n    depends_on:\n      - nos-grpc-server\n\n  nos-grpc-server:\n    image: autonomi/nos:latest-gpu\n    environment:\n      - NOS_HOME=/app/.nos\n      - NOS_GRPC_HOST=[::]\n      - NOS_LOGGING_LEVEL=INFO\n    volumes:\n      - ~/.nosd:/app/.nos\n      - /dev/shm:/dev/shm\n    ports:\n      - 50051:50051\n    ipc: host\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - capabilities: [gpu]\n        limits:\n          memory: 12G\n</code></pre>"},{"location":"examples/tutorials/05-serving-with-docker/index.html#testing-the-server","title":"Testing the server","text":"<p>To test the server's health, you can simply use <code>curl</code>:</p> <pre><code>curl -X \"GET\" \"http://localhost:8000/v1/health\" -H \"accept: application/json\"\n</code></pre> <p>You should see the following response: <pre><code>{\"status\":\"ok\"}\n</code></pre></p> <p>You can now try one of the many requests showcased in the main README.</p>"},{"location":"examples/tutorials/05-serving-with-docker/index.html#debugging-the-server","title":"Debugging the server","text":"<ul> <li>Running on CPUs: You can remove the <code>deploy</code> section from the <code>docker-compose.yml</code> file to run the server without GPU capabilities.</li> <li>Running on GPUs: Make sure you have <code>nvidia-docker</code> installed and that you have the latest NVIDIA drivers installed on your machine. You can check the NVIDIA drivers version by running <code>nvidia-smi</code> on your terminal. If you don't have <code>nvidia-docker</code> installed, you can follow the instructions here.</li> <li>Running on MacOS: You can run the server on MacOS by removing the <code>deploy</code> section from the <code>docker-compose.yml</code> file.</li> <li>Enabling debug logs: You can enable debug logs on both the docker services by setting the <code>NOS_LOGGING_LEVEL</code> environment variable to <code>DEBUG</code> in the <code>docker-compose.yml</code> file. This should provide you with more information on what's happening under the hood.</li> </ul>"},{"location":"nos/models/openmmlab/index.html","title":"NOS / openmmlab","text":"<p>OpenMMLab requires PyTorch 1.13, and does not yet support 2.0. <pre><code>conda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.7 -c pytorch -c nvidia\n</code></pre></p>"},{"location":"docs/blog/archive/2024.html","title":"2024","text":""},{"location":"docs/blog/category/infra.html","title":"infra","text":""},{"location":"docs/blog/category/embeddings.html","title":"embeddings","text":""},{"location":"docs/blog/category/asic.html","title":"asic","text":""},{"location":"docs/blog/category/tutorials.html","title":"tutorials","text":""}]}