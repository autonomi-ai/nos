
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="NOS -- A fast, and flexible AI inference server.">
      
      
        <meta name="author" content="Sudeep Pillai">
      
      
        <link rel="canonical" href="https://docs.nos.run/docs/blog/archive/2024.html">
      
      
      
      
      <link rel="icon" href="../../assets/favicon.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.25">
    
    
      
        <title>2024 - NOS Docs</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.6543a935.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function n(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],n("js",new Date),n("config","G-J38LQZWLM3"),document.addEventListener("DOMContentLoaded",function(){document.forms.search&&document.forms.search.query.addEventListener("blur",function(){this.value&&n("event","search",{search_term:this.value})}),document$.subscribe(function(){var a=document.forms.feedback;if(void 0!==a)for(var e of a.querySelectorAll("[type=submit]"))e.addEventListener("click",function(e){e.preventDefault();var t=document.location.pathname,e=this.getAttribute("data-md-value");n("event","feedback",{page:t,data:e}),a.firstElementChild.disabled=!0;e=a.querySelector(".md-feedback__note [data-md-value='"+e+"']");e&&(e.hidden=!1)}),a.hidden=!1}),location$.subscribe(function(e){n("config","G-J38LQZWLM3",{page_path:e.pathname})})});var e=document.createElement("script");e.async=!0,e.src="https://www.googletagmanager.com/gtag/js?id=G-J38LQZWLM3",document.getElementById("__analytics").insertAdjacentElement("afterEnd",e)}</script>
  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
      
        <meta  property="og:type"  content="website" >
      
        <meta  property="og:title"  content="2024 - NOS Docs" >
      
        <meta  property="og:description"  content="NOS -- A fast, and flexible AI inference server." >
      
        <meta  property="og:image"  content="https://docs.nos.run/assets/images/social/docs/blog/archive/2024.png" >
      
        <meta  property="og:image:type"  content="image/png" >
      
        <meta  property="og:image:width"  content="1200" >
      
        <meta  property="og:image:height"  content="630" >
      
        <meta  property="og:url"  content="https://docs.nos.run/docs/blog/archive/2024.html" >
      
        <meta  name="twitter:card"  content="summary_large_image" >
      
        <meta  name="twitter:title"  content="2024 - NOS Docs" >
      
        <meta  name="twitter:description"  content="NOS -- A fast, and flexible AI inference server." >
      
        <meta  name="twitter:image"  content="https://docs.nos.run/assets/images/social/docs/blog/archive/2024.png" >
      
    
    
  <!-- Add scripts that need to run before here -->
  
  <!-- Add scripts that need to run afterwards here -->
  <meta name="robots" content="noindex, nofollow" />

  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#2024" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../index.html" title="NOS Docs" class="md-header__button md-logo" aria-label="NOS Docs" data-md-component="logo">
      
  <img src="../../assets/favicon.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            NOS Docs
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              2024
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/autonomi-ai/nos" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    autonomi-ai/nos
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../index.html" title="NOS Docs" class="md-nav__button md-logo" aria-label="NOS Docs" data-md-component="logo">
      
  <img src="../../assets/favicon.png" alt="logo">

    </a>
    NOS Docs
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/autonomi-ai/nos" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    autonomi-ai/nos
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../index.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    üè† Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quickstart.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    üî• Quickstart
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../models/supported-models.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    üß† Models
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    ‚ö°Ô∏è Concepts
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            ‚ö°Ô∏è Concepts
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../concepts/architecture-overview.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    What is NOS?
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../concepts/model-spec.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Model specification
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../concepts/model-manager.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    HW-aware execution
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../concepts/runtime-environments.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Runtime environments
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    üìö Usage Guide
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            üìö Usage Guide
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/starting-the-server.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Starting the server
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/running-inference.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Running inference
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/serving-custom-models.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Serving custom models
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    ü§ñ Demos
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            ü§ñ Demos
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../demos/discord-bot.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Build a Discord image-generation bot
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../demos/video-search.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Build a video search engine
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../demos/profiling-models-with-nos.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Profiling models with NOS
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    üë©‚Äçüíª API Reference
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            üë©‚Äçüíª API Reference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_1" >
        
          
          <label class="md-nav__link" for="__nav_7_1" id="__nav_7_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    CLI
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_7_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7_1">
            <span class="md-nav__icon md-icon"></span>
            CLI
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../cli/serve.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    <kbd>nos serve</kbd>
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../cli/system.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    <kbd>nos system</kbd>
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7_2" >
        
          
          <label class="md-nav__link" for="__nav_7_2" id="__nav_7_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    nos.common
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_7_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7_2">
            <span class="md-nav__icon md-icon"></span>
            nos.common
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../api/common/exceptions.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    nos.common.exceptions
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../api/common/metaclass.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    nos.common.metaclass
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../api/common/shm.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    nos.common.shm
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../api/common/spec.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    nos.common.spec
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../api/common/system.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    nos.common.system
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../api/common/types.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    nos.common.types
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../api/common/tasks.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    nos.common.tasks
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../api/client.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    nos.client
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../api/server.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    nos.server
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../api/hub.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    nos.hub
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../api/managers.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    nos.managers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../api/executors.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    nos.executors
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" >
        
          
          <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    üîå Integrations
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            üîå Integrations
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../integrations/skypilot.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    SkyPilot
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ‚úçÔ∏è Blog
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../support.html" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ‚ùì Support / FAQ
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introducing-the-nos-inferentia2-runtime" class="md-nav__link">
    <span class="md-ellipsis">
      Introducing the NOS Inferentia2 Runtime
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#serving-llms-on-a-budget" class="md-nav__link">
    <span class="md-ellipsis">
      Serving LLMs on a budget
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#getting-started-with-nos-tutorials" class="md-nav__link">
    <span class="md-ellipsis">
      üìö Getting started with NOS tutorials
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#introducing-nos-blog" class="md-nav__link">
    <span class="md-ellipsis">
      Introducing NOS Blog!
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
  <div class="md-content" data-md-component="content">
    <div class="md-content__inner">
      <header class="md-typeset">
        <h1 id="2024">2024<a class="headerlink" href="#2024" title="Permanent link">&para;</a></h1>
      </header>
      
        <article class="md-post md-post--excerpt">
  <header class="md-post__header">
    
      <nav class="md-post__authors md-typeset">
        
          <span class="md-author">
            <img src="https://github.com/spillai.png" alt="Sudeep Pillai">
          </span>
        
      </nav>
    
    <div class="md-post__meta md-meta">
      <ul class="md-meta__list">
        <li class="md-meta__item">
          <time datetime="2024-02-01 00:00:00">Feb 1, 2024</time></li>
        
          <li class="md-meta__item">
            in
            
              <a href="../category/infra.html" class="md-meta__link">infra</a>, 
              <a href="../category/embeddings.html" class="md-meta__link">embeddings</a>, 
              <a href="../category/asic.html" class="md-meta__link">asic</a></li>
        
        
          
          <li class="md-meta__item">
            
              11 min read
            
          </li>
        
      </ul>
      
    </div>
  </header>
  <div class="md-post__content md-typeset">
    <h2 id="introducing-the-nos-inferentia2-runtime"><a class="toclink" href="../introducing-the-nos-inferentia2-runtime.html">Introducing the NOS Inferentia2 Runtime</a></h2>
<p><img src="/docs/blog/assets/nos-inf2.jpg" width="100%"></p>
<p>We are excited to announce the availability of the <a href="https://aws.amazon.com/en/ec2/instance-types/inf2/">AWS Inferentia2</a> runtime on <a href="https://github.com/autonomi-ai/nos">NOS</a> - a.k.a. our <strong><a href="../../concepts/runtime-environments.html#üèÉ‚Äç‚ôÇÔ∏è-supported-runtimes"><code>inf2</code></a></strong> runtime. This runtime is designed to easily serve models on AWS Inferentia2, a high-performance, purpose-built chip for inference. In this blog post, we will introduce the AWS Inferentia2 runtime, and show you how to trivially deploy a model on the AWS Inferentia2 device using NOS. If you have followed our previous tutorial on <a href="../serving-llms-on-a-budget.html">serving LLMs on a budget (on NVIDIA hardware)</a>, you will be pleasantly surprised to see how easy it is to deploy a model on the AWS Inferentia2 device using the pre-baked NOS <strong><a href="../../concepts/runtime-environments.html#üèÉ‚Äç‚ôÇÔ∏è-supported-runtimes"><code>inf2</code></a></strong> runtime we provide.</p>
<h3 id="what-is-aws-inferentia2"><a class="toclink" href="../introducing-the-nos-inferentia2-runtime.html#what-is-aws-inferentia2">‚ö°Ô∏è What is AWS Inferentia2?</a></h3>
<p><a href="https://aws.amazon.com/en/ec2/instance-types/inf2/">AWS Inferentia2</a> (Inf2 for short) is the second-generation inference accelerator from AWS. Inf2 instances raise the performance of <a href="https://aws.amazon.com/ec2/instance-types/inf1/">Inf1</a> (originally launched in 2019) by delivering 3x higher compute performance, 4x larger total accelerator memory, up to 4x higher throughput, and up to 10x lower latency. Inf2 instances are the first inference-optimized instances in Amazon EC2 to support scale-out distributed inference with ultra-high-speed connectivity between accelerators. </p>
<p>Relative to the <a href="https://aws.amazon.com/ec2/instance-types/g5/">AWS G5 instances</a> (<a href="https://www.nvidia.com/en-us/data-center/products/a10-gpu/">NVIDIA A10G</a>), Inf2 instances promise up to 50% better performance-per-watt. Inf2 instances are ideal for applications such as natural language processing, recommender systems, image classification and recognition, speech recognition, and language translation that can take advantage of scale-out distributed inference. </p>
<table>
<thead>
<tr>
<th>Instance Size</th>
<th>Inf2 Accelerators</th>
<th>Accelerator Memory (GB)</th>
<th>vCPU</th>
<th>Memory (GiB)</th>
<th>On-Demand Price</th>
<th>Spot Price</th>
</tr>
</thead>
<tbody>
<tr>
<td>inf2.xlarge</td>
<td>1</td>
<td>32</td>
<td>4</td>
<td>16</td>
<td>$0.76</td>
<td>$0.32</td>
</tr>
<tr>
<td>inf2.8xlarge</td>
<td>1</td>
<td>32</td>
<td>32</td>
<td>128</td>
<td>$1.97</td>
<td>$0.79</td>
</tr>
<tr>
<td>inf2.24xlarge</td>
<td>6</td>
<td>192</td>
<td>96</td>
<td>384</td>
<td>$6.49</td>
<td>$2.45</td>
</tr>
<tr>
<td>inf2.48xlarge</td>
<td>12</td>
<td>384</td>
<td>192</td>
<td>768</td>
<td>$12.98</td>
<td>$5.13</td>
</tr>
</tbody>
</table>
<h3 id="nos-inference-runtime"><a class="toclink" href="../introducing-the-nos-inferentia2-runtime.html#nos-inference-runtime">üèÉ‚Äç‚ôÇÔ∏è NOS Inference Runtime</a></h3>
<p>The NOS inference server supports custom runtime environments through the use of the <a href="../api/server.md#inferenceserviceruntime">InferenceServiceRuntime</a> class - a high-level interface for defining new <strong>containerized</strong> and <strong>hardware-aware</strong> runtime environments. NOS already ships with <a href="../../concepts/runtime-environments.html#üèÉ‚Äç‚ôÇÔ∏è-supported-runtimes">runtime environments</a> for NVIDIA GPUs (<code>gpu</code>) and Intel/ARM CPUs (<code>cpu</code>). Today, we're adding the <a href="https://hub.docker.com/repository/docker/autonomi/nos/general">NOS Inferentia2 runtime</a> (<code>inf2</code>) with the <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/setup/neuron-setup/pytorch/neuronx/ubuntu/torch-neuronx-ubuntu22.html#setup-torch-neuronx-ubuntu22">AWS Neuron drivers</a>, the <a href="https://github.com/aws-neuron/aws-neuron-sdk">AWS Neuron SDK</a> and NOS pre-installed. This allows developers to quickly develop applications for AWS Inferentia2, without wasting any precious time on the complexities of setting up the AWS Neuron SDK and the AWS Inferentia2 driver environments.</p>
<h3 id="deploying-a-pytorch-model-on-inferentia2-with-nos"><a class="toclink" href="../introducing-the-nos-inferentia2-runtime.html#deploying-a-pytorch-model-on-inferentia2-with-nos">üì¶ Deploying a PyTorch model on Inferentia2 with NOS</a></h3>
<p>Deploying PyTorch models on AWS Inferentia2 chips presents a unique set of challenges, distinct from the experience with NVIDIA GPUs. This is primarily due to the static graph execution requirement of ASICs, requiring the user to <a href="https://pytorch.org/docs/stable/generated/torch.jit.trace.html">trace</a> and <a href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html">compile models ahead-of-time</a>, making them less accessible to entry-level developers. In some cases, custom model tracing and compilation are essential steps to fully utilize the AWS Inferentia2 accelerators. This demands a deep understanding of the HW-specific deployment/compiler toolchain (<a href="https://developer.nvidia.com/tensorrt">TensorRT</a>, <a href="https://github.com/aws-neuron/aws-neuron-sdk">AWS Neuron SDK</a>), the captured and data-dependent traced PyTorch graph, and the underlying HW-specific kernel/op-support to name just a few challenges. </p>
<div class="admonition tip">
<p class="admonition-title"><strong>Simplifying AI hardware access with NOS</strong></p>
<p>NOS aims to bridge this gap and streamline the deployment process, making it more even accessible for both entry-level and expert developers to leverage the powerful inference capabilities of AWS Inferentia2 for their inference needs. </p>
</div>
<h4 id="1-define-your-custom-inf2-model"><a class="toclink" href="../introducing-the-nos-inferentia2-runtime.html#1-define-your-custom-inf2-model">1. Define your custom <code>inf2</code> model</a></h4>
<p>In this example, we'll be using the <a href="https://github.com/autonomi-ai/nos/blob/main/examples/inf2/embeddings/"><code>inf2/embeddings</code></a> sentence embedding tutorial on <a href="https://github.com/autonomi-ai/nos">NOS</a>. First, we'll define our custom <a href="https://github.com/autonomi-ai/nos/blob/main/examples/inf2/embeddings/models/embeddings_inf2.py#L24"><code>EmbeddingServiceInf2</code></a> model <a href="https://github.com/autonomi-ai/nos/blob/main/examples/inf2/embeddings/models/embeddings_inf2.py"><code>models/embeddings_inf2.py</code></a> and a <a href="https://github.com/autonomi-ai/nos/blob/main/examples/inf2/embeddings/serve.yaml"><code>serve.yaml</code></a> serve specification that will be used by NOS to serve our model on the AWS Inferentia2 device. The relevant files are shown below:</p>
<div class="annotate">
<div class="language-text highlight"><span class="filename">Directory structure of <code>nos/examples/inf2/embeddings</code></span><pre><span></span><code><span id="__span-10-1"><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a>$ tree .
</span><span id="__span-10-2"><a id="__codelineno-10-2" name="__codelineno-10-2" href="#__codelineno-10-2"></a>‚îú‚îÄ‚îÄ job-inf2-embeddings-deployment.yaml
</span><span id="__span-10-3"><a id="__codelineno-10-3" name="__codelineno-10-3" href="#__codelineno-10-3"></a>‚îú‚îÄ‚îÄ models
</span><span id="__span-10-4"><a id="__codelineno-10-4" name="__codelineno-10-4" href="#__codelineno-10-4"></a>‚îÇ¬†¬† ‚îî‚îÄ‚îÄ embeddings_inf2.py  (1)
</span><span id="__span-10-5"><a id="__codelineno-10-5" name="__codelineno-10-5" href="#__codelineno-10-5"></a>‚îú‚îÄ‚îÄ README.md
</span><span id="__span-10-6"><a id="__codelineno-10-6" name="__codelineno-10-6" href="#__codelineno-10-6"></a>‚îú‚îÄ‚îÄ serve.yaml  (2)
</span><span id="__span-10-7"><a id="__codelineno-10-7" name="__codelineno-10-7" href="#__codelineno-10-7"></a>‚îî‚îÄ‚îÄ tests
</span><span id="__span-10-8"><a id="__codelineno-10-8" name="__codelineno-10-8" href="#__codelineno-10-8"></a>    ‚îú‚îÄ‚îÄ test_embeddings_inf2_client.py  (3)
</span><span id="__span-10-9"><a id="__codelineno-10-9" name="__codelineno-10-9" href="#__codelineno-10-9"></a>    ‚îî‚îÄ‚îÄ test_embeddings_inf2.py
</span></code></pre></div>
</div>
<ol>
<li>Main python module that defines the <a href="https://github.com/autonomi-ai/nos/blob/main/examples/inf2/embeddings/models/embeddings_inf2.py#L24"><code>EmbeddingServiceInf2</code></a> model.</li>
<li>The <a href="https://github.com/autonomi-ai/nos/blob/main/examples/inf2/embeddings/serve.yaml"><code>serve.yaml</code></a> serve specification that defines the custom <code>inf2</code> runtime and registers the <a href="https://github.com/autonomi-ai/nos/blob/main/examples/inf2/embeddings/models/embeddings_inf2.py#L24"><code>EmbeddingServiceInf2</code></a> model with NOS.</li>
<li>The pytest test for calling the <a href="https://github.com/autonomi-ai/nos/blob/main/examples/inf2/embeddings/models/embeddings_inf2.py#L24"><code>EmbeddingServiceInf2</code></a> service via gRPC.</li>
</ol>
<p>The embeddings interface is defined in the <a href="https://github.com/autonomi-ai/nos/blob/main/examples/inf2/embeddings/models/embeddings_inf2.py#L24"><code>EmbeddingServiceInf2</code></a> module in <a href="https://github.com/autonomi-ai/nos/blob/main/examples/inf2/embeddings/models/embeddings_inf2.py"><code>models/embeddings_inf2.py</code></a>, where the <a href="https://github.com/autonomi-ai/nos/blob/main/examples/inf2/embeddings/models/embeddings_inf2.py#L67"><code>__call__</code></a> method returns the embedding of the text prompt using <a href="https://huggingface.co/BAAI/bge-small-en-v1.5"><code>BAAI/bge-small-en-v1.5</code></a> embedding model. </p>
<h4 id="2-define-the-custom-inf2-runtime-with-the-nos-serve-specification"><a class="toclink" href="../introducing-the-nos-inferentia2-runtime.html#2-define-the-custom-inf2-runtime-with-the-nos-serve-specification">2. Define the custom <code>inf2</code> runtime with the NOS serve specification</a></h4>
<p>The <a href="https://github.com/autonomi-ai/nos/blob/main/examples/inf2/embeddings/serve.yaml"><code>serve.yaml</code></a> serve specification defines the custom embedding model, and a custom <code>inf2</code> runtime that NOS uses to execute our model. Follow the annotations below to understand the different components of the serve specification.</p>
<div class="annotate">
<div class="language-yaml highlight"><span class="filename">serve.yaml</span><pre><span></span><code><span id="__span-11-1"><a id="__codelineno-11-1" name="__codelineno-11-1" href="#__codelineno-11-1"></a><span class="nt">images</span><span class="p">:</span>
</span><span id="__span-11-2"><a id="__codelineno-11-2" name="__codelineno-11-2" href="#__codelineno-11-2"></a><span class="w">  </span><span class="nt">embeddings-inf2</span><span class="p">:</span>
</span><span id="__span-11-3"><a id="__codelineno-11-3" name="__codelineno-11-3" href="#__codelineno-11-3"></a><span class="w">    </span><span class="nt">base</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">autonomi/nos:latest-inf2  (1)</span>
</span><span id="__span-11-4"><a id="__codelineno-11-4" name="__codelineno-11-4" href="#__codelineno-11-4"></a><span class="w">    </span><span class="nt">env</span><span class="p">:</span>
</span><span id="__span-11-5"><a id="__codelineno-11-5" name="__codelineno-11-5" href="#__codelineno-11-5"></a><span class="w">      </span><span class="nt">NOS_LOGGING_LEVEL</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">DEBUG</span>
</span><span id="__span-11-6"><a id="__codelineno-11-6" name="__codelineno-11-6" href="#__codelineno-11-6"></a><span class="w">      </span><span class="nt">NOS_NEURON_CORES</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span>
</span><span id="__span-11-7"><a id="__codelineno-11-7" name="__codelineno-11-7" href="#__codelineno-11-7"></a><span class="w">    </span><span class="nt">run</span><span class="p">:</span>
</span><span id="__span-11-8"><a id="__codelineno-11-8" name="__codelineno-11-8" href="#__codelineno-11-8"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">python -m pip config set global.extra-index-url https://pip.repos.neuron.amazonaws.com</span>
</span><span id="__span-11-9"><a id="__codelineno-11-9" name="__codelineno-11-9" href="#__codelineno-11-9"></a><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pip install sentence-transformers  (2)</span>
</span><span id="__span-11-10"><a id="__codelineno-11-10" name="__codelineno-11-10" href="#__codelineno-11-10"></a>
</span><span id="__span-11-11"><a id="__codelineno-11-11" name="__codelineno-11-11" href="#__codelineno-11-11"></a><span class="nt">models</span><span class="p">:</span>
</span><span id="__span-11-12"><a id="__codelineno-11-12" name="__codelineno-11-12" href="#__codelineno-11-12"></a><span class="w">  </span><span class="nt">BAAI/bge-small-en-v1.5</span><span class="p">:</span>
</span><span id="__span-11-13"><a id="__codelineno-11-13" name="__codelineno-11-13" href="#__codelineno-11-13"></a><span class="w">    </span><span class="nt">model_cls</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">EmbeddingServiceInf2</span>
</span><span id="__span-11-14"><a id="__codelineno-11-14" name="__codelineno-11-14" href="#__codelineno-11-14"></a><span class="w">    </span><span class="nt">model_path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">models/embeddings_inf2.py</span>
</span><span id="__span-11-15"><a id="__codelineno-11-15" name="__codelineno-11-15" href="#__codelineno-11-15"></a><span class="w">    </span><span class="nt">default_method</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">__call__</span>
</span><span id="__span-11-16"><a id="__codelineno-11-16" name="__codelineno-11-16" href="#__codelineno-11-16"></a><span class="w">    </span><span class="nt">runtime_env</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">embeddings-inf2  (3)</span>
</span></code></pre></div>
</div>
<ol>
<li>Specifies the base runtime image to use - we use the pre-baked <code>autonomi/nos:latest-inf2</code> runtime image to build our custom runtime image. This custom NOS runtime comes pre-installed with the AWS Neuron drivers and the AWS Neuron SDK.</li>
<li>Installs the <code>sentence-transformers</code> library, which is used to embed the text prompt using the <code>BAAI/bge-small-en-v1.5</code> model.</li>
<li>Specifies the custom runtime environment to use for the specific model deployment - <code>embeddings-inf2</code> - which is used to execute the <code>EmbeddingServiceInf2</code> model.</li>
</ol>
<p>In this example, we'll be using the <a href="https://github.com/huggingface/optimum-neuron">Huggingface Optimum</a> library to help us simplify the deployment process to the Inf2 chip. However, for custom model architectures and optimizations, we have built our own PyTorch tracer and compiler for a growing list of popular models on the <a href="https://huggingface.co/models">Huggingface Hub</a>. </p>
<details class="question">
<summary>Need support for custom models on AWS Inferentia2?</summary>
<p>If you're interested in deploying a custom model on the AWS Inferentia2 chip with NOS, please reach out to us on our <a href="https://github.com/autonomi-ai/nos/issues">GitHub Issues</a> page or at <a href="mailto:support@autonomi.ai">support@autonomi.ai</a>, and we'll be happy to help you out.</p>
</details>
<h4 id="3-deploy-the-embedding-service-on-aws-inf2xlarge-with-skypilot"><a class="toclink" href="../introducing-the-nos-inferentia2-runtime.html#3-deploy-the-embedding-service-on-aws-inf2xlarge-with-skypilot">3. Deploy the embedding service on AWS <code>inf2.xlarge</code> with SkyPilot</a></h4>
<p>Now that we have defined our custom model, let's deploy this service on <a href="https://aws.amazon.com/en/ec2/instance-types/inf2/">AWS Inferentia2</a> using <a href="https://github.com/skypilot-org/skypilot">SkyPilot</a>. In this example, we're going to use SkyPilot's <code>sky launch</code> command to deploy our NOS service on an AWS <code>inf2.xlarge</code> on-demand instance. </p>
<p>Before we launch the service, let's look at the <a href="https://github.com/autonomi-ai/nos/blob/main/examples/inf2/embeddings/job-inf2-embeddings-deployment.yaml"><code>job-inf2-embeddings-deployment.yaml</code></a> file that we will use to provision the <code>inf2</code> instance and deploy the <code>EmbeddingServiceInf2</code> model.</p>
<div class="annotate">
<div class="language-yaml highlight"><span class="filename">job-inf2-embeddings-deployment.yaml</span><pre><span></span><code><span id="__span-12-1"><a id="__codelineno-12-1" name="__codelineno-12-1" href="#__codelineno-12-1"></a><span class="nt">file_mounts</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">(1)</span>
</span><span id="__span-12-2"><a id="__codelineno-12-2" name="__codelineno-12-2" href="#__codelineno-12-2"></a><span class="w">  </span><span class="l l-Scalar l-Scalar-Plain">/app</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">.</span>
</span><span id="__span-12-3"><a id="__codelineno-12-3" name="__codelineno-12-3" href="#__codelineno-12-3"></a>
</span><span id="__span-12-4"><a id="__codelineno-12-4" name="__codelineno-12-4" href="#__codelineno-12-4"></a><span class="nt">resources</span><span class="p">:</span>
</span><span id="__span-12-5"><a id="__codelineno-12-5" name="__codelineno-12-5" href="#__codelineno-12-5"></a><span class="w">  </span><span class="nt">cloud</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">aws</span>
</span><span id="__span-12-6"><a id="__codelineno-12-6" name="__codelineno-12-6" href="#__codelineno-12-6"></a><span class="w">  </span><span class="nt">region</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">us-west-2</span>
</span><span id="__span-12-7"><a id="__codelineno-12-7" name="__codelineno-12-7" href="#__codelineno-12-7"></a><span class="w">  </span><span class="nt">instance_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">inf2.xlarge (2)</span>
</span><span id="__span-12-8"><a id="__codelineno-12-8" name="__codelineno-12-8" href="#__codelineno-12-8"></a><span class="w">  </span><span class="nt">image_id</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ami-096319086cc3d5f23</span><span class="w"> </span><span class="c1"># us-west-2 (3)</span>
</span><span id="__span-12-9"><a id="__codelineno-12-9" name="__codelineno-12-9" href="#__codelineno-12-9"></a><span class="w">  </span><span class="nt">disk_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">256</span>
</span><span id="__span-12-10"><a id="__codelineno-12-10" name="__codelineno-12-10" href="#__codelineno-12-10"></a><span class="w">  </span><span class="nt">ports</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8000</span>
</span><span id="__span-12-11"><a id="__codelineno-12-11" name="__codelineno-12-11" href="#__codelineno-12-11"></a>
</span><span id="__span-12-12"><a id="__codelineno-12-12" name="__codelineno-12-12" href="#__codelineno-12-12"></a><span class="nt">setup</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
</span><span id="__span-12-13"><a id="__codelineno-12-13" name="__codelineno-12-13" href="#__codelineno-12-13"></a><span class="w">  </span><span class="no">sudo apt-get install -y docker-compose-plugin</span>
</span><span id="__span-12-14"><a id="__codelineno-12-14" name="__codelineno-12-14" href="#__codelineno-12-14"></a>
</span><span id="__span-12-15"><a id="__codelineno-12-15" name="__codelineno-12-15" href="#__codelineno-12-15"></a><span class="w">  </span><span class="no">cd /app</span>
</span><span id="__span-12-16"><a id="__codelineno-12-16" name="__codelineno-12-16" href="#__codelineno-12-16"></a><span class="w">  </span><span class="no">cd /app &amp;&amp; python3 -m venv .venv &amp;&amp; source .venv/bin/activate</span>
</span><span id="__span-12-17"><a id="__codelineno-12-17" name="__codelineno-12-17" href="#__codelineno-12-17"></a><span class="w">  </span><span class="no">pip install git+https://github.com/autonomi-ai/nos.git pytest (4)</span>
</span><span id="__span-12-18"><a id="__codelineno-12-18" name="__codelineno-12-18" href="#__codelineno-12-18"></a>
</span><span id="__span-12-19"><a id="__codelineno-12-19" name="__codelineno-12-19" href="#__codelineno-12-19"></a><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
</span><span id="__span-12-20"><a id="__codelineno-12-20" name="__codelineno-12-20" href="#__codelineno-12-20"></a><span class="w">  </span><span class="no">source /app/.venv/bin/activate</span>
</span><span id="__span-12-21"><a id="__codelineno-12-21" name="__codelineno-12-21" href="#__codelineno-12-21"></a><span class="w">  </span><span class="no">cd /app &amp;&amp; nos serve up -c serve.yaml --http (5)</span>
</span></code></pre></div>
</div>
<ol>
<li>Mounts the local <code>./app</code> directory so that the <code>serve.yaml</code>, <code>models/</code> and <code>tests/</code> directories are available on the remote instance.</li>
<li>Specifies the AWS Inferentia2 instance type to use - we use the <code>inf2.xlarge</code> instance type.</li>
<li>Specifies the Amazon Machine Instance (AMI) use that come pre-installed with AWS Neuron drivers.</li>
<li>We simply need <code>pytest</code> for testing the client-side logic in <code>tests/test_embeddings_inf2_client.py</code></li>
<li>Starts the NOS server with the <code>serve.yaml</code> specification. The runtime flag <code>--runtime inf2</code> is optional, and automatically detected by NOS as illustrated here.</li>
</ol>
<div class="admonition note">
<p class="admonition-title">Provisioning <code>inf2.xlarge</code> instances</p>
<p>To provision an <code>inf2.xlarge</code> instance, you will need to have an AWS account and the necessary <a href="https://console.aws.amazon.com/servicequotas/home/services/ec2/quotas/">service quotas</a> set for the <code>inf2</code> instance nodes. For more information on service quotas, please refer to the <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-resource-limits.html">AWS documentation</a>.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Using SkyPilot with <code>inf2</code> instances</p>
<p>Due to a <a href="https://github.com/skypilot-org/skypilot/issues/2968">job submission bug</a> in the SkyPilot CLI for <code>inf2</code> instances, you will need to use the <code>skypilot-nightly[aws]</code> (<code>pip install skypilot-nightly[aws]</code>) package to provision <code>inf2</code> instances correctly with the <code>sky launch</code> command below. </p>
</div>
<p>Let's deploy the <code>inf2</code> embeddings service using the following command:
<div class="language-bash highlight"><pre><span></span><code><span id="__span-13-1"><a id="__codelineno-13-1" name="__codelineno-13-1" href="#__codelineno-13-1"></a>sky<span class="w"> </span>launch<span class="w"> </span>-c<span class="w"> </span>inf2-embeddings-service<span class="w"> </span>job-inf2-embeddings-deployment.yaml
</span></code></pre></div></p>
<details class="success">
<summary><code>sky launch</code> output</summary>
<p>You should see the following output from the <code>sky launch</code> command:
<div class="language-bash highlight"><pre><span></span><code><span id="__span-14-1"><a id="__codelineno-14-1" name="__codelineno-14-1" href="#__codelineno-14-1"></a><span class="o">(</span>nos-infra-py38<span class="o">)</span><span class="w"> </span>inf2/embeddings<span class="w"> </span>spillai-desktop<span class="w"> </span><span class="o">[</span><span class="w"> </span>sky<span class="w"> </span>launch<span class="w"> </span>-c<span class="w"> </span>inf2-embeddings-service<span class="w"> </span>job-inf2-embeddings-deployment.yaml
</span><span id="__span-14-2"><a id="__codelineno-14-2" name="__codelineno-14-2" href="#__codelineno-14-2"></a>Task<span class="w"> </span>from<span class="w"> </span>YAML<span class="w"> </span>spec:<span class="w"> </span>job-inf2-embeddings-deployment.yaml
</span><span id="__span-14-3"><a id="__codelineno-14-3" name="__codelineno-14-3" href="#__codelineno-14-3"></a>I<span class="w"> </span><span class="m">01</span>-31<span class="w"> </span><span class="m">21</span>:48:06<span class="w"> </span>optimizer.py:694<span class="o">]</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nv">Optimizer</span><span class="w"> </span><span class="o">==</span>
</span><span id="__span-14-4"><a id="__codelineno-14-4" name="__codelineno-14-4" href="#__codelineno-14-4"></a>I<span class="w"> </span><span class="m">01</span>-31<span class="w"> </span><span class="m">21</span>:48:06<span class="w"> </span>optimizer.py:717<span class="o">]</span><span class="w"> </span>Estimated<span class="w"> </span>cost:<span class="w"> </span><span class="nv">$0</span>.8<span class="w"> </span>/<span class="w"> </span>hour
</span><span id="__span-14-5"><a id="__codelineno-14-5" name="__codelineno-14-5" href="#__codelineno-14-5"></a>I<span class="w"> </span><span class="m">01</span>-31<span class="w"> </span><span class="m">21</span>:48:06<span class="w"> </span>optimizer.py:717<span class="o">]</span>
</span><span id="__span-14-6"><a id="__codelineno-14-6" name="__codelineno-14-6" href="#__codelineno-14-6"></a>I<span class="w"> </span><span class="m">01</span>-31<span class="w"> </span><span class="m">21</span>:48:06<span class="w"> </span>optimizer.py:840<span class="o">]</span><span class="w"> </span>Considered<span class="w"> </span>resources<span class="w"> </span><span class="o">(</span><span class="m">1</span><span class="w"> </span>node<span class="o">)</span>:
</span><span id="__span-14-7"><a id="__codelineno-14-7" name="__codelineno-14-7" href="#__codelineno-14-7"></a>I<span class="w"> </span><span class="m">01</span>-31<span class="w"> </span><span class="m">21</span>:48:06<span class="w"> </span>optimizer.py:910<span class="o">]</span><span class="w"> </span>------------------------------------------------------------------------------------------
</span><span id="__span-14-8"><a id="__codelineno-14-8" name="__codelineno-14-8" href="#__codelineno-14-8"></a>I<span class="w"> </span><span class="m">01</span>-31<span class="w"> </span><span class="m">21</span>:48:06<span class="w"> </span>optimizer.py:910<span class="o">]</span><span class="w">  </span>CLOUD<span class="w">   </span>INSTANCE<span class="w">      </span>vCPUs<span class="w">   </span>Mem<span class="o">(</span>GB<span class="o">)</span><span class="w">   </span>ACCELERATORS<span class="w">   </span>REGION/ZONE<span class="w">   </span>COST<span class="w"> </span><span class="o">(</span>$<span class="o">)</span><span class="w">   </span>CHOSEN
</span><span id="__span-14-9"><a id="__codelineno-14-9" name="__codelineno-14-9" href="#__codelineno-14-9"></a>I<span class="w"> </span><span class="m">01</span>-31<span class="w"> </span><span class="m">21</span>:48:06<span class="w"> </span>optimizer.py:910<span class="o">]</span><span class="w"> </span>------------------------------------------------------------------------------------------
</span><span id="__span-14-10"><a id="__codelineno-14-10" name="__codelineno-14-10" href="#__codelineno-14-10"></a>I<span class="w"> </span><span class="m">01</span>-31<span class="w"> </span><span class="m">21</span>:48:06<span class="w"> </span>optimizer.py:910<span class="o">]</span><span class="w">  </span>AWS<span class="w">     </span>inf2.xlarge<span class="w">   </span><span class="m">4</span><span class="w">       </span><span class="m">16</span><span class="w">        </span>Inferentia:1<span class="w">   </span>us-west-2<span class="w">     </span><span class="m">0</span>.76<span class="w">          </span>‚úî
</span><span id="__span-14-11"><a id="__codelineno-14-11" name="__codelineno-14-11" href="#__codelineno-14-11"></a>I<span class="w"> </span><span class="m">01</span>-31<span class="w"> </span><span class="m">21</span>:48:06<span class="w"> </span>optimizer.py:910<span class="o">]</span><span class="w"> </span>------------------------------------------------------------------------------------------
</span><span id="__span-14-12"><a id="__codelineno-14-12" name="__codelineno-14-12" href="#__codelineno-14-12"></a>I<span class="w"> </span><span class="m">01</span>-31<span class="w"> </span><span class="m">21</span>:48:06<span class="w"> </span>optimizer.py:910<span class="o">]</span>
</span><span id="__span-14-13"><a id="__codelineno-14-13" name="__codelineno-14-13" href="#__codelineno-14-13"></a>Launching<span class="w"> </span>a<span class="w"> </span>new<span class="w"> </span>cluster<span class="w"> </span><span class="s1">&#39;inf2-embeddings-service&#39;</span>.<span class="w"> </span>Proceed?<span class="w"> </span><span class="o">[</span>Y/n<span class="o">]</span>:<span class="w"> </span>y
</span><span id="__span-14-14"><a id="__codelineno-14-14" name="__codelineno-14-14" href="#__codelineno-14-14"></a>I<span class="w"> </span><span class="m">01</span>-31<span class="w"> </span><span class="m">21</span>:48:18<span class="w"> </span>cloud_vm_ray_backend.py:4389<span class="o">]</span><span class="w"> </span>Creating<span class="w"> </span>a<span class="w"> </span>new<span class="w"> </span>cluster:<span class="w"> </span><span class="s1">&#39;inf2-embeddings-service&#39;</span><span class="w"> </span><span class="o">[</span>1x<span class="w"> </span>AWS<span class="o">(</span>inf2.xlarge,<span class="w"> </span><span class="o">{</span><span class="s1">&#39;Inferentia&#39;</span>:<span class="w"> </span><span class="m">1</span><span class="o">}</span>,<span class="w"> </span><span class="nv">image_id</span><span class="o">={</span><span class="s1">&#39;us-west-2&#39;</span>:<span class="w"> </span><span class="s1">&#39;ami-096319086cc3d5f23&#39;</span><span class="o">}</span>,<span class="w"> </span><span class="nv">ports</span><span class="o">=[</span><span class="s1">&#39;8000&#39;</span><span class="o">])]</span>.
</span><span id="__span-14-15"><a id="__codelineno-14-15" name="__codelineno-14-15" href="#__codelineno-14-15"></a>I<span class="w"> </span><span class="m">01</span>-31<span class="w"> </span><span class="m">21</span>:48:18<span class="w"> </span>cloud_vm_ray_backend.py:4389<span class="o">]</span><span class="w"> </span>Tip:<span class="w"> </span>to<span class="w"> </span>reuse<span class="w"> </span>an<span class="w"> </span>existing<span class="w"> </span>cluster,<span class="w"> </span>specify<span class="w"> </span>--cluster<span class="w"> </span><span class="o">(</span>-c<span class="o">)</span>.<span class="w"> </span>Run<span class="w"> </span><span class="sb">`</span>sky<span class="w"> </span>status<span class="sb">`</span><span class="w"> </span>to<span class="w"> </span>see<span class="w"> </span>existing<span class="w"> </span>clusters.
</span><span id="__span-14-16"><a id="__codelineno-14-16" name="__codelineno-14-16" href="#__codelineno-14-16"></a>I<span class="w"> </span><span class="m">01</span>-31<span class="w"> </span><span class="m">21</span>:48:18<span class="w"> </span>cloud_vm_ray_backend.py:1386<span class="o">]</span><span class="w"> </span>To<span class="w"> </span>view<span class="w"> </span>detailed<span class="w"> </span>progress:<span class="w"> </span>tail<span class="w"> </span>-n100<span class="w"> </span>-f<span class="w"> </span>/home/spillai/sky_logs/sky-2024-01-31-21-48-06-108390/provision.log
</span><span id="__span-14-17"><a id="__codelineno-14-17" name="__codelineno-14-17" href="#__codelineno-14-17"></a>I<span class="w"> </span><span class="m">01</span>-31<span class="w"> </span><span class="m">21</span>:48:19<span class="w"> </span>provisioner.py:79<span class="o">]</span><span class="w"> </span>Launching<span class="w"> </span>on<span class="w"> </span>AWS<span class="w"> </span>us-west-2<span class="w"> </span><span class="o">(</span>us-west-2a,us-west-2b,us-west-2c,us-west-2d<span class="o">)</span>
</span><span id="__span-14-18"><a id="__codelineno-14-18" name="__codelineno-14-18" href="#__codelineno-14-18"></a>I<span class="w"> </span><span class="m">01</span>-31<span class="w"> </span><span class="m">21</span>:49:37<span class="w"> </span>provisioner.py:429<span class="o">]</span><span class="w"> </span>Successfully<span class="w"> </span>provisioned<span class="w"> </span>or<span class="w"> </span>found<span class="w"> </span>existing<span class="w"> </span>instance.
</span><span id="__span-14-19"><a id="__codelineno-14-19" name="__codelineno-14-19" href="#__codelineno-14-19"></a>I<span class="w"> </span><span class="m">01</span>-31<span class="w"> </span><span class="m">21</span>:51:03<span class="w"> </span>provisioner.py:531<span class="o">]</span><span class="w"> </span>Successfully<span class="w"> </span>provisioned<span class="w"> </span>cluster:<span class="w"> </span>inf2-embeddings-service
</span><span id="__span-14-20"><a id="__codelineno-14-20" name="__codelineno-14-20" href="#__codelineno-14-20"></a>I<span class="w"> </span><span class="m">01</span>-31<span class="w"> </span><span class="m">21</span>:51:04<span class="w"> </span>cloud_vm_ray_backend.py:4418<span class="o">]</span><span class="w"> </span>Processing<span class="w"> </span>file<span class="w"> </span>mounts.
</span><span id="__span-14-21"><a id="__codelineno-14-21" name="__codelineno-14-21" href="#__codelineno-14-21"></a>I<span class="w"> </span><span class="m">01</span>-31<span class="w"> </span><span class="m">21</span>:51:05<span class="w"> </span>cloud_vm_ray_backend.py:4450<span class="o">]</span><span class="w"> </span>To<span class="w"> </span>view<span class="w"> </span>detailed<span class="w"> </span>progress:<span class="w"> </span>tail<span class="w"> </span>-n100<span class="w"> </span>-f<span class="w"> </span>~/sky_logs/sky-2024-01-31-21-48-06-108390/file_mounts.log
</span><span id="__span-14-22"><a id="__codelineno-14-22" name="__codelineno-14-22" href="#__codelineno-14-22"></a>I<span class="w"> </span><span class="m">01</span>-31<span class="w"> </span><span class="m">21</span>:51:05<span class="w"> </span>backend_utils.py:1286<span class="o">]</span><span class="w"> </span>Syncing<span class="w"> </span><span class="o">(</span>to<span class="w"> </span><span class="m">1</span><span class="w"> </span>node<span class="o">)</span>:<span class="w"> </span>.<span class="w"> </span>-&gt;<span class="w"> </span>~/.sky/file_mounts/app
</span><span id="__span-14-23"><a id="__codelineno-14-23" name="__codelineno-14-23" href="#__codelineno-14-23"></a>I<span class="w"> </span><span class="m">01</span>-31<span class="w"> </span><span class="m">21</span>:51:06<span class="w"> </span>cloud_vm_ray_backend.py:3158<span class="o">]</span><span class="w"> </span>Running<span class="w"> </span>setup<span class="w"> </span>on<span class="w"> </span><span class="m">1</span><span class="w"> </span>node.
</span><span id="__span-14-24"><a id="__codelineno-14-24" name="__codelineno-14-24" href="#__codelineno-14-24"></a>...
</span><span id="__span-14-25"><a id="__codelineno-14-25" name="__codelineno-14-25" href="#__codelineno-14-25"></a><span class="o">(</span>task,<span class="w"> </span><span class="nv">pid</span><span class="o">=</span><span class="m">23904</span><span class="o">)</span><span class="w"> </span>‚úì<span class="w"> </span>Launching<span class="w"> </span>docker<span class="w"> </span>compose<span class="w"> </span>with<span class="w"> </span>command:<span class="w"> </span>docker<span class="w"> </span>compose<span class="w"> </span>-f
</span><span id="__span-14-26"><a id="__codelineno-14-26" name="__codelineno-14-26" href="#__codelineno-14-26"></a><span class="o">(</span>task,<span class="w"> </span><span class="nv">pid</span><span class="o">=</span><span class="m">23904</span><span class="o">)</span><span class="w"> </span>/home/ubuntu/.nos/tmp/serve/docker-compose.app.yml<span class="w"> </span>up
</span><span id="__span-14-27"><a id="__codelineno-14-27" name="__codelineno-14-27" href="#__codelineno-14-27"></a><span class="o">(</span>task,<span class="w"> </span><span class="nv">pid</span><span class="o">=</span><span class="m">23904</span><span class="o">)</span><span class="w">  </span>Network<span class="w"> </span>serve_default<span class="w">  </span>Creating
</span><span id="__span-14-28"><a id="__codelineno-14-28" name="__codelineno-14-28" href="#__codelineno-14-28"></a><span class="o">(</span>task,<span class="w"> </span><span class="nv">pid</span><span class="o">=</span><span class="m">23904</span><span class="o">)</span><span class="w">  </span>Network<span class="w"> </span>serve_default<span class="w">  </span>Created
</span><span id="__span-14-29"><a id="__codelineno-14-29" name="__codelineno-14-29" href="#__codelineno-14-29"></a><span class="o">(</span>task,<span class="w"> </span><span class="nv">pid</span><span class="o">=</span><span class="m">23904</span><span class="o">)</span><span class="w">  </span>Container<span class="w"> </span>serve-nos-server-1<span class="w">  </span>Creating
</span><span id="__span-14-30"><a id="__codelineno-14-30" name="__codelineno-14-30" href="#__codelineno-14-30"></a><span class="o">(</span>task,<span class="w"> </span><span class="nv">pid</span><span class="o">=</span><span class="m">23904</span><span class="o">)</span><span class="w">  </span>Container<span class="w"> </span>serve-nos-server-1<span class="w">  </span>Created
</span><span id="__span-14-31"><a id="__codelineno-14-31" name="__codelineno-14-31" href="#__codelineno-14-31"></a><span class="o">(</span>task,<span class="w"> </span><span class="nv">pid</span><span class="o">=</span><span class="m">23904</span><span class="o">)</span><span class="w">  </span>Container<span class="w"> </span>serve-nos-http-gateway-1<span class="w">  </span>Creating
</span><span id="__span-14-32"><a id="__codelineno-14-32" name="__codelineno-14-32" href="#__codelineno-14-32"></a><span class="o">(</span>task,<span class="w"> </span><span class="nv">pid</span><span class="o">=</span><span class="m">23904</span><span class="o">)</span><span class="w">  </span>Container<span class="w"> </span>serve-nos-http-gateway-1<span class="w">  </span>Created
</span><span id="__span-14-33"><a id="__codelineno-14-33" name="__codelineno-14-33" href="#__codelineno-14-33"></a><span class="o">(</span>task,<span class="w"> </span><span class="nv">pid</span><span class="o">=</span><span class="m">23904</span><span class="o">)</span><span class="w"> </span>Attaching<span class="w"> </span>to<span class="w"> </span>serve-nos-http-gateway-1,<span class="w"> </span>serve-nos-server-1
</span><span id="__span-14-34"><a id="__codelineno-14-34" name="__codelineno-14-34" href="#__codelineno-14-34"></a><span class="o">(</span>task,<span class="w"> </span><span class="nv">pid</span><span class="o">=</span><span class="m">23904</span><span class="o">)</span><span class="w"> </span>serve-nos-http-gateway-1<span class="w">  </span><span class="p">|</span><span class="w"> </span>WARNING:<span class="w">  </span>Current<span class="w"> </span>configuration<span class="w"> </span>will<span class="w"> </span>not<span class="w"> </span>reload<span class="w"> </span>as<span class="w"> </span>not<span class="w"> </span>all<span class="w"> </span>conditions<span class="w"> </span>are<span class="w"> </span>met,<span class="w"> </span>please<span class="w"> </span>refer<span class="w"> </span>to<span class="w"> </span>documentation.
</span><span id="__span-14-35"><a id="__codelineno-14-35" name="__codelineno-14-35" href="#__codelineno-14-35"></a><span class="o">(</span>task,<span class="w"> </span><span class="nv">pid</span><span class="o">=</span><span class="m">23904</span><span class="o">)</span><span class="w"> </span>serve-nos-server-1<span class="w">        </span><span class="p">|</span><span class="w">  </span>‚úì<span class="w"> </span>InferenceExecutor<span class="w"> </span>::<span class="w"> </span>Backend<span class="w"> </span>initializing<span class="w"> </span><span class="o">(</span>as<span class="w"> </span>daemon<span class="o">)</span><span class="w"> </span>...
</span><span id="__span-14-36"><a id="__codelineno-14-36" name="__codelineno-14-36" href="#__codelineno-14-36"></a><span class="o">(</span>task,<span class="w"> </span><span class="nv">pid</span><span class="o">=</span><span class="m">23904</span><span class="o">)</span><span class="w"> </span>serve-nos-server-1<span class="w">        </span><span class="p">|</span><span class="w">  </span>‚úì<span class="w"> </span>InferenceExecutor<span class="w"> </span>::<span class="w"> </span>Backend<span class="w"> </span>initialized<span class="w"> </span><span class="o">(</span><span class="nv">elapsed</span><span class="o">=</span><span class="m">2</span>.9s<span class="o">)</span>.
</span><span id="__span-14-37"><a id="__codelineno-14-37" name="__codelineno-14-37" href="#__codelineno-14-37"></a><span class="o">(</span>task,<span class="w"> </span><span class="nv">pid</span><span class="o">=</span><span class="m">23904</span><span class="o">)</span><span class="w"> </span>serve-nos-server-1<span class="w">        </span><span class="p">|</span><span class="w">  </span>‚úì<span class="w"> </span>InferenceExecutor<span class="w"> </span>::<span class="w"> </span>Connected<span class="w"> </span>to<span class="w"> </span>backend.
</span><span id="__span-14-38"><a id="__codelineno-14-38" name="__codelineno-14-38" href="#__codelineno-14-38"></a><span class="o">(</span>task,<span class="w"> </span><span class="nv">pid</span><span class="o">=</span><span class="m">23904</span><span class="o">)</span><span class="w"> </span>serve-nos-server-1<span class="w">        </span><span class="p">|</span><span class="w">  </span>‚úì<span class="w"> </span>Starting<span class="w"> </span>gRPC<span class="w"> </span>server<span class="w"> </span>on<span class="w"> </span><span class="o">[</span>::<span class="o">]</span>:50051
</span><span id="__span-14-39"><a id="__codelineno-14-39" name="__codelineno-14-39" href="#__codelineno-14-39"></a><span class="o">(</span>task,<span class="w"> </span><span class="nv">pid</span><span class="o">=</span><span class="m">23904</span><span class="o">)</span><span class="w"> </span>serve-nos-server-1<span class="w">        </span><span class="p">|</span><span class="w">  </span>‚úì<span class="w"> </span>InferenceService<span class="w"> </span>::<span class="w"> </span>Deployment<span class="w"> </span><span class="nb">complete</span><span class="w"> </span><span class="o">(</span><span class="nv">elapsed</span><span class="o">=</span><span class="m">0</span>.0s<span class="o">)</span>
</span><span id="__span-14-40"><a id="__codelineno-14-40" name="__codelineno-14-40" href="#__codelineno-14-40"></a><span class="o">(</span>task,<span class="w"> </span><span class="nv">pid</span><span class="o">=</span><span class="m">23904</span><span class="o">)</span><span class="w"> </span>serve-nos-server-1<span class="w">        </span><span class="p">|</span><span class="w"> </span><span class="o">(</span>EmbeddingServiceInf2<span class="w"> </span><span class="nv">pid</span><span class="o">=</span><span class="m">404</span><span class="o">)</span><span class="w"> </span><span class="m">2024</span>-01-31<span class="w"> </span><span class="m">21</span>:53:58.566<span class="w"> </span><span class="p">|</span><span class="w"> </span>INFO<span class="w">     </span><span class="p">|</span><span class="w"> </span>nos.neuron.device:setup_environment:36<span class="w"> </span>-<span class="w"> </span>Setting<span class="w"> </span>up<span class="w"> </span>neuron<span class="w"> </span>env<span class="w"> </span>with<span class="w"> </span><span class="m">2</span><span class="w"> </span>cores
</span><span id="__span-14-41"><a id="__codelineno-14-41" name="__codelineno-14-41" href="#__codelineno-14-41"></a>...
</span><span id="__span-14-42"><a id="__codelineno-14-42" name="__codelineno-14-42" href="#__codelineno-14-42"></a><span class="o">(</span>task,<span class="w"> </span><span class="nv">pid</span><span class="o">=</span><span class="m">23904</span><span class="o">)</span><span class="w"> </span>serve-nos-server-1<span class="w">        </span><span class="p">|</span><span class="w"> </span><span class="o">(</span>EmbeddingServiceInf2<span class="w"> </span><span class="nv">pid</span><span class="o">=</span><span class="m">404</span><span class="o">)</span><span class="w"> </span><span class="m">2024</span>-02-01T05:54:36Z<span class="w"> </span>Compiler<span class="w"> </span>status<span class="w"> </span>PASS
</span><span id="__span-14-43"><a id="__codelineno-14-43" name="__codelineno-14-43" href="#__codelineno-14-43"></a><span class="o">(</span>task,<span class="w"> </span><span class="nv">pid</span><span class="o">=</span><span class="m">23904</span><span class="o">)</span><span class="w"> </span>serve-nos-server-1<span class="w">        </span><span class="p">|</span><span class="w"> </span><span class="o">(</span>EmbeddingServiceInf2<span class="w"> </span><span class="nv">pid</span><span class="o">=</span><span class="m">404</span><span class="o">)</span><span class="w"> </span><span class="m">2024</span>-01-31<span class="w"> </span><span class="m">21</span>:54:46.928<span class="w"> </span><span class="p">|</span><span class="w"> </span>INFO<span class="w">     </span><span class="p">|</span><span class="w"> </span>EmbeddingServiceInf2:__init__:61<span class="w"> </span>-<span class="w"> </span>Saved<span class="w"> </span>model<span class="w"> </span>to<span class="w"> </span>/app/.nos/cache/neuron/BAAI/bge-small-en-v1.5-bs-1-sl-384
</span><span id="__span-14-44"><a id="__codelineno-14-44" name="__codelineno-14-44" href="#__codelineno-14-44"></a><span class="o">(</span>task,<span class="w"> </span><span class="nv">pid</span><span class="o">=</span><span class="m">23904</span><span class="o">)</span><span class="w"> </span>serve-nos-server-1<span class="w">        </span><span class="p">|</span><span class="w"> </span><span class="o">(</span>EmbeddingServiceInf2<span class="w"> </span><span class="nv">pid</span><span class="o">=</span><span class="m">404</span><span class="o">)</span><span class="w"> </span><span class="m">2024</span>-01-31<span class="w"> </span><span class="m">21</span>:54:47.037<span class="w"> </span><span class="p">|</span><span class="w"> </span>INFO<span class="w">     </span><span class="p">|</span><span class="w"> </span>EmbeddingServiceInf2:__init__:64<span class="w"> </span>-<span class="w"> </span>Loaded<span class="w"> </span>neuron<span class="w"> </span>model:<span class="w"> </span>BAAI/bge-small-en-v1.5
</span><span id="__span-14-45"><a id="__codelineno-14-45" name="__codelineno-14-45" href="#__codelineno-14-45"></a>...
</span><span id="__span-14-46"><a id="__codelineno-14-46" name="__codelineno-14-46" href="#__codelineno-14-46"></a><span class="o">(</span>task,<span class="w"> </span><span class="nv">pid</span><span class="o">=</span><span class="m">23904</span><span class="o">)</span><span class="w"> </span>serve-nos-server-1<span class="w">        </span><span class="p">|</span><span class="w"> </span><span class="m">2024</span>-01-31<span class="w"> </span><span class="m">22</span>:25:43.710<span class="w"> </span><span class="p">|</span><span class="w"> </span>INFO<span class="w">     </span><span class="p">|</span><span class="w"> </span>nos.server._service:Run:360<span class="w"> </span>-<span class="w"> </span>Executing<span class="w"> </span>request<span class="w"> </span><span class="o">[</span><span class="nv">model</span><span class="o">=</span>BAAI/bge-small-en-v1.5,<span class="w"> </span><span class="nv">method</span><span class="o">=</span>None<span class="o">]</span>
</span><span id="__span-14-47"><a id="__codelineno-14-47" name="__codelineno-14-47" href="#__codelineno-14-47"></a><span class="o">(</span>task,<span class="w"> </span><span class="nv">pid</span><span class="o">=</span><span class="m">23904</span><span class="o">)</span><span class="w"> </span>serve-nos-server-1<span class="w">        </span><span class="p">|</span><span class="w"> </span><span class="m">2024</span>-01-31<span class="w"> </span><span class="m">22</span>:25:43.717<span class="w"> </span><span class="p">|</span><span class="w"> </span>INFO<span class="w">     </span><span class="p">|</span><span class="w"> </span>nos.server._service:Run:362<span class="w"> </span>-<span class="w"> </span>Executed<span class="w"> </span>request<span class="w"> </span><span class="o">[</span><span class="nv">model</span><span class="o">=</span>BAAI/bge-small-en-v1.5,<span class="w"> </span><span class="nv">method</span><span class="o">=</span>None,<span class="w"> </span><span class="nv">elapsed</span><span class="o">=</span><span class="m">7</span>.1ms<span class="o">]</span>
</span></code></pre></div></p>
</details>
<p>Once complete, you should see the following (trimmed) output from the <code>sky launch</code> command:
<div class="language-bash highlight"><pre><span></span><code><span id="__span-15-1"><a id="__codelineno-15-1" name="__codelineno-15-1" href="#__codelineno-15-1"></a>‚úì<span class="w"> </span>InferenceExecutor<span class="w"> </span>::<span class="w"> </span>Backend<span class="w"> </span>initializing<span class="w"> </span><span class="o">(</span>as<span class="w"> </span>daemon<span class="o">)</span><span class="w"> </span>...
</span><span id="__span-15-2"><a id="__codelineno-15-2" name="__codelineno-15-2" href="#__codelineno-15-2"></a>‚úì<span class="w"> </span>InferenceExecutor<span class="w"> </span>::<span class="w"> </span>Backend<span class="w"> </span>initialized<span class="w"> </span><span class="o">(</span><span class="nv">elapsed</span><span class="o">=</span><span class="m">2</span>.9s<span class="o">)</span>.
</span><span id="__span-15-3"><a id="__codelineno-15-3" name="__codelineno-15-3" href="#__codelineno-15-3"></a>‚úì<span class="w"> </span>InferenceExecutor<span class="w"> </span>::<span class="w"> </span>Connected<span class="w"> </span>to<span class="w"> </span>backend.
</span><span id="__span-15-4"><a id="__codelineno-15-4" name="__codelineno-15-4" href="#__codelineno-15-4"></a>‚úì<span class="w"> </span>Starting<span class="w"> </span>gRPC<span class="w"> </span>server<span class="w"> </span>on<span class="w"> </span><span class="o">[</span>::<span class="o">]</span>:50051
</span><span id="__span-15-5"><a id="__codelineno-15-5" name="__codelineno-15-5" href="#__codelineno-15-5"></a>‚úì<span class="w"> </span>InferenceService<span class="w"> </span>::<span class="w"> </span>Deployment<span class="w"> </span><span class="nb">complete</span><span class="w"> </span><span class="o">(</span><span class="nv">elapsed</span><span class="o">=</span><span class="m">0</span>.0s<span class="o">)</span>
</span><span id="__span-15-6"><a id="__codelineno-15-6" name="__codelineno-15-6" href="#__codelineno-15-6"></a>Setting<span class="w"> </span>up<span class="w"> </span>neuron<span class="w"> </span>env<span class="w"> </span>with<span class="w"> </span><span class="m">2</span><span class="w"> </span>cores
</span><span id="__span-15-7"><a id="__codelineno-15-7" name="__codelineno-15-7" href="#__codelineno-15-7"></a>...
</span><span id="__span-15-8"><a id="__codelineno-15-8" name="__codelineno-15-8" href="#__codelineno-15-8"></a>Compiler<span class="w"> </span>status<span class="w"> </span>PASS
</span><span id="__span-15-9"><a id="__codelineno-15-9" name="__codelineno-15-9" href="#__codelineno-15-9"></a>Saved<span class="w"> </span>model<span class="w"> </span>to<span class="w"> </span>/app/.nos/cache/neuron/BAAI/bge-small-en-v1.5-bs-1-sl-384
</span><span id="__span-15-10"><a id="__codelineno-15-10" name="__codelineno-15-10" href="#__codelineno-15-10"></a>Loaded<span class="w"> </span>neuron<span class="w"> </span>model:<span class="w"> </span>BAAI/bge-small-en-v1.5
</span></code></pre></div></p>
<h4 id="3-test-your-custom-model-on-aws-inf2-instance"><a class="toclink" href="../introducing-the-nos-inferentia2-runtime.html#3-test-your-custom-model-on-aws-inf2-instance">3. Test your custom model on AWS Inf2 instance</a></h4>
<p>Once the service is deployed, you should be able to simply make a cURL request to the <code>inf2</code> instance to test the server-side logic of the embeddings model.</p>
<div class="tabbed-set tabbed-alternate" data-tabs="2:2"><input checked="checked" id="__tabbed_2_1" name="__tabbed_2" type="radio" /><input id="__tabbed_2_2" name="__tabbed_2" type="radio" /><div class="tabbed-labels"><label for="__tabbed_2_1">Using cURL (remote)</label><label for="__tabbed_2_2">Using the gRPC client (on the <code>inf2</code> instance)</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<div class="language-bash highlight"><pre><span></span><code><span id="__span-16-1"><a id="__codelineno-16-1" name="__codelineno-16-1" href="#__codelineno-16-1"></a><span class="nb">export</span><span class="w"> </span><span class="nv">IP</span><span class="o">=</span><span class="k">$(</span>sky<span class="w"> </span>status<span class="w"> </span>--ip<span class="w"> </span>inf2-embeddings-service<span class="k">)</span>
</span><span id="__span-16-2"><a id="__codelineno-16-2" name="__codelineno-16-2" href="#__codelineno-16-2"></a>
</span><span id="__span-16-3"><a id="__codelineno-16-3" name="__codelineno-16-3" href="#__codelineno-16-3"></a>curl<span class="w"> </span><span class="se">\</span>
</span><span id="__span-16-4"><a id="__codelineno-16-4" name="__codelineno-16-4" href="#__codelineno-16-4"></a>-X<span class="w"> </span>POST<span class="w"> </span>http://<span class="si">${</span><span class="nv">IP</span><span class="si">}</span>:8000/v1/infer<span class="w"> </span><span class="se">\</span>
</span><span id="__span-16-5"><a id="__codelineno-16-5" name="__codelineno-16-5" href="#__codelineno-16-5"></a>-H<span class="w"> </span><span class="s1">&#39;Content-Type: application/json&#39;</span><span class="w"> </span><span class="se">\</span>
</span><span id="__span-16-6"><a id="__codelineno-16-6" name="__codelineno-16-6" href="#__codelineno-16-6"></a>-d<span class="w"> </span><span class="s1">&#39;{</span>
</span><span id="__span-16-7"><a id="__codelineno-16-7" name="__codelineno-16-7" href="#__codelineno-16-7"></a><span class="s1">    &quot;model_id&quot;: &quot;BAAI/bge-small-en-v1.5&quot;,</span>
</span><span id="__span-16-8"><a id="__codelineno-16-8" name="__codelineno-16-8" href="#__codelineno-16-8"></a><span class="s1">    &quot;inputs&quot;: {</span>
</span><span id="__span-16-9"><a id="__codelineno-16-9" name="__codelineno-16-9" href="#__codelineno-16-9"></a><span class="s1">        &quot;texts&quot;: [&quot;fox jumped over the moon&quot;]</span>
</span><span id="__span-16-10"><a id="__codelineno-16-10" name="__codelineno-16-10" href="#__codelineno-16-10"></a><span class="s1">    }</span>
</span><span id="__span-16-11"><a id="__codelineno-16-11" name="__codelineno-16-11" href="#__codelineno-16-11"></a><span class="s1">}&#39;</span>
</span></code></pre></div>
</div>
<div class="tabbed-block">
<p>Optionally, you can also test the gRPC service using the provided <a href="https://github.com/autonomi-ai/nos/blob/main/examples/inf2/embeddings/tests/test_embeddings_inf2_client.py"><code>tests/test_embeddings_inf2_client.py</code></a>. For this test however, you'll need to ssh into the <code>inf2</code> instance and run the following command.</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-17-1"><a id="__codelineno-17-1" name="__codelineno-17-1" href="#__codelineno-17-1"></a>ssh<span class="w"> </span>inf2-embeddings-service
</span></code></pre></div>
<p>Once you're on the <code>inf2.xlarge</code> instance, you can run <code>pytest -sv tests/test_embeddings_inf2_client.py</code> to test the server-side logic of the embeddings model. </p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-18-1"><a id="__codelineno-18-1" name="__codelineno-18-1" href="#__codelineno-18-1"></a>$<span class="w"> </span>pytest<span class="w"> </span>-sv<span class="w"> </span>tests/test_embeddings_inf2_client.py
</span></code></pre></div>
<p>Here's a simplified version of the test to execute the embeddings model.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-19-1"><a id="__codelineno-19-1" name="__codelineno-19-1" href="#__codelineno-19-1"></a><span class="kn">from</span> <span class="nn">nos.client</span> <span class="kn">import</span> <span class="n">Client</span>
</span><span id="__span-19-2"><a id="__codelineno-19-2" name="__codelineno-19-2" href="#__codelineno-19-2"></a>
</span><span id="__span-19-3"><a id="__codelineno-19-3" name="__codelineno-19-3" href="#__codelineno-19-3"></a><span class="c1"># Create the client</span>
</span><span id="__span-19-4"><a id="__codelineno-19-4" name="__codelineno-19-4" href="#__codelineno-19-4"></a><span class="n">client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="s2">&quot;[::]:50051&quot;</span><span class="p">)</span>
</span><span id="__span-19-5"><a id="__codelineno-19-5" name="__codelineno-19-5" href="#__codelineno-19-5"></a><span class="k">assert</span> <span class="n">client</span><span class="o">.</span><span class="n">WaitForServer</span><span class="p">()</span>
</span><span id="__span-19-6"><a id="__codelineno-19-6" name="__codelineno-19-6" href="#__codelineno-19-6"></a>
</span><span id="__span-19-7"><a id="__codelineno-19-7" name="__codelineno-19-7" href="#__codelineno-19-7"></a><span class="c1"># Load the embeddings model</span>
</span><span id="__span-19-8"><a id="__codelineno-19-8" name="__codelineno-19-8" href="#__codelineno-19-8"></a><span class="n">model</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">Module</span><span class="p">(</span><span class="s2">&quot;BAAI/bge-small-en-v1.5&quot;</span><span class="p">)</span>
</span><span id="__span-19-9"><a id="__codelineno-19-9" name="__codelineno-19-9" href="#__codelineno-19-9"></a>
</span><span id="__span-19-10"><a id="__codelineno-19-10" name="__codelineno-19-10" href="#__codelineno-19-10"></a><span class="c1"># Embed text with the model</span>
</span><span id="__span-19-11"><a id="__codelineno-19-11" name="__codelineno-19-11" href="#__codelineno-19-11"></a><span class="n">texts</span> <span class="o">=</span> <span class="s2">&quot;What is the meaning of life?&quot;</span>
</span><span id="__span-19-12"><a id="__codelineno-19-12" name="__codelineno-19-12" href="#__codelineno-19-12"></a><span class="n">response</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">texts</span><span class="o">=</span><span class="n">texts</span><span class="p">)</span>
</span></code></pre></div>
</div>
</div>
</div>
<h3 id="whats-it-going-to-cost-me"><a class="toclink" href="../introducing-the-nos-inferentia2-runtime.html#whats-it-going-to-cost-me">ü§ë What's it going to cost me?</a></h3>
<p>The table below shows the costs of deploying one of these <em>latency-optimized</em> (<code>bsize=1</code>) embedding services on a single Inf2 instance on AWS. While the costs are only one part of the equation, it is important to note that the AWS Inf2 instances are ~25% cheaper than the NVIDIA A10G instances, and offer a more cost-effective solution for inference workloads on AWS. In the coming weeks, we'll be digging into evaluating the performance of the Inf2 instances with respect to their NVIDIA GPU counterparts on inference metrics such as latency/throughput and cost metrics such as number of requests / $, montly costs etc.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Cloud Instance</th>
<th>Spot</th>
<th>Cost / hr</th>
<th>Cost / month</th>
<th># of Req. / $</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://huggingface.co/BAAI/bge-small-en-v1.5">BAAI/bge-small-en-v1.5</a></td>
<td><code>inf2.xlarge</code></td>
<td>-</td>
<td>$0.75</td>
<td>~$540</td>
<td>~685K / $1</td>
</tr>
<tr>
<td><strong><a href="https://huggingface.co/BAAI/bge-small-en-v1.5">BAAI/bge-small-en-v1.5</a></strong></td>
<td><code>inf2.xlarge</code></td>
<td>‚úÖ</td>
<td><strong>$0.32</strong></td>
<td><strong>~$230</strong></td>
<td>~1.6M / $1</td>
</tr>
</tbody>
</table>
<h3 id="wrapping-up"><a class="toclink" href="../introducing-the-nos-inferentia2-runtime.html#wrapping-up">üéÅ Wrapping up</a></h3>
<p>In this post, we introduced the new NOS <strong><a href="../../concepts/runtime-environments.html#üèÉ‚Äç‚ôÇÔ∏è-supported-runtimes"><code>inf2</code></a></strong> runtime that allows developers to easily develop, and serve models on the <a href="https://aws.amazon.com/en/ec2/instance-types/inf2/">AWS Inferentia2</a> chip. With more cost-efficient, and inference-optimized chips coming to market (<a href="https://cloud.google.com/tpu/docs/v5e-inference">Google TPUs</a>, <a href="https://groq.com/products/">Groq</a>, <a href="https://tenstorrent.com/cards/">Tenstorrent</a> etc), we believe it is important for developers to be able to easily access and deploy models on these devices. The specialized <a href="../../concepts/runtime-environments.html#‚ö°Ô∏è-nos-inference-runtime">NOS Inference Runtime</a> aims to do just that - a fast, and frictionless way to deploy models onto any of the AI accelerators, be it NVIDIA GPUs or AWS Inferentia2 chips, in the cloud, or on-prem.</p>
<p>Thanks for reading, and we hope you found this post useful - and finally, give <a href="https://github.com/autonomi-ai/nos">NOS</a> a try. If you have any questions, or would like to learn more about the <a href="https://github.com/autonomi-ai/nos">NOS</a> <code>inf2</code> runtime, please reach out to us on our <a href="https://github.com/autonomi-ai/nos/issues">GitHub Issues</a> page or join us on <a href="https://discord.gg/QAGgvTuvgg">Discord</a>. 
<br></p>
    
  </div>
</article>
      
        <article class="md-post md-post--excerpt">
  <header class="md-post__header">
    
      <nav class="md-post__authors md-typeset">
        
          <span class="md-author">
            <img src="https://github.com/spillai.png" alt="Sudeep Pillai">
          </span>
        
      </nav>
    
    <div class="md-post__meta md-meta">
      <ul class="md-meta__list">
        <li class="md-meta__item">
          <time datetime="2024-01-18 00:00:00">Jan 18, 2024</time></li>
        
          <li class="md-meta__item">
            in
            
              <a href="../category/infra.html" class="md-meta__link">infra</a>, 
              <a href="../category/tutorials.html" class="md-meta__link">tutorials</a></li>
        
        
          
          <li class="md-meta__item">
            
              10 min read
            
          </li>
        
      </ul>
      
    </div>
  </header>
  <div class="md-post__content md-typeset">
    <h2 id="serving-llms-on-a-budget"><a class="toclink" href="../serving-llms-on-a-budget.html">Serving LLMs on a budget</a></h2>
<p><img src="/docs/blog/assets/nos-phixtral.jpg" width="100%"></p>
<p>Deploying Large Language Models (LLMs) and Mixture of Experts (MoEs) are all the rage today, and for good reason. They are the most powerful and <a href="https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard">closest</a> open-source models in terms of performance to OpenAI GPT-3.5 today. However, it turns out that deploying these models can still be somewhat of a lift for most ML engineers and researchers, both in terms of engineering work and operational costs. For example, the recently announced <a href="https://mistral.ai/news/mixtral-of-experts/">Mixtral 8x7B</a> requires 2x <a href="https://www.nvidia.com/en-us/data-center/a100/">NVIDIA A100-80G GPUs</a>, which can cost upwards of $5000 / month (on-demand) on CSPs.</p>
<p>With recent advancements in <a href="https://huggingface.co/docs/optimum/intel/optimization_inc#optimization">model compression</a>, <a href="https://github.com/mit-han-lab/llm-awq">quantization</a> and <a href="https://mistral.ai/news/mixtral-of-experts/">model mixing</a>, we are now seeing an exciting race unfold to deploy these expert models on a budget, without sacrificing significantly on performance. In this blog post, we'll show you how to deploy the <a href="https://huggingface.co/mlabonne/phixtral-4x2_8">mlabonne/phixtral-4x2_8</a> model on a single <a href="https://www.nvidia.com/en-us/data-center/l4/">NVIDIA L4 GPU</a> for under $160 / month and easily scale-out a dirt-cheap, dedicated inference service of your own. We'll be using <a href="https://github.com/skypilot-org/skypilot">SkyPilot</a> to deploy and manage our <a href="https://github.com/autonomi-ai/nos">NOS</a> service on spot (pre-emptible) instances, making them especially cost-efficient.</p>
<h3 id="what-is-phixtral"><a class="toclink" href="../serving-llms-on-a-budget.html#what-is-phixtral">üß† What is Phixtral?</a></h3>
<p>Inspired inspired by the <a href="https://huggingface.co/mistralai/Mixtral-8x7B-v0.1">mistralai/Mixtral-8x7B-v0.1</a> architecture, <a href="https://huggingface.co/mlabonne/phixtral-4x2_8">mlabonne/phixtral-4x2_8</a> is the first Mixure of Experts (MoE) made with 4 <a href="https://huggingface.co/microsoft/phi-2">microsoft/phi-2</a> models that was recently MIT licensed. The general idea behind mixture-of-experts is to combine the capabilities of multiple models to achieve better performance than each individual model. They are <a href="https://huggingface.co/blog/moe#what-is-a-mixture-of-experts-moe">significantly more memory-efficient</a> for inference too, but that's a post for a later date. In this case, we combine the capabilities of 4 <a href="https://huggingface.co/microsoft/phi-2">microsoft/phi-2</a> models to achieve better performance than each of the individual 2.7B parameter models it's composed of. </p>
<details class="note">
<summary>Breakdown of the <a href="https://huggingface.co/mlabonne/phixtral-4x2_8">mlabonne/phixtral-4x2_8</a> model</summary>
<p>Here's the breakdown of the 4 models that make up the <a href="https://huggingface.co/mlabonne/phixtral-4x2_8">mlabonne/phixtral-4x2_8</a> model:</p>
<div class="language-yaml highlight"><pre><span></span><code><span id="__span-12-1"><a id="__codelineno-12-1" name="__codelineno-12-1" href="#__codelineno-12-1"></a><span class="nt">base_model</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">cognitivecomputations/dolphin-2_6-phi-2</span>
</span><span id="__span-12-2"><a id="__codelineno-12-2" name="__codelineno-12-2" href="#__codelineno-12-2"></a><span class="nt">gate_mode</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">cheap_embed</span>
</span><span id="__span-12-3"><a id="__codelineno-12-3" name="__codelineno-12-3" href="#__codelineno-12-3"></a><span class="nt">experts</span><span class="p">:</span>
</span><span id="__span-12-4"><a id="__codelineno-12-4" name="__codelineno-12-4" href="#__codelineno-12-4"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">source_model</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">cognitivecomputations/dolphin-2_6-phi-2</span>
</span><span id="__span-12-5"><a id="__codelineno-12-5" name="__codelineno-12-5" href="#__codelineno-12-5"></a><span class="w">    </span><span class="nt">positive_prompts</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;&quot;</span><span class="p p-Indicator">]</span>
</span><span id="__span-12-6"><a id="__codelineno-12-6" name="__codelineno-12-6" href="#__codelineno-12-6"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">source_model</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">lxuechen/phi-2-dpo</span>
</span><span id="__span-12-7"><a id="__codelineno-12-7" name="__codelineno-12-7" href="#__codelineno-12-7"></a><span class="w">    </span><span class="nt">positive_prompts</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;&quot;</span><span class="p p-Indicator">]</span>
</span><span id="__span-12-8"><a id="__codelineno-12-8" name="__codelineno-12-8" href="#__codelineno-12-8"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">source_model</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Yhyu13/phi-2-sft-dpo-gpt4_en-ep1</span>
</span><span id="__span-12-9"><a id="__codelineno-12-9" name="__codelineno-12-9" href="#__codelineno-12-9"></a><span class="w">    </span><span class="nt">positive_prompts</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;&quot;</span><span class="p p-Indicator">]</span>
</span><span id="__span-12-10"><a id="__codelineno-12-10" name="__codelineno-12-10" href="#__codelineno-12-10"></a><span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">source_model</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">mrm8488/phi-2-coder</span>
</span><span id="__span-12-11"><a id="__codelineno-12-11" name="__codelineno-12-11" href="#__codelineno-12-11"></a><span class="w">    </span><span class="nt">positive_prompts</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;&quot;</span><span class="p p-Indicator">]</span>
</span></code></pre></div>
<p>You can go to the original model card <a href="https://huggingface.co/mlabonne/phixtral-4x2_8">here</a> for more details on how the model was merged using <a href="https://github.com/cg123/mergekit">mergekit</a>.</p>
</details>
<p>Now, let's take a look at the performance of the <a href="https://huggingface.co/mlabonne/phixtral-4x2_8">mlabonne/phixtral-4x2_8</a> model on the <a href="https://github.com/mlabonne/llm-autoeval?tab=readme-ov-file#evaluation-parameters">Nous Suite</a> compared to other models in the 2.7B parameter range. </p>
<table>
<thead>
<tr>
<th>Model</th>
<th>AGIEval</th>
<th>GPT4All</th>
<th>TruthfulQA</th>
<th>Bigbench</th>
<th>Average</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><a href="https://huggingface.co/mlabonne/phixtral-4x2_8">mlabonne/phixtral-4x2_8</a></strong></td>
<td><strong>33.91</strong></td>
<td>70.44</td>
<td>48.78</td>
<td><strong>37.82</strong></td>
<td><strong>47.78</strong></td>
</tr>
<tr>
<td><a href="https://huggingface.co/cognitivecomputations/dolphin-2_6-phi-2">dolphin-2_6-phi-2</a></td>
<td>33.12</td>
<td>69.85</td>
<td>47.39</td>
<td>37.2</td>
<td>46.89</td>
</tr>
<tr>
<td><a href="https://huggingface.co/lxuechen/phi-2-dpo">phi-2-dpo</a></td>
<td>30.39</td>
<td><strong>71.68</strong></td>
<td><strong>50.75</strong></td>
<td>34.9</td>
<td>46.93</td>
</tr>
<tr>
<td><a href="https://huggingface.co/microsoft/phi-2">phi-2</a></td>
<td>27.98</td>
<td>70.8</td>
<td>44.43</td>
<td>35.21</td>
<td>44.61</td>
</tr>
</tbody>
</table>
<h3 id="serving-phixtral-on-a-budget-with-skypilot-and-nos"><a class="toclink" href="../serving-llms-on-a-budget.html#serving-phixtral-on-a-budget-with-skypilot-and-nos">üí∏ Serving Phixtral on a budget with SkyPilot and NOS</a></h3>
<p>Let's now see how we can deploy the <a href="https://huggingface.co/mlabonne/phixtral-4x2_8">mlabonne/phixtral-4x2_8</a> model on a single NVIDIA L4 GPU for under $160 / month. We'll be using <a href="https://github.com/skypilot-org/skypilot">SkyPilot</a> to deploy and manage our <a href="https://github.com/autonomi-ai/nos">NOS</a> service on spot (pre-emptible) instances, making them especially cost-efficient.</p>
<div class="admonition question">
<p class="admonition-title">What's SkyPilot?</p>
<p>If you're new to SkyPilot, we recommend you go through our <a href="/docs/integrations/skypilot.html" target="_blank">NOS x SkyPilot integration page</a> first to familiarize yourself with the tool.</p>
</div>
<h4 id="1-define-your-custom-model-and-serve-specification"><a class="toclink" href="../serving-llms-on-a-budget.html#1-define-your-custom-model-and-serve-specification">1. Define your custom model and serve specification</a></h4>
<p>In this example, we'll be using the <a href="https://github.com/autonomi-ai/nos-playground/blob/main/examples/llm-streaming-chat/"><code>llm-streaming-chat</code></a> tutorial on <a href="https://github.com/autonomi-ai/nos-playground">NOS playground</a>. First, we'll define our custom phixtral chat model <a href="https://github.com/autonomi-ai/nos-playground/blob/main/examples/llm-streaming-chat/models/phixtral_chat.py"><code>phixtral_chat.py</code></a> and a <a href="https://github.com/autonomi-ai/nos-playground/blob/main/examples/llm-streaming-chat/serve.phixtral.yaml"><code>serve.phixtral.yaml</code></a> serve specification that will be used by NOS to serve our model. The relevant files are shown below:</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-13-1"><a id="__codelineno-13-1" name="__codelineno-13-1" href="#__codelineno-13-1"></a><span class="o">(</span>nos-py38<span class="o">)</span><span class="w"> </span>nos-playground/examples/llm-streaming-chat<span class="w"> </span>$<span class="w"> </span>tree<span class="w"> </span>.
</span><span id="__span-13-2"><a id="__codelineno-13-2" name="__codelineno-13-2" href="#__codelineno-13-2"></a>‚îú‚îÄ‚îÄ<span class="w"> </span>models
</span><span id="__span-13-3"><a id="__codelineno-13-3" name="__codelineno-13-3" href="#__codelineno-13-3"></a>‚îÇ<span class="w">¬†¬† </span>‚îî‚îÄ‚îÄ<span class="w"> </span>phixtral_chat.py
</span><span id="__span-13-4"><a id="__codelineno-13-4" name="__codelineno-13-4" href="#__codelineno-13-4"></a>‚îú‚îÄ‚îÄ<span class="w"> </span>serve.phixtral.yaml
</span></code></pre></div>
<p>The entire chat interface is defined in the <code>StreamingChat</code> module in <a href="https://github.com/autonomi-ai/nos-playground/blob/main/examples/llm-streaming-chat/models/phixtral_chat.py"><code>phixtral_chat.py</code></a>, where the <code>chat</code> method returns a string iterable for the gRPC / HTTP server to stream back model predictions to the client. </p>
<p>The <a href="https://github.com/autonomi-ai/nos-playground/blob/main/examples/llm-streaming-chat/serve.phixtral.yaml"><code>serve.phixtral.yaml</code></a> serve specification defines the custom chat model, and a custom runtime that NOS uses to execute our model. Follow the annotations below to understand the different components of the serve specification.</p>
<div class="annotate">
<div class="language-yaml highlight"><span class="filename">serve.phixtral.yaml</span><pre><span></span><code><span id="__span-14-1"><a id="__codelineno-14-1" name="__codelineno-14-1" href="#__codelineno-14-1"></a><span class="nt">images</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">(1)</span>
</span><span id="__span-14-2"><a id="__codelineno-14-2" name="__codelineno-14-2" href="#__codelineno-14-2"></a><span class="w">  </span><span class="l l-Scalar l-Scalar-Plain">llm-py310-cu121</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">(2)</span>
</span><span id="__span-14-3"><a id="__codelineno-14-3" name="__codelineno-14-3" href="#__codelineno-14-3"></a><span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">base</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">autonomi/nos:latest-py310-cu121 (3)</span>
</span><span id="__span-14-4"><a id="__codelineno-14-4" name="__codelineno-14-4" href="#__codelineno-14-4"></a><span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">pip</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">(4)</span>
</span><span id="__span-14-5"><a id="__codelineno-14-5" name="__codelineno-14-5" href="#__codelineno-14-5"></a><span class="w">      </span><span class="l l-Scalar l-Scalar-Plain">- bitsandbytes</span>
</span><span id="__span-14-6"><a id="__codelineno-14-6" name="__codelineno-14-6" href="#__codelineno-14-6"></a><span class="w">      </span><span class="l l-Scalar l-Scalar-Plain">- transformers</span>
</span><span id="__span-14-7"><a id="__codelineno-14-7" name="__codelineno-14-7" href="#__codelineno-14-7"></a><span class="w">      </span><span class="l l-Scalar l-Scalar-Plain">- einops</span>
</span><span id="__span-14-8"><a id="__codelineno-14-8" name="__codelineno-14-8" href="#__codelineno-14-8"></a><span class="w">      </span><span class="l l-Scalar l-Scalar-Plain">- accelerate</span>
</span><span id="__span-14-9"><a id="__codelineno-14-9" name="__codelineno-14-9" href="#__codelineno-14-9"></a>
</span><span id="__span-14-10"><a id="__codelineno-14-10" name="__codelineno-14-10" href="#__codelineno-14-10"></a><span class="nt">models</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">(5)</span>
</span><span id="__span-14-11"><a id="__codelineno-14-11" name="__codelineno-14-11" href="#__codelineno-14-11"></a><span class="w">  </span><span class="l l-Scalar l-Scalar-Plain">mlabonne/phixtral-4x2_8</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">(6)</span>
</span><span id="__span-14-12"><a id="__codelineno-14-12" name="__codelineno-14-12" href="#__codelineno-14-12"></a><span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">model_cls</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">StreamingChat (7)</span>
</span><span id="__span-14-13"><a id="__codelineno-14-13" name="__codelineno-14-13" href="#__codelineno-14-13"></a><span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">model_path</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">models/phixtral_chat.py (8)</span>
</span><span id="__span-14-14"><a id="__codelineno-14-14" name="__codelineno-14-14" href="#__codelineno-14-14"></a><span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">init_kwargs</span><span class="p p-Indicator">:</span>
</span><span id="__span-14-15"><a id="__codelineno-14-15" name="__codelineno-14-15" href="#__codelineno-14-15"></a><span class="w">      </span><span class="nt">model_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">mlabonne/phixtral-4x2_8</span>
</span><span id="__span-14-16"><a id="__codelineno-14-16" name="__codelineno-14-16" href="#__codelineno-14-16"></a><span class="w w-Error">    </span><span class="nt">default_method</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">chat</span>
</span><span id="__span-14-17"><a id="__codelineno-14-17" name="__codelineno-14-17" href="#__codelineno-14-17"></a><span class="w">    </span><span class="nt">runtime_env</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">llm-gpu</span>
</span><span id="__span-14-18"><a id="__codelineno-14-18" name="__codelineno-14-18" href="#__codelineno-14-18"></a><span class="w">    </span><span class="nt">deployment</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">(9)</span>
</span><span id="__span-14-19"><a id="__codelineno-14-19" name="__codelineno-14-19" href="#__codelineno-14-19"></a><span class="w">      </span><span class="l l-Scalar l-Scalar-Plain">resources</span><span class="p p-Indicator">:</span>
</span><span id="__span-14-20"><a id="__codelineno-14-20" name="__codelineno-14-20" href="#__codelineno-14-20"></a><span class="w">        </span><span class="nt">device</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">auto</span>
</span><span id="__span-14-21"><a id="__codelineno-14-21" name="__codelineno-14-21" href="#__codelineno-14-21"></a><span class="w">        </span><span class="nt">device_memory</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">7Gi (10)</span>
</span></code></pre></div>
</div>
<ol>
<li>Specifies the custom runtime images that will be used to serve our model.</li>
<li>Specifies the name of the custom runtime image (referenced below in <code>runtime_env</code>). </li>
<li>Specifies the base NOS image to use for the custom runtime image. We provide a few pre-built images on <a href="https://hub.docker.com/repository/docker/autonomi/nos/general">dockerhub</a>. </li>
<li>Specifies the pip dependencies to install in the custom runtime image.</li>
<li>Specifies all the custom models we intend to serve.</li>
<li>Specifies the unique name of the custom model (model identifier).</li>
<li>Specifies the model class to use for the custom model.</li>
<li>Specifies the path to the model class definition.</li>
<li>Specifies the deployment resources needed for the custom model.</li>
<li>Specifies the GPU memory to allocate for the custom model.</li>
</ol>
<h4 id="2-test-your-custom-model-locally-with-nos"><a class="toclink" href="../serving-llms-on-a-budget.html#2-test-your-custom-model-locally-with-nos">2. Test your custom model locally with NOS</a></h4>
<p>In order to start the NOS server locally, we can simply run the following command:</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-15-1"><a id="__codelineno-15-1" name="__codelineno-15-1" href="#__codelineno-15-1"></a>nos<span class="w"> </span>serve<span class="w"> </span>up<span class="w"> </span>-c<span class="w"> </span>serve.phixtral.yaml<span class="w"> </span>--http
</span></code></pre></div>
<p>This will build the custom runtime image, and start the NOS server locally, exposing an OpenAI compatible HTTP proxy on port <code>8000</code>. This will allow you to chat with your custom LLM endpoint using any OpenAI API compatible client. </p>
<h4 id="3-deploy-your-nos-service-with-skypilot"><a class="toclink" href="../serving-llms-on-a-budget.html#3-deploy-your-nos-service-with-skypilot">3. Deploy your NOS service with SkyPilot</a></h4>
<p>Now that we have defined our serve YAML specification, let's deploy this service on GCP using <a href="https://github.com/skypilot-org/skypilot">SkyPilot</a>. In this example, we're going to use SkyPilot's <code>sky serve</code> command to deploy our NOS service on spot (pre-emptible) instances on GCP. </p>
<details class="note">
<summary>Deploy on any cloud provider (AWS, Azure, GCP, OCI, Lambda Labs, etc.)</summary>
<p>SkyPilot supports deploying NOS services on any cloud provider. In this example, we're going to use GCP, but you can easily deploy on AWS, Azure, or any other cloud provider of your choice. You can override <code>gcp</code> by providing the <code>--cloud</code> flag to <code>sky serve up</code>.</p>
</details>
<p>Let's define a serving configuration in a <a href="https://github.com/autonomi-ai/nos-playground/blob/main/deployments/deploy-llms-with-skypilot/service-phixtral.sky.yaml"><code>service-phixtral.sky.yaml</code></a> file. This YAML specification will be used by SkyPilot to deploy and manage our NOS service on pre-emptible instances, automatically provisioning and recovering from failovers, setting up new instances when needed on server pre-emptions. </p>
<div class="annotate">
<div class="language-yaml highlight"><span class="filename">service-phixtral.sky.yaml</span><pre><span></span><code><span id="__span-16-1"><a id="__codelineno-16-1" name="__codelineno-16-1" href="#__codelineno-16-1"></a><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">service-phixtral</span>
</span><span id="__span-16-2"><a id="__codelineno-16-2" name="__codelineno-16-2" href="#__codelineno-16-2"></a>
</span><span id="__span-16-3"><a id="__codelineno-16-3" name="__codelineno-16-3" href="#__codelineno-16-3"></a><span class="nt">file_mounts</span><span class="p">:</span>
</span><span id="__span-16-4"><a id="__codelineno-16-4" name="__codelineno-16-4" href="#__codelineno-16-4"></a><span class="w">  </span><span class="nt">/app</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">./app (1)</span>
</span><span id="__span-16-5"><a id="__codelineno-16-5" name="__codelineno-16-5" href="#__codelineno-16-5"></a>
</span><span id="__span-16-6"><a id="__codelineno-16-6" name="__codelineno-16-6" href="#__codelineno-16-6"></a><span class="nt">resources</span><span class="p">:</span>
</span><span id="__span-16-7"><a id="__codelineno-16-7" name="__codelineno-16-7" href="#__codelineno-16-7"></a><span class="w">  </span><span class="nt">cloud</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gcp</span>
</span><span id="__span-16-8"><a id="__codelineno-16-8" name="__codelineno-16-8" href="#__codelineno-16-8"></a><span class="w">  </span><span class="nt">accelerators</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">L4:1</span>
</span><span id="__span-16-9"><a id="__codelineno-16-9" name="__codelineno-16-9" href="#__codelineno-16-9"></a><span class="w">  </span><span class="nt">use_spot</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True (2)</span>
</span><span id="__span-16-10"><a id="__codelineno-16-10" name="__codelineno-16-10" href="#__codelineno-16-10"></a><span class="w">  </span><span class="nt">ports</span><span class="p">:</span>
</span><span id="__span-16-11"><a id="__codelineno-16-11" name="__codelineno-16-11" href="#__codelineno-16-11"></a><span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8000</span>
</span><span id="__span-16-12"><a id="__codelineno-16-12" name="__codelineno-16-12" href="#__codelineno-16-12"></a>
</span><span id="__span-16-13"><a id="__codelineno-16-13" name="__codelineno-16-13" href="#__codelineno-16-13"></a><span class="nt">service</span><span class="p">:</span>
</span><span id="__span-16-14"><a id="__codelineno-16-14" name="__codelineno-16-14" href="#__codelineno-16-14"></a><span class="w">  </span><span class="nt">readiness_probe</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">(3)</span>
</span><span id="__span-16-15"><a id="__codelineno-16-15" name="__codelineno-16-15" href="#__codelineno-16-15"></a><span class="w">    </span><span class="l l-Scalar l-Scalar-Plain">path</span><span class="p p-Indicator">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/v1/health</span><span class="w"> </span>
</span><span id="__span-16-16"><a id="__codelineno-16-16" name="__codelineno-16-16" href="#__codelineno-16-16"></a><span class="w">  </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2 (4)</span>
</span><span id="__span-16-17"><a id="__codelineno-16-17" name="__codelineno-16-17" href="#__codelineno-16-17"></a>
</span><span id="__span-16-18"><a id="__codelineno-16-18" name="__codelineno-16-18" href="#__codelineno-16-18"></a><span class="nt">setup</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
</span><span id="__span-16-19"><a id="__codelineno-16-19" name="__codelineno-16-19" href="#__codelineno-16-19"></a><span class="w">  </span><span class="no">sudo apt-get install -y docker-compose-plugin</span>
</span><span id="__span-16-20"><a id="__codelineno-16-20" name="__codelineno-16-20" href="#__codelineno-16-20"></a><span class="w">  </span><span class="no">pip install torch-nos</span>
</span><span id="__span-16-21"><a id="__codelineno-16-21" name="__codelineno-16-21" href="#__codelineno-16-21"></a>
</span><span id="__span-16-22"><a id="__codelineno-16-22" name="__codelineno-16-22" href="#__codelineno-16-22"></a><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">|</span>
</span><span id="__span-16-23"><a id="__codelineno-16-23" name="__codelineno-16-23" href="#__codelineno-16-23"></a><span class="w">  </span><span class="no">cd /app &amp;&amp; nos serve up -c serve.phixtral.yaml --http (5)</span>
</span></code></pre></div>
</div>
<ol>
<li>Setup file-mounts to mount the local <code>./app</code> directory so that the <code>serve.phixtral.yaml</code> and <code>models/</code> directory are available on the remote instance.</li>
<li>Use spot (pre-emptible) instances instead of on-demand instances.</li>
<li>Define the readiness probe path for the service. This allows the SkyPilot controller to check the health of the service and recover from failures if needed.</li>
<li>Define the number of replicas to deploy.</li>
<li>Define the <code>run</code> command to execute on each replica. In this case, we're simply starting the NOS server with the phixtral model deployed on init. </li>
</ol>
<p>To deploy our NOS service, we can simply run the following command:</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-17-1"><a id="__codelineno-17-1" name="__codelineno-17-1" href="#__codelineno-17-1"></a>sky<span class="w"> </span>serve<span class="w"> </span>up<span class="w"> </span>-n<span class="w"> </span>service-mixtral<span class="w"> </span>service-mixtral.sky.yaml
</span></code></pre></div>
<p>SkyPilot will automatically pick the cheapest region and zone to deploy our service, and provision the necessary cloud resources to deploy the NOS server. In this case, you'll notice that SkyPilot provisioned two <a href="https://www.nvidia.com/en-us/data-center/l4/">NVIDIA L4 GPU</a> instances on GCP in the <code>us-central1-a</code> availability zone. </p>
<p>You should see the following output:</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-18-1"><a id="__codelineno-18-1" name="__codelineno-18-1" href="#__codelineno-18-1"></a><span class="o">(</span>nos-infra-py38<span class="o">)</span><span class="w"> </span>deployments/deploy-llms-with-skypilot<span class="w"> </span>$<span class="w"> </span>sky<span class="w"> </span>serve<span class="w"> </span>up<span class="w"> </span>-n<span class="w"> </span>service-mixtral<span class="w"> </span>service-mixtral.sky.yaml
</span><span id="__span-18-2"><a id="__codelineno-18-2" name="__codelineno-18-2" href="#__codelineno-18-2"></a>Service<span class="w"> </span>from<span class="w"> </span>YAML<span class="w"> </span>spec:<span class="w"> </span>service-mixtral.sky.yaml
</span><span id="__span-18-3"><a id="__codelineno-18-3" name="__codelineno-18-3" href="#__codelineno-18-3"></a>Service<span class="w"> </span>Spec:
</span><span id="__span-18-4"><a id="__codelineno-18-4" name="__codelineno-18-4" href="#__codelineno-18-4"></a>Readiness<span class="w"> </span>probe<span class="w"> </span>method:<span class="w">           </span>GET<span class="w"> </span>/v1/health
</span><span id="__span-18-5"><a id="__codelineno-18-5" name="__codelineno-18-5" href="#__codelineno-18-5"></a>Readiness<span class="w"> </span>initial<span class="w"> </span>delay<span class="w"> </span>seconds:<span class="w">  </span><span class="m">1200</span>
</span><span id="__span-18-6"><a id="__codelineno-18-6" name="__codelineno-18-6" href="#__codelineno-18-6"></a>Replica<span class="w"> </span>autoscaling<span class="w"> </span>policy:<span class="w">       </span>Fixed<span class="w"> </span><span class="m">2</span><span class="w"> </span>replicas
</span><span id="__span-18-7"><a id="__codelineno-18-7" name="__codelineno-18-7" href="#__codelineno-18-7"></a>Replica<span class="w"> </span>auto<span class="w"> </span>restart:<span class="w">             </span>True
</span><span id="__span-18-8"><a id="__codelineno-18-8" name="__codelineno-18-8" href="#__codelineno-18-8"></a>Each<span class="w"> </span>replica<span class="w"> </span>will<span class="w"> </span>use<span class="w"> </span>the<span class="w"> </span>following<span class="w"> </span>resources<span class="w"> </span><span class="o">(</span>estimated<span class="o">)</span>:
</span><span id="__span-18-9"><a id="__codelineno-18-9" name="__codelineno-18-9" href="#__codelineno-18-9"></a>I<span class="w"> </span><span class="m">01</span>-19<span class="w"> </span><span class="m">16</span>:01:58<span class="w"> </span>optimizer.py:694<span class="o">]</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nv">Optimizer</span><span class="w"> </span><span class="o">==</span>
</span><span id="__span-18-10"><a id="__codelineno-18-10" name="__codelineno-18-10" href="#__codelineno-18-10"></a>I<span class="w"> </span><span class="m">01</span>-19<span class="w"> </span><span class="m">16</span>:01:58<span class="w"> </span>optimizer.py:705<span class="o">]</span><span class="w"> </span>Target:<span class="w"> </span>minimizing<span class="w"> </span>cost
</span><span id="__span-18-11"><a id="__codelineno-18-11" name="__codelineno-18-11" href="#__codelineno-18-11"></a>I<span class="w"> </span><span class="m">01</span>-19<span class="w"> </span><span class="m">16</span>:01:58<span class="w"> </span>optimizer.py:717<span class="o">]</span><span class="w"> </span>Estimated<span class="w"> </span>cost:<span class="w"> </span><span class="nv">$0</span>.2<span class="w"> </span>/<span class="w"> </span>hour
</span><span id="__span-18-12"><a id="__codelineno-18-12" name="__codelineno-18-12" href="#__codelineno-18-12"></a>I<span class="w"> </span><span class="m">01</span>-19<span class="w"> </span><span class="m">16</span>:01:58<span class="w"> </span>optimizer.py:717<span class="o">]</span>
</span><span id="__span-18-13"><a id="__codelineno-18-13" name="__codelineno-18-13" href="#__codelineno-18-13"></a>I<span class="w"> </span><span class="m">01</span>-19<span class="w"> </span><span class="m">16</span>:01:58<span class="w"> </span>optimizer.py:840<span class="o">]</span><span class="w"> </span>Considered<span class="w"> </span>resources<span class="w"> </span><span class="o">(</span><span class="m">1</span><span class="w"> </span>node<span class="o">)</span>:
</span><span id="__span-18-14"><a id="__codelineno-18-14" name="__codelineno-18-14" href="#__codelineno-18-14"></a>I<span class="w"> </span><span class="m">01</span>-19<span class="w"> </span><span class="m">16</span>:01:58<span class="w"> </span>optimizer.py:910<span class="o">]</span><span class="w"> </span>----------------------------------------------------------------------------------------------------
</span><span id="__span-18-15"><a id="__codelineno-18-15" name="__codelineno-18-15" href="#__codelineno-18-15"></a>I<span class="w"> </span><span class="m">01</span>-19<span class="w"> </span><span class="m">16</span>:01:58<span class="w"> </span>optimizer.py:910<span class="o">]</span><span class="w">  </span>CLOUD<span class="w">   </span>INSTANCE<span class="w">              </span>vCPUs<span class="w">   </span>Mem<span class="o">(</span>GB<span class="o">)</span><span class="w">   </span>ACCELERATORS<span class="w">   </span>REGION/ZONE<span class="w">     </span>COST<span class="w"> </span><span class="o">(</span>$<span class="o">)</span><span class="w">   </span>CHOSEN
</span><span id="__span-18-16"><a id="__codelineno-18-16" name="__codelineno-18-16" href="#__codelineno-18-16"></a>I<span class="w"> </span><span class="m">01</span>-19<span class="w"> </span><span class="m">16</span>:01:58<span class="w"> </span>optimizer.py:910<span class="o">]</span><span class="w"> </span>----------------------------------------------------------------------------------------------------
</span><span id="__span-18-17"><a id="__codelineno-18-17" name="__codelineno-18-17" href="#__codelineno-18-17"></a>I<span class="w"> </span><span class="m">01</span>-19<span class="w"> </span><span class="m">16</span>:01:58<span class="w"> </span>optimizer.py:910<span class="o">]</span><span class="w">  </span>GCP<span class="w">     </span>g2-standard-4<span class="o">[</span>Spot<span class="o">]</span><span class="w">   </span><span class="m">4</span><span class="w">       </span><span class="m">16</span><span class="w">        </span>L4:1<span class="w">           </span>us-central1-a<span class="w">   </span><span class="m">0</span>.22<span class="w">          </span>‚úî
</span><span id="__span-18-18"><a id="__codelineno-18-18" name="__codelineno-18-18" href="#__codelineno-18-18"></a>I<span class="w"> </span><span class="m">01</span>-19<span class="w"> </span><span class="m">16</span>:01:58<span class="w"> </span>optimizer.py:910<span class="o">]</span><span class="w"> </span>----------------------------------------------------------------------------------------------------
</span><span id="__span-18-19"><a id="__codelineno-18-19" name="__codelineno-18-19" href="#__codelineno-18-19"></a>I<span class="w"> </span><span class="m">01</span>-19<span class="w"> </span><span class="m">16</span>:01:58<span class="w"> </span>optimizer.py:910<span class="o">]</span>
</span><span id="__span-18-20"><a id="__codelineno-18-20" name="__codelineno-18-20" href="#__codelineno-18-20"></a>Launching<span class="w"> </span>a<span class="w"> </span>new<span class="w"> </span>service<span class="w"> </span><span class="s1">&#39;service-mixtral&#39;</span>.<span class="w"> </span>Proceed?<span class="w"> </span><span class="o">[</span>Y/n<span class="o">]</span>:<span class="w"> </span>y
</span><span id="__span-18-21"><a id="__codelineno-18-21" name="__codelineno-18-21" href="#__codelineno-18-21"></a>Launching<span class="w"> </span>controller<span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="s1">&#39;service-mixtral&#39;</span>
</span><span id="__span-18-22"><a id="__codelineno-18-22" name="__codelineno-18-22" href="#__codelineno-18-22"></a>...
</span><span id="__span-18-23"><a id="__codelineno-18-23" name="__codelineno-18-23" href="#__codelineno-18-23"></a>I<span class="w"> </span><span class="m">01</span>-19<span class="w"> </span><span class="m">16</span>:02:14<span class="w"> </span>cloud_vm_ray_backend.py:1912<span class="o">]</span><span class="w"> </span>Launching<span class="w"> </span>on<span class="w"> </span>GCP<span class="w"> </span>us-west1<span class="w"> </span><span class="o">(</span>us-west1-a<span class="o">)</span>
</span><span id="__span-18-24"><a id="__codelineno-18-24" name="__codelineno-18-24" href="#__codelineno-18-24"></a>I<span class="w"> </span><span class="m">01</span>-19<span class="w"> </span><span class="m">16</span>:02:30<span class="w"> </span>log_utils.py:45<span class="o">]</span><span class="w"> </span>Head<span class="w"> </span>node<span class="w"> </span>is<span class="w"> </span>up.
</span><span id="__span-18-25"><a id="__codelineno-18-25" name="__codelineno-18-25" href="#__codelineno-18-25"></a>I<span class="w"> </span><span class="m">01</span>-19<span class="w"> </span><span class="m">16</span>:03:03<span class="w"> </span>cloud_vm_ray_backend.py:1717<span class="o">]</span><span class="w"> </span>Successfully<span class="w"> </span>provisioned<span class="w"> </span>or<span class="w"> </span>found<span class="w"> </span>existing<span class="w"> </span>VM.
</span><span id="__span-18-26"><a id="__codelineno-18-26" name="__codelineno-18-26" href="#__codelineno-18-26"></a>I<span class="w"> </span><span class="m">01</span>-19<span class="w"> </span><span class="m">16</span>:03:05<span class="w"> </span>cloud_vm_ray_backend.py:4558<span class="o">]</span><span class="w"> </span>Processing<span class="w"> </span>file<span class="w"> </span>mounts.
</span><span id="__span-18-27"><a id="__codelineno-18-27" name="__codelineno-18-27" href="#__codelineno-18-27"></a>...
</span><span id="__span-18-28"><a id="__codelineno-18-28" name="__codelineno-18-28" href="#__codelineno-18-28"></a>I<span class="w"> </span><span class="m">01</span>-19<span class="w"> </span><span class="m">16</span>:03:20<span class="w"> </span>cloud_vm_ray_backend.py:3325<span class="o">]</span><span class="w"> </span>Setup<span class="w"> </span>completed.
</span><span id="__span-18-29"><a id="__codelineno-18-29" name="__codelineno-18-29" href="#__codelineno-18-29"></a>I<span class="w"> </span><span class="m">01</span>-19<span class="w"> </span><span class="m">16</span>:03:29<span class="w"> </span>cloud_vm_ray_backend.py:3422<span class="o">]</span><span class="w"> </span>Job<span class="w"> </span>submitted<span class="w"> </span>with<span class="w"> </span>Job<span class="w"> </span>ID:<span class="w"> </span><span class="m">11</span>
</span><span id="__span-18-30"><a id="__codelineno-18-30" name="__codelineno-18-30" href="#__codelineno-18-30"></a>
</span><span id="__span-18-31"><a id="__codelineno-18-31" name="__codelineno-18-31" href="#__codelineno-18-31"></a>Service<span class="w"> </span>name:<span class="w"> </span>service-mixtral
</span><span id="__span-18-32"><a id="__codelineno-18-32" name="__codelineno-18-32" href="#__codelineno-18-32"></a>Endpoint<span class="w"> </span>URL:<span class="w"> </span>XX.XXX.X.XXX:30001
</span><span id="__span-18-33"><a id="__codelineno-18-33" name="__codelineno-18-33" href="#__codelineno-18-33"></a>To<span class="w"> </span>see<span class="w"> </span>detailed<span class="w"> </span>info:<span class="w">           </span>sky<span class="w"> </span>serve<span class="w"> </span>status<span class="w"> </span>service-mixtral<span class="w"> </span><span class="o">[</span>--endpoint<span class="o">]</span>
</span><span id="__span-18-34"><a id="__codelineno-18-34" name="__codelineno-18-34" href="#__codelineno-18-34"></a>To<span class="w"> </span>teardown<span class="w"> </span>the<span class="w"> </span>service:<span class="w">        </span>sky<span class="w"> </span>serve<span class="w"> </span>down<span class="w"> </span>service-mixtral
</span><span id="__span-18-35"><a id="__codelineno-18-35" name="__codelineno-18-35" href="#__codelineno-18-35"></a>
</span><span id="__span-18-36"><a id="__codelineno-18-36" name="__codelineno-18-36" href="#__codelineno-18-36"></a>To<span class="w"> </span>see<span class="w"> </span>logs<span class="w"> </span>of<span class="w"> </span>a<span class="w"> </span>replica:<span class="w">       </span>sky<span class="w"> </span>serve<span class="w"> </span>logs<span class="w"> </span>service-mixtral<span class="w"> </span><span class="o">[</span>REPLICA_ID<span class="o">]</span>
</span><span id="__span-18-37"><a id="__codelineno-18-37" name="__codelineno-18-37" href="#__codelineno-18-37"></a>To<span class="w"> </span>see<span class="w"> </span>logs<span class="w"> </span>of<span class="w"> </span>load<span class="w"> </span>balancer:<span class="w">   </span>sky<span class="w"> </span>serve<span class="w"> </span>logs<span class="w"> </span>--load-balancer<span class="w"> </span>service-mixtral
</span><span id="__span-18-38"><a id="__codelineno-18-38" name="__codelineno-18-38" href="#__codelineno-18-38"></a>To<span class="w"> </span>see<span class="w"> </span>logs<span class="w"> </span>of<span class="w"> </span>controller:<span class="w">      </span>sky<span class="w"> </span>serve<span class="w"> </span>logs<span class="w"> </span>--controller<span class="w"> </span>service-mixtral
</span><span id="__span-18-39"><a id="__codelineno-18-39" name="__codelineno-18-39" href="#__codelineno-18-39"></a>
</span><span id="__span-18-40"><a id="__codelineno-18-40" name="__codelineno-18-40" href="#__codelineno-18-40"></a>To<span class="w"> </span>monitor<span class="w"> </span>replica<span class="w"> </span>status:<span class="w">      </span>watch<span class="w"> </span>-n10<span class="w"> </span>sky<span class="w"> </span>serve<span class="w"> </span>status<span class="w"> </span>service-mixtral
</span><span id="__span-18-41"><a id="__codelineno-18-41" name="__codelineno-18-41" href="#__codelineno-18-41"></a>To<span class="w"> </span>send<span class="w"> </span>a<span class="w"> </span><span class="nb">test</span><span class="w"> </span>request:<span class="w">         </span>curl<span class="w"> </span>-L<span class="w"> </span>XX.XX.X.XXX:30001
</span></code></pre></div>
<p>Once the service is deployed, you can get the IP address of the SkyPilot service via:. </p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-19-1"><a id="__codelineno-19-1" name="__codelineno-19-1" href="#__codelineno-19-1"></a>sky<span class="w"> </span>serve<span class="w"> </span>status<span class="w"> </span>service-phixtral<span class="w"> </span>--endpoint
</span></code></pre></div>
<p>We'll refer to <code>&lt;sky-serve-ip&gt;</code> as the load balancer's IP address, that takes the full form of <code>&lt;sky-serve-ip&gt;:30001</code>. You should now be able to ping the load-balancer endpoint directly with <code>cURL</code> and see the following output:</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-20-1"><a id="__codelineno-20-1" name="__codelineno-20-1" href="#__codelineno-20-1"></a>$<span class="w"> </span>curl<span class="w"> </span>-L<span class="w"> </span>http://&lt;sky-serve-ip&gt;:30001/v1/health
</span><span id="__span-20-2"><a id="__codelineno-20-2" name="__codelineno-20-2" href="#__codelineno-20-2"></a><span class="o">{</span><span class="s2">&quot;status&quot;</span>:<span class="s2">&quot;ok&quot;</span><span class="o">}</span>
</span></code></pre></div>
<h3 id="chatting-with-your-custom-phixtral-service"><a class="toclink" href="../serving-llms-on-a-budget.html#chatting-with-your-custom-phixtral-service">üí¨ Chatting with your custom Phixtral service</a></h3>
<p>You're now ready to chat with your hosted, custom LLM endpoint! Here's a quick demo of the <a href="https://huggingface.co/mlabonne/phixtral-4x2_8">mlabonne/phixtral-4x2_8</a> model served with NOS across 2 spot (pre-emptible) instances. </p>
<script async id="asciicast-632285" src="https://asciinema.org/a/632285.js"></script>

<p>On the top, you'll see the logs from both the serve replicas, and the corresponding chats that are happening <em>concurrently</em> on the bottom. SkyPilot handles the load-balancing and routing of requests to the replicas, while NOS handles the custom model serving and streaming inference. Below, we show you how you can chat with your hosted LLM endpoint using <code>cURL</code>, an OpenAI compatible client, or the OpenAI Python client.</p>
<div class="tabbed-set tabbed-alternate" data-tabs="2:3"><input checked="checked" id="__tabbed_2_1" name="__tabbed_2" type="radio" /><input id="__tabbed_2_2" name="__tabbed_2" type="radio" /><input id="__tabbed_2_3" name="__tabbed_2" type="radio" /><div class="tabbed-labels"><label for="__tabbed_2_1">Using cURL</label><label for="__tabbed_2_2">Using an OpenAI compatible client</label><label for="__tabbed_2_3">Using the OpenAI Python client</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<div class="language-bash highlight"><pre><span></span><code><span id="__span-21-1"><a id="__codelineno-21-1" name="__codelineno-21-1" href="#__codelineno-21-1"></a>curl<span class="w"> </span><span class="se">\</span>
</span><span id="__span-21-2"><a id="__codelineno-21-2" name="__codelineno-21-2" href="#__codelineno-21-2"></a>-X<span class="w"> </span>POST<span class="w"> </span>-L<span class="w"> </span>http://&lt;sky-serve-ip&gt;:30001/v1/chat/completions<span class="w"> </span><span class="se">\</span>
</span><span id="__span-21-3"><a id="__codelineno-21-3" name="__codelineno-21-3" href="#__codelineno-21-3"></a>-H<span class="w"> </span><span class="s2">&quot;Content-Type: application/json&quot;</span><span class="w"> </span><span class="se">\</span>
</span><span id="__span-21-4"><a id="__codelineno-21-4" name="__codelineno-21-4" href="#__codelineno-21-4"></a>-d<span class="w"> </span><span class="s1">&#39;{</span>
</span><span id="__span-21-5"><a id="__codelineno-21-5" name="__codelineno-21-5" href="#__codelineno-21-5"></a><span class="s1">    &quot;model&quot;: &quot;mlabonne/phixtral-4x2_8&quot;,</span>
</span><span id="__span-21-6"><a id="__codelineno-21-6" name="__codelineno-21-6" href="#__codelineno-21-6"></a><span class="s1">    &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Tell me a joke in 300 words&quot;}],</span>
</span><span id="__span-21-7"><a id="__codelineno-21-7" name="__codelineno-21-7" href="#__codelineno-21-7"></a><span class="s1">    &quot;temperature&quot;: 0.7, &quot;stream&quot;: true</span>
</span><span id="__span-21-8"><a id="__codelineno-21-8" name="__codelineno-21-8" href="#__codelineno-21-8"></a><span class="s1">  }&#39;</span>
</span></code></pre></div>
</div>
<div class="tabbed-block">
<p>Below, we show how you can use any OpenAI API compatible client to chat with your hosted LLM endpoint. We will use the popular <a href="https://github.com/simonw/llm">llm</a> CLI tool from <a href="https://simonwillison.net/">Simon Willison</a> to chat with our hosted LLM endpoint.</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-22-1"><a id="__codelineno-22-1" name="__codelineno-22-1" href="#__codelineno-22-1"></a><span class="c1"># Install the llm CLI tool</span>
</span><span id="__span-22-2"><a id="__codelineno-22-2" name="__codelineno-22-2" href="#__codelineno-22-2"></a>$<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>llm
</span><span id="__span-22-3"><a id="__codelineno-22-3" name="__codelineno-22-3" href="#__codelineno-22-3"></a>
</span><span id="__span-22-4"><a id="__codelineno-22-4" name="__codelineno-22-4" href="#__codelineno-22-4"></a><span class="c1"># Install the llm-nosrun plugin to talk to your service</span>
</span><span id="__span-22-5"><a id="__codelineno-22-5" name="__codelineno-22-5" href="#__codelineno-22-5"></a>$<span class="w"> </span>llm<span class="w"> </span>install<span class="w"> </span>llm-nosrun
</span><span id="__span-22-6"><a id="__codelineno-22-6" name="__codelineno-22-6" href="#__codelineno-22-6"></a>
</span><span id="__span-22-7"><a id="__codelineno-22-7" name="__codelineno-22-7" href="#__codelineno-22-7"></a><span class="c1"># List the models</span>
</span><span id="__span-22-8"><a id="__codelineno-22-8" name="__codelineno-22-8" href="#__codelineno-22-8"></a>$<span class="w"> </span>llm<span class="w"> </span>models<span class="w"> </span>list
</span><span id="__span-22-9"><a id="__codelineno-22-9" name="__codelineno-22-9" href="#__codelineno-22-9"></a>
</span><span id="__span-22-10"><a id="__codelineno-22-10" name="__codelineno-22-10" href="#__codelineno-22-10"></a><span class="c1"># Chat with your endpoint</span>
</span><span id="__span-22-11"><a id="__codelineno-22-11" name="__codelineno-22-11" href="#__codelineno-22-11"></a>$<span class="w"> </span><span class="nv">NOSRUN_API_BASE</span><span class="o">=</span>http://&lt;sky-serve-ip&gt;:30001/v1<span class="w"> </span>llm<span class="w"> </span>-m<span class="w"> </span>mlabonne/phixtral-4x2_8<span class="w"> </span><span class="s2">&quot;Tell me a joke in 300 words&quot;</span>
</span></code></pre></div>
</div>
<div class="tabbed-block">
<p>Below, we show how you can use the <a href="https://github.com/openai/openai-python">OpenAI Python Client</a> to chat with your hosted LLM endpoint.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-23-1"><a id="__codelineno-23-1" name="__codelineno-23-1" href="#__codelineno-23-1"></a><span class="kn">import</span> <span class="nn">openai</span>
</span><span id="__span-23-2"><a id="__codelineno-23-2" name="__codelineno-23-2" href="#__codelineno-23-2"></a>
</span><span id="__span-23-3"><a id="__codelineno-23-3" name="__codelineno-23-3" href="#__codelineno-23-3"></a><span class="c1"># Create a stream and print the output</span>
</span><span id="__span-23-4"><a id="__codelineno-23-4" name="__codelineno-23-4" href="#__codelineno-23-4"></a><span class="n">client</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">OpenAI</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="s2">&quot;no-key-required&quot;</span><span class="p">,</span> <span class="n">base_url</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;http://&lt;sky-serve-ip&gt;:30001/v1&quot;</span><span class="p">)</span>
</span><span id="__span-23-5"><a id="__codelineno-23-5" name="__codelineno-23-5" href="#__codelineno-23-5"></a><span class="n">stream</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
</span><span id="__span-23-6"><a id="__codelineno-23-6" name="__codelineno-23-6" href="#__codelineno-23-6"></a>    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;mlabonne/phixtral-4x2_8&quot;</span><span class="p">,</span>
</span><span id="__span-23-7"><a id="__codelineno-23-7" name="__codelineno-23-7" href="#__codelineno-23-7"></a>    <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Tell me a joke in 300 words&quot;</span><span class="p">}],</span>
</span><span id="__span-23-8"><a id="__codelineno-23-8" name="__codelineno-23-8" href="#__codelineno-23-8"></a>    <span class="n">stream</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="__span-23-9"><a id="__codelineno-23-9" name="__codelineno-23-9" href="#__codelineno-23-9"></a><span class="p">)</span>
</span><span id="__span-23-10"><a id="__codelineno-23-10" name="__codelineno-23-10" href="#__codelineno-23-10"></a><span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">stream</span><span class="p">:</span>
</span><span id="__span-23-11"><a id="__codelineno-23-11" name="__codelineno-23-11" href="#__codelineno-23-11"></a>    <span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">delta</span><span class="o">.</span><span class="n">content</span> <span class="ow">or</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>
</span></code></pre></div>
</div>
</div>
</div>
<div class="admonition info">
<p class="admonition-title">Info</p>
<p>In these examples, we use SkyPilot's load-balancer port <code>30001</code>  which redirects HTTP traffic to one of the many NOS replicas (on port <code>8000</code>) in a round-robin fashion. This allows us to scale-out our service to multiple replicas, and load-balance requests across them.</p>
</div>
<h3 id="whats-it-going-to-cost-me"><a class="toclink" href="../serving-llms-on-a-budget.html#whats-it-going-to-cost-me">ü§ë What's it going to cost me?</a></h3>
<p>In the example above, we were able to deploy the <a href="https://huggingface.co/mlabonne/phixtral-4x2_8">mlabonne/phixtral-4x2_8</a> model on a single <a href="https://www.nvidia.com/en-us/data-center/l4/">NVIDIA L4 GPU</a> for <strong>$0.22</strong> / hour / replica, or <strong>$160</strong> / month / replica. This is a <strong>45x</strong> improvement over the cost of deploying the <a href="https://huggingface.co/mistralai/Mixtral-8x7B-v0.1">mistralai/Mixtral-8x7B-v0.1</a> model on 2x <a href="https://www.nvidia.com/en-us/data-center/a100/">NVIDIA A100-80G GPUs</a>, which can cost upwards of $7000 / month (on-demand) on CSPs. As advancements in model compression, quantization and model mixing continue to improve, we expect more users to be able to fine-tune, distill and deploy these expert <a href="https://www.microsoft.com/en-us/research/blog/orca-2-teaching-small-language-models-how-to-reason/">small-langauge models</a> on a budget, without sacrificing significantly on performance.</p>
<p>The table below shows the costs of deploying one of these popular MoE LLM models on a single GPU server on GCP. As you can see, the cost of deploying a single model can range from $500 to $7300 / month, depending on the model and of course CSP (kept fixed here). </p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Cloud Provider</th>
<th>GPU</th>
<th>VRAM</th>
<th>Spot</th>
<th>Cost / hr</th>
<th>Cost / month</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://huggingface.co/mistralai/Mixtral-8x7B-v0.1">mistralai/Mixtral-8x7B-v0.1</a></td>
<td>GCP</td>
<td>2x NVIDIA A100-80G</td>
<td>~94‚ÄØGB</td>
<td>-</td>
<td>$10.05</td>
<td>~$7236</td>
</tr>
<tr>
<td><a href="https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ">TheBloke/Mixtral-8x7B-Instruct-v0.1-AWQ</a></td>
<td>GCP</td>
<td>NVIDIA A100-40G</td>
<td>~25GB</td>
<td>-</td>
<td>$3.67</td>
<td>~$2680</td>
</tr>
<tr>
<td><a href="https://huggingface.co/mlabonne/phixtral-4x2_8">mlabonne/phixtral-4x2_8</a></td>
<td>GCP</td>
<td>NVIDIA L4</td>
<td>~9GB</td>
<td>-</td>
<td>$0.70</td>
<td>~$500</td>
</tr>
<tr>
<td><strong><a href="https://huggingface.co/mlabonne/phixtral-4x2_8">mlabonne/phixtral-4x2_8</a></strong></td>
<td>GCP</td>
<td>NVIDIA L4</td>
<td>~9GB</td>
<td>‚úÖ</td>
<td><strong>$0.22</strong></td>
<td><strong>~$160</strong></td>
</tr>
</tbody>
</table>
<p>However, the <strong>onus is on the developer</strong> to figure out the <strong>right instance type, spot instance strategy, and the right number of replicas</strong> to deploy to ensure that the service is both <em>cost-efficient and performant</em>. In the coming weeks, we're going to be introducing some exciting tools to help developers alleviate this pain and provide transparency to make the right infrastructure decisions for their services. Stay tuned!</p>
<h3 id="wrapping-up"><a class="toclink" href="../serving-llms-on-a-budget.html#wrapping-up">üéÅ Wrapping up</a></h3>
<p>In this blog post, we showed you how to deploy the <a href="https://huggingface.co/mlabonne/phixtral-4x2_8">mlabonne/phixtral-4x2_8</a> model on a single NVIDIA L4 GPU for under $160 / month and scale-out a dirt-cheap inference service of your own. We used <a href="https://github.com/skypilot-org/skypilot">SkyPilot</a> to deploy and manage our <a href="https://github.com/autonomi-ai/nos">NOS</a> service on spot (pre-emptible) instances, making them especially cost-efficient.</p>
<p>In our next blog post, we‚Äôll take it one step further. We'll explore how you can serve multiple models on the same GPU so that <strong>your infrastructure costs don‚Äôt have to scale with the number of models you serve</strong>. The <strong>TL;DR</strong> is that you will soon be able to serve multiple models with fixed and predictable pricing, making model serving more accessible and cost-efficient than ever before.</p>
<p><br></p>
    
  </div>
</article>
      
        <article class="md-post md-post--excerpt">
  <header class="md-post__header">
    
      <nav class="md-post__authors md-typeset">
        
          <span class="md-author">
            <img src="https://github.com/spillai.png" alt="Sudeep Pillai">
          </span>
        
      </nav>
    
    <div class="md-post__meta md-meta">
      <ul class="md-meta__list">
        <li class="md-meta__item">
          <time datetime="2024-01-16 00:00:00">Jan 16, 2024</time></li>
        
          <li class="md-meta__item">
            in
            
              <a href="../category/infra.html" class="md-meta__link">infra</a>, 
              <a href="../category/tutorials.html" class="md-meta__link">tutorials</a></li>
        
        
          
          <li class="md-meta__item">
            
              2 min read
            
          </li>
        
      </ul>
      
    </div>
  </header>
  <div class="md-post__content md-typeset">
    <h2 id="getting-started-with-nos-tutorials"><a class="toclink" href="../-getting-started-with-nos-tutorials.html">üìö Getting started with NOS tutorials</a></h2>
<p>We are thrilled to announce a new addition to our resources - the <a href="https://github.com/autonomi-ai/nos/tree/main/examples/tutorials"><strong>NOS Tutorials</strong></a>! This series of tutorials is designed to empower users with the knowledge and tools needed to leverage NOS for serving models efficiently and effectively. Whether you're a seasoned developer or just starting out, our tutorials offer insights into various aspects of using NOS, making your journey with model serving a breeze.</p>
<p>Over the next few weeks, we'll walk you through the process of using NOS to serve models, from the basics to more advanced topics. We'll also cover how to use NOS in a production environment, ensuring you have all the tools you need to take your projects to the next level. Finally, keep yourself updated on NOS by giving us a üåü on <a href="https://github.com/autonomi-ai/nos">Github</a>.</p>
<div class="admonition tip">
<p class="admonition-title">Can't wait? Show me the code!</p>
<p>If you can't wait to get started, head over to our <a href="https://github.com/autonomi-ai/nos/tree/main/examples/tutorials">tutorials</a> page on <a href="https://github.com/autonomi-ai/nos">Github</a> to dive right in to the code!</p>
</div>
<h3 id="whats-inside-the-nos-tutorials"><a class="toclink" href="../-getting-started-with-nos-tutorials.html#whats-inside-the-nos-tutorials">üåü What‚Äôs Inside the NOS Tutorials?</a></h3>
<p>The NOS Tutorials encompass a wide range of topics, each focusing on different facets of model serving. Here's a sneak peek into what you can expect:</p>
<h4 id="1-serving-custom-models-01-serving-custom-models"><a class="toclink" href="../-getting-started-with-nos-tutorials.html#1-serving-custom-models-01-serving-custom-models">1. Serving custom models: <a href="https://github.com/autonomi-ai/nos/tree/main/examples/tutorials/01-serving-custom-models"><code>01-serving-custom-models</code></a></a></h4>
<p>Dive into the world of custom GPU models with NOS. This tutorial shows you how easy it is to wrap your <a href="https://pytorch.org/">PyTorch</a> code with NOS, and serve them via a REST / gRPC API.</p>
<h4 id="2-serving-multiple-methods-02-serving-multiple-methods"><a class="toclink" href="../-getting-started-with-nos-tutorials.html#2-serving-multiple-methods-02-serving-multiple-methods">2. Serving multiple methods: <a href="https://github.com/autonomi-ai/nos/tree/main/examples/tutorials/02-serving-multiple-methods"><code>02-serving-multiple-methods</code></a></a></h4>
<p>Learn how to expose several custom methods of a model for serving. This tutorial is perfect for those looking to tailor their model's functionality to specific requirements, enhancing its utility and performance.</p>
<h4 id="3-serve-llms-with-streaming-support-03-llm-streaming-chat"><a class="toclink" href="../-getting-started-with-nos-tutorials.html#3-serve-llms-with-streaming-support-03-llm-streaming-chat">3. Serve LLMs with streaming support: <a href="https://github.com/autonomi-ai/nos/tree/main/examples/tutorials/03-llm-streaming-chat"><code>03-llm-streaming-chat</code></a></a></h4>
<p>Get hands-on with serving an LLM with streaming support. This tutorial focuses on using <a href="https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0"><code>TinyLlama/TinyLlama-1.1B-Chat-v0.1</code></a>, showcasing how to implement streaming capabilities with NOS for smoother, more efficient language model interactions.</p>
<h4 id="4-serve-multiple-models-on-the-same-gpu-04-serving-multiple-models"><a class="toclink" href="../-getting-started-with-nos-tutorials.html#4-serve-multiple-models-on-the-same-gpu-04-serving-multiple-models">4. Serve multiple models on the same GPU: <a href="https://github.com/autonomi-ai/nos/tree/main/examples/tutorials/04-serving-multiple-models"><code>04-serving-multiple-models</code></a></a></h4>
<p>Step up your game by serving multiple models on the same GPU. This tutorial explores the integration of models like <a href="https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0"><code>TinyLlama/TinyLlama-1.1B-Chat-v0.1</code></a> and <a href="https://huggingface.co/distil-whisper/distil-small.en">distil-whisper/distil-small.en</a>, enabling multi-modal applications such as audio transcription combined with summarization on a single GPU.</p>
<h4 id="5-serving-models-in-production-with-docker-05-serving-with-docker"><a class="toclink" href="../-getting-started-with-nos-tutorials.html#5-serving-models-in-production-with-docker-05-serving-with-docker">5. Serving models in production with Docker <a href="https://github.com/autonomi-ai/nos/tree/main/examples/tutorials/05-serving-with-docker"><code>05-serving-with-docker</code></a></a></h4>
<p>Enter the realm of production environments with our Docker tutorial. This guide is essential for anyone looking to use NOS in a more structured, scalable environment. You'll learn how to deploy your production NOS images with Docker and Docker Compose, ensuring your model serving works with existing ML infrastructure as reliably as possible.</p>
<div class="admonition info">
<p class="admonition-title">Stay tuned!</p>
<p>üîó <strong>Stay tuned</strong>, as we'll continuously update the section with more tutorials and resources to keep you ahead in the ever-evolving world of model serving!</p>
</div>
<p>Happy Model Serving!</p>
<hr />
<p><em>This blog post is brought to you by the <a href="https://github.com/autonomi-ai/">NOS Team</a> - committed to making model serving fast, efficient, and accessible to all!</em></p>
    
  </div>
</article>
      
        <article class="md-post md-post--excerpt">
  <header class="md-post__header">
    
      <nav class="md-post__authors md-typeset">
        
          <span class="md-author">
            <img src="https://github.com/spillai.png" alt="Sudeep Pillai">
          </span>
        
      </nav>
    
    <div class="md-post__meta md-meta">
      <ul class="md-meta__list">
        <li class="md-meta__item">
          <time datetime="2024-01-02 00:00:00">Jan 2, 2024</time></li>
        
          <li class="md-meta__item">
            in
            
              <a href="../category/infra.html" class="md-meta__link">infra</a></li>
        
        
          
          <li class="md-meta__item">
            
              3 min read
            
          </li>
        
      </ul>
      
    </div>
  </header>
  <div class="md-post__content md-typeset">
    <h2 id="introducing-nos-blog"><a class="toclink" href="../introducing-nos-blog.html">Introducing NOS Blog!</a></h2>
<p>At <a href="https://autonomi.ai">Autonomi AI</a>, we build infrastructure tools to make AI <em>fast</em>, <em>easy</em> and <em>affordable</em>. We‚Äôre in the early development years of the ‚ÄúLinux OS for AI‚Äù, where the commoditization of open-source models and tools will be the critical to the safe and ubiquitous use of AI. Needless to say, it‚Äôs the most exciting and ambitious infrastructure project our generation is going to witness in the coming decade. </p>
<p>A few weeks back, we open-sourced <a href="https://github.com/autonomi-ai/nos"><strong>NOS</strong></a> - a fast and flexible inference server for¬†<a href="https://pytorch.org/">PyTorch</a>¬†that can run a whole host of open-source AI models (LLMs, Stable Diffusion, CLIP, Whisper, Object Detection etc) all under one-roof. <strong>Today, we‚Äôre finally excited to launch the NOS blog</strong>.</p>
<h3 id="why-are-we-building-yet-another-ai-inference-server"><a class="toclink" href="../introducing-nos-blog.html#why-are-we-building-yet-another-ai-inference-server">üéØ Why are we building yet another AI inference server?</a></h3>
<p>Most inference API implementations today deeply couple the API framework (<a href="https://fastapi.tiangolo.com/">FastAPI</a>, <a href="https://flask.palletsprojects.com/en/3.0.x/">Flask</a>) with the modeling backend (<a href="https://pytorch.org/">PyTorch</a>, <a href="https://www.tensorflow.org/">TF</a> etc) - in other words, it doesn‚Äôt let you separate the concerns for the AI backend (e.g. AI hardware, drivers, model compilation, execution runtime, scale out, memory efficiency, async/batched execution, multi-model management etc) from your AI application (e.g. auth, observability, telemetry, web integrations etc), especially if you‚Äôre looking to build a production-ready application.</p>
<p>We‚Äôve made it very easy for developers to host new PyTorch models as APIs and take them to production without having to worry about any of the backend infrastructure concerns. We build on some awesome projects like <a href="https://fastapi.tiangolo.com/">FastAPI</a>, <a href="https://ray.io/">Ray</a>, <a href="https://www.huggingface.co">Hugging Face</a>, <a href="https://github.com/huggingface/transformers">transformers</a> and <a href="https://github.com/huggingface/transformers">diffusers</a>.</p>
<p>We‚Äôve been big believers of <em>multi-modal</em> from the very beginning, and you can do all of it with NOS today.¬†Give us a üåü on <a href="https://github.com/autonomi-ai/nos">Github</a> if you're stoked -- NOS can run locally on your Linux desktop (with a gaming GPU), in any cloud GPU (NVIDIA L4, A100s, etc) and even on CPUs. Very soon, we'll support running models on <a href="https://www.apple.com/newsroom/2023/10/apple-unveils-m3-m3-pro-and-m3-max-the-most-advanced-chips-for-a-personal-computer/">Apple Silicon</a> and custom AI accelerators such as <a href="https://aws.amazon.com/ec2/instance-types/inf2/">Inferentia2</a> from Amazon Web Services (AWS).</p>
<div class="admonition info">
<p class="admonition-title">What's coming?</p>
<p>Over the coming weeks, we‚Äôll be announcing some <em>awesome features</em> that we believe will make the power of large foundation models more accessible, cheaper and easy-to-use than ever before. </p>
</div>
<h3 id="nos-in-a-nutshell"><a class="toclink" href="../introducing-nos-blog.html#nos-in-a-nutshell">ü•ú NOS, in a nutshell</a></h3>
<p>NOS was built from the ground-up, with developers in mind. Here are a few things we think developers care about:</p>
<ul>
<li>ü•∑ <strong>Flexible</strong>: Support for OSS models with custom runtimes with pip, conda and cuda/driver dependencies.</li>
<li>üîå <strong>Pluggable:</strong> Simple API over a high-performance gRPC or REST API that supports batched requests, and streaming.</li>
<li>üöÄ <strong>Scalable</strong>: Serve multiple custom models simultaneously on a single or multi-GPU instance, without having to worry about memory management and model scaling.</li>
<li>üèõÔ∏è <strong>Local</strong>: Local execution means that you control your data, and you‚Äôre free to build NOS for domains that are more restrictive with data-privacy.</li>
<li>‚òÅÔ∏è <strong>Cloud-agnostic:</strong> Fully containerized means that you can develop, test and deploy NOS locally, on-prem, on any cloud or AI CSP.</li>
<li>üì¶ <strong>Extensible</strong>: Written entirely in Python so it‚Äôs easily hackable and extensible with an Apache-2.0 License for commercial use.</li>
</ul>
<p>Go ahead and check out our <a href="https://github.com/autonomi-ai/nos-playground">playground</a>, and try out some of the more recent models with NOS.</p>
<h3 id="relevant-links"><a class="toclink" href="../introducing-nos-blog.html#relevant-links">üîó Relevant Links</a></h3>
<ul>
<li>‚≠êÔ∏è Github: <a href="https://github.com/autonomi-ai/nos">https://github.com/autonomi-ai/nos</a></li>
<li>üë©‚Äçüíª Playground: <a href="https://github.com/autonomi-ai/nos-playground">https://github.com/autonomi-ai/nos-playground</a></li>
<li>üìö Docs: <a href="https://docs.nos.run/">https://docs.nos.run/</a></li>
<li>üí¨ <a href="https://discord.gg/QAGgvTuvgg">Discord</a>, <a href="https://twitter.com/autonomi_ai">X / Twitter</a>, <a href="https://www.linkedin.com/company/autonomi-ai/">LinkedIn</a></li>
</ul>
    
  </div>
</article>
      
      
        
          



<nav class="md-pagination">
  
</nav>
        
      
    </div>
  </div>

          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2022 - 2024 <a href="https://autonomi.ai" target="_blank">Autonomi AI, Inc.</a> All rights reserved.

    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="mailto:support@autonomi.ai" target="_blank" rel="noopener" title="" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M498.1 5.6c10.1 7 15.4 19.1 13.5 31.2l-64 416c-1.5 9.7-7.4 18.2-16 23s-18.9 5.4-28 1.6L284 427.7l-68.5 74.1c-8.9 9.7-22.9 12.9-35.2 8.1S160 493.2 160 480v-83.6c0-4 1.5-7.8 4.2-10.7l167.6-182.9c5.8-6.3 5.6-16-.4-22s-15.7-6.4-22-.7L106 360.8l-88.3-44.2C7.1 311.3.3 300.7 0 288.9s5.9-22.8 16.1-28.7l448-256c10.7-6.1 23.9-5.5 34 1.4z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://github.com/autonomi-ai/nos" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://www.linkedin.com/company/74939899" target="_blank" rel="noopener" title="www.linkedin.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://twitter.com/autonomi_ai" target="_blank" rel="noopener" title="twitter.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://discord.gg/QAGgvTuvgg" target="_blank" rel="noopener" title="discord.gg" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.5.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M524.531 69.836a1.5 1.5 0 0 0-.764-.7A485.065 485.065 0 0 0 404.081 32.03a1.816 1.816 0 0 0-1.923.91 337.461 337.461 0 0 0-14.9 30.6 447.848 447.848 0 0 0-134.426 0 309.541 309.541 0 0 0-15.135-30.6 1.89 1.89 0 0 0-1.924-.91 483.689 483.689 0 0 0-119.688 37.107 1.712 1.712 0 0 0-.788.676C39.068 183.651 18.186 294.69 28.43 404.354a2.016 2.016 0 0 0 .765 1.375 487.666 487.666 0 0 0 146.825 74.189 1.9 1.9 0 0 0 2.063-.676A348.2 348.2 0 0 0 208.12 430.4a1.86 1.86 0 0 0-1.019-2.588 321.173 321.173 0 0 1-45.868-21.853 1.885 1.885 0 0 1-.185-3.126 251.047 251.047 0 0 0 9.109-7.137 1.819 1.819 0 0 1 1.9-.256c96.229 43.917 200.41 43.917 295.5 0a1.812 1.812 0 0 1 1.924.233 234.533 234.533 0 0 0 9.132 7.16 1.884 1.884 0 0 1-.162 3.126 301.407 301.407 0 0 1-45.89 21.83 1.875 1.875 0 0 0-1 2.611 391.055 391.055 0 0 0 30.014 48.815 1.864 1.864 0 0 0 2.063.7A486.048 486.048 0 0 0 610.7 405.729a1.882 1.882 0 0 0 .765-1.352c12.264-126.783-20.532-236.912-86.934-334.541ZM222.491 337.58c-28.972 0-52.844-26.587-52.844-59.239s23.409-59.241 52.844-59.241c29.665 0 53.306 26.82 52.843 59.239 0 32.654-23.41 59.241-52.843 59.241Zm195.38 0c-28.971 0-52.843-26.587-52.843-59.239s23.409-59.241 52.843-59.241c29.667 0 53.307 26.82 52.844 59.239 0 32.654-23.177 59.241-52.844 59.241Z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../../..", "features": ["content.code.copy", "content.code.annotate", "navigation.instant", "navigation.tracking", "toc.follow"], "search": "../../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.081f42fc.min.js"></script>
      
    
  </body>
</html>