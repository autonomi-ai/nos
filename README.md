<center><img src="./docs/assets/nos-header.svg" alt="Nitrous Oxide for your AI Infrastructure"></center>
<p></p>
<p align="center">
<a href="https://pypi.org/project/torch-nos/">
    <img alt="PyPI Version" src="https://badge.fury.io/py/torch-nos.svg">
</a>
<a href="https://pypi.org/project/torch-nos/">
    <img alt="PyPI Version" src="https://img.shields.io/pypi/pyversions/torch-nos">
</a>
<a href="https://www.pepy.tech/projects/torch-nos">
    <img alt="PyPI Downloads" src="https://img.shields.io/pypi/dm/torch-nos">
</a>
<a href="https://github.com/autonomi-ai/nos/blob/main/LICENSE">
    <img alt="PyPi Downloads" src="https://img.shields.io/github/license/torch-nos/torch-nos.svg">
</a><br>
<a href="https://discord.gg/QAGgvTuvgg">
    <img alt="Discord" src="https://img.shields.io/badge/discord-chat-purple?color=%235765F2&label=discord&logo=discord">
</a>
<a href="https://twitter.com/autonomi_ai">
    <img alt="PyPi Version" src="https://img.shields.io/twitter/follow/autonomi_ai.svg?style=social&logo=twitter">
</a>
</p>
<p align="center">
<a href="https://nos.run/"><b>Website</b></a> | <a href="https://docs.nos.run/"><b>Docs</b></a> |  <a href="https://discord.gg/QAGgvTuvgg"><b>Discord</b></a>
</p>

## âš¡ï¸ What is NOS?
**NOS (`torch-nos`)** is a fast and flexible Pytorch inference server, specifically designed for optimizing and running inference of popular foundational AI models.

- ğŸ‘©â€ğŸ’» **Easy-to-use**: Built for [PyTorch](https://pytorch.org/) and designed to optimize, serve and auto-scale Pytorch models in production without compromising on developer experience.
- ğŸ¥· **Flexible**: Run and serve several foundational AI models ([Stable Diffusion](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0), [CLIP](https://huggingface.co/openai/clip-vit-base-patch32), [Whisper](https://huggingface.co/openai/whisper-large-v2)) in a single place.
- ğŸ”Œ **Pluggable:** Plug your front-end to NOS with out-of-the-box high-performance gRPC/REST APIs, avoiding all kinds of ML model deployment hassles.
- ğŸš€ **Scalable**: Optimize and scale models easily for maximum HW performance without a PhD in ML, distributed systems or infrastructure.
- ğŸ“¦ **Extensible**: Easily hack and add custom models, optimizations, and HW-support in a Python-first environment.
- âš™ï¸ **HW-accelerated:** Take full advantage of your underlying HW (GPUs, ASICs) without compromise.
- â˜ï¸ **Cloud-agnostic:** Run on any cloud HW (AWS, GCP, Azure, Lambda Labs, On-Prem) with our ready-to-use inference server containers.


> **NOS** inherits its name from **N**itrous **O**xide **S**ystem, the performance-enhancing system typically used in racing cars. NOS is designed to be modular and easy to extend.


## ğŸš€ Getting Started

Get started with the full NOS server by installing via pip:

  ```shell
  $ conda env create -n nos-py38 python=3.8
  $ conda activate nos-py38
  $ conda install pytorch>=2.0.1 torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia
  $ pip install torch-nos[server]
  ```

If you want to simply use a light-weight NOS client and run inference on your local machine (via docker), you can install the client-only package:

  ```shell
  $ conda env create -n nos-py38 python=3.8
  $ conda activate nos-py38
  $ pip install torch-nos
  ```

## ğŸ”¥ Quickstart / Show me the code

### Image Generation as-a-Service


<table>
<tr>
<td> gRPC API âš¡ </td>
<td> REST API </td>
</tr>
<tr>
<td>

```python
from nos.client import Client

client = Client("[::]:50051")

sdxl = client.Module("stabilityai/stable-diffusion-xl-base-1-0")
image, = sdxl(prompts=["fox jumped over the moon"],
              width=1024, height=1024, num_images=1)
```

</td>
<td>


```bash
curl \
-X POST http://localhost:8000/infer \
-H 'Content-Type: application/json' \
-d '{
      "model_id": "stabilityai/stable-diffusion-xl-base-1-0",
      "inputs": {
          "prompts": ["fox jumped over the moon"],
          "width": 1024,
          "height": 1024,
          "num_images": 1
      }
    }'
```

</td>
</tr>
</table>

### Text & Image Embedding-as-a-Service (CLIP-as-a-Service)

<table>
<tr>
<td> gRPC API âš¡ </td>
<td> REST API </td>
</tr>
<tr>
<td>

```python
from nos.client import Client

client = Client("[::]:50051")

clip = client.Module("openai/clip")
txt_vec = clip.encode_text(text=["fox jumped over the moon"])
```

</td>
<td>

```bash
curl \
-X POST http://localhost:8000/infer \
-H 'Content-Type: application/json' \
-d '{
      "model_id": "openai/clip",
      "method": "encode_text",
      "inputs": {
          "texts": ["fox jumped over the moon"]
      }
    }'
```

</td>
</tr>
</table>


## ğŸ“‚ Directory Structure

```bash
â”œâ”€â”€ docker         # Dockerfile for CPU/GPU servers
â”œâ”€â”€ docs           # mkdocs documentation
â”œâ”€â”€ examples       # example guides, jupyter notebooks, demos
â”œâ”€â”€ makefiles      # makefiles for building/testing
â”œâ”€â”€ nos
â”‚Â Â  â”œâ”€â”€ cli        # CLI (hub, system)
â”‚Â Â  â”œâ”€â”€ client     # gRPC / REST client
â”‚Â Â  â”œâ”€â”€ common     # common utilities
â”‚Â Â  â”œâ”€â”€ executors  # runtime executor (i.e. Ray)
â”‚Â Â  â”œâ”€â”€ hub        # hub utilies
â”‚Â Â  â”œâ”€â”€ managers   # model manager / multiplexer
â”‚Â Â  â”œâ”€â”€ models     # model zoo
â”‚Â Â  â”œâ”€â”€ proto      # protobuf defs for NOS gRPC service
â”‚Â Â  â”œâ”€â”€ server     # server backend (gRPC)
â”‚Â Â  â””â”€â”€ test       # pytest utilities
â”œâ”€â”€ requirements   # requirement extras (server, docs, tests)
â”œâ”€â”€ scripts        # basic scripts
â””â”€â”€ tests          # pytests (client, server, benchmark)
```

## ğŸ“š Documentation

- [Quickstart](./docs/quickstart.md)
- [Models](./docs/models/supported-models.md)
- **Concepts**: [NOS Architecture](./docs/concepts/architecture-overview.md)
- **Demos**: [Building a Discord Image Generation Bot](./docs/demos/discord-bot.md), [Video Search Demo](./docs/demos/video-search.md)

## ğŸ›£ Roadmap

### HW / Cloud Support

- [x] **Commodity GPUs**
    - [x] NVIDIA GPUs (20XX, 30XX, 40XX)
    - [ ] AMD GPUs (RX 7000)

- [x] **Cloud GPUs**
    - [x] NVIDIA (H100, A100, A10G, A30G, T4, L4)
    - [ ] AMD (MI200, MI250)

- [x] **Cloud Service Providers** (via [SkyPilot](https://github.com/skypilot-org/skypilot))
    - [x] AWS, GCP, Azure
    - [ ] **Opinionated Cloud:** Lambda Labs, RunPod, etc

- [ ] **Cloud ASICs**
    - [ ] [AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/) ([Inf1](https://aws.amazon.com/ec2/instance-types/inf1/)/[Inf2](https://aws.amazon.com/ec2/instance-types/inf2/))
    - [ ] Google TPU
    - [ ] Coming soon! (Habana Gaudi, Tenstorrent)


## ğŸ“„ License

This project is licensed under the [Apache-2.0 License](LICENSE).

## ğŸ“¡ Telemetry

NOS collects anonymous usage data using [Sentry](https://sentry.io/). This is used to help us understand how the community is using NOS and to help us prioritize features. You can opt-out of telemetry by setting `NOS_TELEMETRY_ENABLED=0`.

## ğŸ¤ Contributing
We welcome contributions! Please see our [contributing guide](CONTRIBUTING.md) for more information.

### ğŸ”—  Quick Links

* ğŸ’¬ Send us an email at [support@autonomi.ai](mailto:support@autonomi.ai) or join our [Discord](https://discord.gg/QAGgvTuvgg) for help.
* ğŸ“£ Follow us on [Twitter](https://twitter.com/autonomi\_ai), and [LinkedIn](https://www.linkedin.com/company/autonomi-ai) to keep up-to-date on our products.

<style> .md-typeset h1, .md-content__button { display: none; } </style>
