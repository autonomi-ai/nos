syntax = "proto3";

import "google/protobuf/empty.proto";

package nos.inference;


message ModelListResponse {
  repeated string models = 1;
}


// Model information request specification
// Currently, we provide batch size and number of replicas
message ModelInfoRequest {
  string model_name = 1;
}

message ModelInfoResponse {
}


message TextRequest {
  string text = 1;
}


message ImageRequest {
  bytes image_bytes = 1;
}


message InitModelRequest {
  string model_name = 1;
  optional int32 num_replicas = 2;
}


message InitModelResponse {
  string result = 1;
}


message DeleteModelRequest {
  string model_name = 1;
}


message DeleteModelResponse {
  string result = 1;
}


// Inference request specification
message InferenceRequest {
  // Method to use for inference
  string method = 1;
  // Model name to use for inference
  string model_name = 2;
  // Input data (text or image)
  oneof request {
    TextRequest text_request = 3;
    ImageRequest image_request = 4;
  }
}


message InferenceResponse {
  bytes result = 1;
}


// Nos server Ping / health response
message PingResponse {
    string status = 1;  // (e.g. "ok" or "not_ok")
}

message ServiceInfoResponse {
    string version = 1;  // (e.g. "0.1.0")
}

// Service definition
service InferenceService {
  // Check health status of the inference server.
  rpc Ping(google.protobuf.Empty) returns (PingResponse) {}

  // Get service information (version, release date etc.)
  rpc GetServiceInfo(google.protobuf.Empty) returns (ServiceInfoResponse) {}

  // List available models from Hugging Face Hub
  rpc ListModels(google.protobuf.Empty) returns (ModelListResponse) {};

  // Load model from Hugging Face Hub
  rpc InitModel(InitModelRequest) returns (InitModelResponse) {}

  // Delete model from deployment
  rpc DeleteModel(DeleteModelRequest) returns (DeleteModelResponse) {}

  // Get model information from the deployment
  rpc GetModelInfo(ModelInfoRequest) returns (ModelInfoResponse) {};

  // Perform text-to-vector prediction
  rpc Predict(InferenceRequest) returns (InferenceResponse) {}
}
